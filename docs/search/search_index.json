{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction These materials aim to help undergraduate students explore and understand changes in the global environment using Google Earth Engine. They introduce core concepts of geographic information systems and remote sensing, methods for implementing these concepts with Earth Engine\u2019s javascript code editor, and a series of real-world problems and applications. This is a work in progress. I will be using it in Fall 2022 to teach a course in Environmental Studies and Geography at Middlebury College and it will be updated continuously. I\u2019ve organized the primer into chapters that each contain about a week\u2019s worth of content. Each chapter contains three sections: Introduces key words, concepts, and principles of spatial analysis, spatial problem-solving, geographic information systems, and remote sensing. Introduces code snippets to implement concepts with Google Earth Engine\u2019s code editor. Sketches geographic and environmental problems for students to solve by bringing together concepts and code. I am also developing problem sets to assess near and far transfer of understanding. In my course, these function as exams and for that reason I keep them in a private repository. If you are an instructor who would like access to these materials, please contact me. Jeff Howarth Associate Professor of Geography Middlebury College, Middlebury, Vermont, USA jhowarth@middlebury.edu","title":"Introduction"},{"location":"#introduction","text":"These materials aim to help undergraduate students explore and understand changes in the global environment using Google Earth Engine. They introduce core concepts of geographic information systems and remote sensing, methods for implementing these concepts with Earth Engine\u2019s javascript code editor, and a series of real-world problems and applications. This is a work in progress. I will be using it in Fall 2022 to teach a course in Environmental Studies and Geography at Middlebury College and it will be updated continuously. I\u2019ve organized the primer into chapters that each contain about a week\u2019s worth of content. Each chapter contains three sections: Introduces key words, concepts, and principles of spatial analysis, spatial problem-solving, geographic information systems, and remote sensing. Introduces code snippets to implement concepts with Google Earth Engine\u2019s code editor. Sketches geographic and environmental problems for students to solve by bringing together concepts and code. I am also developing problem sets to assess near and far transfer of understanding. In my course, these function as exams and for that reason I keep them in a private repository. If you are an instructor who would like access to these materials, please contact me. Jeff Howarth Associate Professor of Geography Middlebury College, Middlebury, Vermont, USA jhowarth@middlebury.edu","title":"Introduction"},{"location":"00/code/account/","text":"You will need to sign up for a Google Earth Engine account. This is free for students. We will walk through the sign-up process in our first meeting. You sign up through the Google Earth Engine web site .","title":"Account"},{"location":"00/code/code_editor/","text":"Earth Engine Code Editor We will use the web-based Code Editor for Earth Engine. You might as well bookmark this site in your browser. It provides an interface for writing and running code, while also providing tools for managing files and assets, composing and inspecting maps, finding documentation, and printing results and widgets. The diagram shown below is from Google\u2019s Earth Engine documentation . As you can see, the Code Editor consists of a lot of panels and buttons. As you use the code editor, you will quickly become familiar with navigating the interface. In our first lab meeting, we will go over the following common tasks. Common Task Panel or button Manage files Scripts Assets Find help for tools Docs Write and run code Text Editor Save Inspect results Console Inspect locations Inspector Inspect tasks Tasks Navigate map Pan Zoom Change basemap Map Satellite Draw geometry Point Line Shape Rectangle","title":"Code Editor"},{"location":"00/code/code_editor/#earth-engine-code-editor","text":"We will use the web-based Code Editor for Earth Engine. You might as well bookmark this site in your browser. It provides an interface for writing and running code, while also providing tools for managing files and assets, composing and inspecting maps, finding documentation, and printing results and widgets. The diagram shown below is from Google\u2019s Earth Engine documentation . As you can see, the Code Editor consists of a lot of panels and buttons. As you use the code editor, you will quickly become familiar with navigating the interface. In our first lab meeting, we will go over the following common tasks. Common Task Panel or button Manage files Scripts Assets Find help for tools Docs Write and run code Text Editor Save Inspect results Console Inspect locations Inspector Inspect tasks Tasks Navigate map Pan Zoom Change basemap Map Satellite Draw geometry Point Line Shape Rectangle","title":"Earth Engine Code Editor"},{"location":"00/code/javaScript101/","text":"JavaScript 101 The Code Editor lets you work with Earth Engine by writing code in JavaScript. Like any language, JavaScript has a vocabulary and grammar that you will learn through practice. This page introduces some key terms, syntax, and contexts for writing code with JavaScript. Like most of the pages tagged with in this primer, this page contains code snippets that you can copy and paste into the code editor to see their effects. To do this, open the code editor , copy and past the code snippets into it, and click RUN to execute the snippet. You can also save you script as a file by clicking the SAVE button. The file name should not have spaces and should end with .js to mark that it is a JavaScript file (e.g. javascript101.js ). Scripts A script is a sequence of statements that can be executed (when you \u2018run\u2019 the script). Lines A line is the basic organizing unit of a script. Think of a line like a ruling on a sheet of paper, while a statement is like the sentence that you write on the ruling. Each line has a number, shown on the left side of the text editor panel. These line indexes are helpful for troubleshooting code because if you make an error the code editor will usually tell you the line number that tripped it up. Comments A comment tells the computer: please ignore this . You often use comments to document your code and write little notes to yourself and to people who may read and want to reuse your code. They are like putting notes in the margins of a book without affecting the content of the book or like the director\u2019s commentary of a movie that can be muted when you watch. A line comment tells the computer not to execute anything that follows two forward slashes on a line. A multi-line comment tells the computer not to execute anything from a defined beginning and ending which may span multiple lines. // LINE COMMENTS start with two forward slashes. Like this line. The computer won't execute anything on this line that follows the slashes. /* MULTI LINE COMMENTS start with a forward slash and a star and end with a star and a forward slash. The computer won't execute anything between the stars, even if your comment spans many many lines . */ Script header It is good practice to begin every script with a header that states a title, your name, the date, and a brief description of the script\u2019s purpose. To do this, you can use either a line comment or multi-line comment. Here is an example of a header with line comments. Note how each line begins with two forward slashes. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: Javascript 101 // NAME: Your name here please // DATE: Today's date // PURPOSE: Getting started with JavaScript // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Here is an example of a header with a multi-line comment. /* ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TITLE: Javascript 101 NAME: Your name here please DATE: Today's date PURPOSE: Getting started with JavaScript ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ */ Either method is fine. The most important thing is to remember to start each script with a header. Statements, objects, and variables When you write a script, you write statements to create and manipulate different kinds of data objects . An object class is an abstract template, while an instance is a manifestation of it. For example, the concept of \u2018a book\u2019 is like an object class, while \u2018the book sitting next to your bed\u2019 is an instance of this class. Sometimes you hear the words \u201ctype\u201d and \u201ctoken\u201d to describe the same distinction. A common statement involves first creating a container to store something and then putting an instance of a class into the container. In JavaScript, the container is called a variable . A variable can store different kinds of objects and you can name the variable (almost) anything that you want. // This statement creates the variable age_daughter to store an instance of a NUMBER object. var age_daughter = 4 ; // This statement creates the variable age_son to store an instance of a STRING object. A string starts and ends with a single quote. var age_son = 'six years old' ; // A string object can also start and end with double quotes, // (but don't mix and match them). var name_daughter = \"isabel\" ; // This variable stores a set of strings as a LIST object that is defined by square brackets. var kid_names = [ 'Aldo' , 'Isabel' ]; // This list stores a set of numbers. var kid_ages = [ 6 , 4 ]; // Curly brackets (or braces) can be used to define DICTIONARIES, or a set of KEY:VALUE pairs. var kidStats = { 'name' : 'Aldo' , 'age' : 6 , 'birthplace' : 'Middlebury, VT' }; Statement syntax If you examine these simple statements, you might recognize some basic rules of syntax. 1. How do you define a variable? In JavaScript, the statement starts with the keyword var followed by the equal sign. That clause says \u2018create a variable named\u2026\u2019. 2. What do you notice about the variable names? They follow a simple rule: The name of a variable cannot contain spaces. To honor this rule, you can follow two naming conventions for variables. // An example of the snake_case. var this_will_work = 'snake_case' ; // An example of the camelCase. var thisWillAlsoWork = 'camelCase' ; 3. What defines the object class of the instance? This is subtle, but important. The object class is coded by the format of the text in the statement. OBJECT CLASS FORMAT EXAMPLE NUMBER plain old number 4 STRING \u2018 \u2018 \u201d \u201c \u2018isabel\u2019 \u201ckid\u2019s\u201c LIST [ ] [\u2018aldo\u2019, \u2018isabel\u2019] DICTIONARY {:} {\u2018name\u2019: \u2018aldo\u2019} 4. How does each statement end? Do you see that little thing at the end of each statement winking at you? This is a mark of good coding practice: Statements should end in a semi-colon . // Statements should end in a SEMI-COLON, or the editor complains. var dangling = 'I feel incomplete...' var complete = 'This feels better, thanks.' ; In the above example, the editor will flag the line with a little italic i when you forget the semi-colon, but you may not notice this because the code will often still run. As you learn to write more complicated statements, however, forgetting to include a semi-colon can torpedo your code from executing. So it is good practice to think of the semi-colon as the period of a sentence and always end with it. Code documentation It is good practice to write comments to document every statement in your code. This will help you read your code if you put a script down for a period of time and return to it later. It will also help other people read and borrow from your code or help you troubleshoot errors. In this course, I will ask you to follow three principles of good coding practice with respect to documenting your scripts. 1. Write a comment for each statement that briefly describes its purpose. 2. Always document code with complete sentences that end with periods. 3. Write concise 1-2 sentence statements, not long-winded paragraphs. Print to console panel As you write code, it is good practice to check your work as you go. A common way of doing this is to print variables and then look at the result in the Console panel of the Code Editor. // Use print function and pass a string to it by enclosing in parentheses. print ( 'This string will print in the Console panel.' ); // You can also pass a variable to the print function. print ( kid_names ); // Use commas to pass more than one item to the print function. print ( \"my kids\" , kid_names ); Functions The print command is an example of a function , or something more like a verb than a noun. A function can take arguments and deliver results. As illustrated by the print function, you pass arguments to a function by enclosing them in parentheses. Object methods Every object class will have a set of functions that you can call to manipulate instances of that object class. These are often called methods . Here is an example for a list object. // Use a period and parentheses to call a METHOD of an object (a function that works with an object). var kids_reversed = kid_names . reverse (); print ( 'Kids in reverse order' , kids_reversed ); Earth Engine objects All the examples above deal with native JavaScript objects and methods. Earth Engine provides a large set of additional objects and methods for working with geographic information. In the chapters that follow, you will learn how to use JavaScript to access these tools. The syntax and grammar for using these tools are largely the same as what we have discussed above. If this all feels new and slightly overwhelming, please do not stress out. It will become familiar through practice. Adapted from Earth Engine 101 by Dave Thau .","title":"Javascript 101"},{"location":"00/code/javaScript101/#javascript-101","text":"The Code Editor lets you work with Earth Engine by writing code in JavaScript. Like any language, JavaScript has a vocabulary and grammar that you will learn through practice. This page introduces some key terms, syntax, and contexts for writing code with JavaScript. Like most of the pages tagged with in this primer, this page contains code snippets that you can copy and paste into the code editor to see their effects. To do this, open the code editor , copy and past the code snippets into it, and click RUN to execute the snippet. You can also save you script as a file by clicking the SAVE button. The file name should not have spaces and should end with .js to mark that it is a JavaScript file (e.g. javascript101.js ).","title":"JavaScript 101"},{"location":"code/cart/community_palette/","text":"Introduction ee-palettes is a resource created by Gennadii Donchyts, Fedor Baart & Justin Braaten. You can access this resource through the project\u2019s GitHub repository . The README file (the web page that is displayed when you click the GitHub link) thoroughly documents how to work with the resource. The snippets below aim to provide some additional support for EE newbies. 1. Load the module. In this community resource, palettes are contained in an Earth Engine module , or a collection of data objects that can be called from other scripts. This follows the modular programming paradigm that seeks to break complex tasks into simple, reusable sub-tasks. Many EE problems require the coder to create a palette to call as a visualization parameter. So rather than force all coders to do this from scratch each time, Gena, Fedor, and Justin wrote up a module that contains many different solutions to this task. Then they very kindly shared this module (for no cost) with the broader EE user community. The snippet below loads the ee-palettes module into a variable called palettes . // Load ee-palettes module. var palettes = require ( 'users/gena/packages:palettes' ); 2. Define a palette. The ee-palettes module contains a dictionary composed of sub-dictionaries, lists, and strings. To see this, run this snippet. // Inspect the module. print ( 'community palettes' , palettes ); In the Console, you should see a dictionary structured like this: Module dictionary Color schemes Palette groups Color lists To select a color list for your palette, use the following pattern: // Pattern for selecting palettes. var palette_name = // You can name this anything you want. module_dictionary . // Whatever you named the variable when you loaded the module. color_scheme . // Schemes organized by source (e.g. ColorBrewer) and sometimes data constraints (e.g. qualitative, sequential, diverging). palette_group // A general range of colors displayed by a palette. [ color_list_index ] // The list of colors used to display the palette (akin to the palette's 'resolution'). ; For example: // Define a palette. var palette = // Whatever you want to name the palette. palettes . // Whatever you named the variable when you loaded the module. cmocean . // Name of the color scheme. Deep // Name of the palette group. [ 7 ] // Index number of color list. ; print ( 'palette' , palette ); Typically, you call the palette in one line: // Define a palette. var palette = palettes . cmocean . Deep [ 7 ]; print ( 'palette' , palette ); 3. Invert the palette. Sometimes you will need to reverse the order of colors in a color list. For example, the Red-Blue diverging ColorBrewer scheme is inherently ordered from red to blue. But what if you want to use this palette to display temperature anomalies, where negative numbers represent cooler than normal temperatures and positive values represent warmer than normal temperatures? Good cartographic practice aims to make the visual display of data intuitive for the map reader. To use cool colors for cool temperatures and warm colors for warm temperatures, you can reverse the color list. // Define palette and reverse the color list. var palette_reversed = palettes . colorbrewer . RdBu [ 9 ]. reverse (); print ( 'palette reversed' , palette_reversed ); Compare the result from above with the original palette to check your understanding. // Original palette. var palette_original = palettes . colorbrewer . RdBu [ 9 ]. reverse (); print ( 'palette original' , palette_original ); 4. Slice the palette. Sometimes you will want to use a portion of the palette, but not the entire palette. For example, imagine that you would like to map concentrations of chlorophyll-a. You find a palette from the niccoli color scheme that transitions from dark blue to green through a yellow-peach to white. For your map, you would like the palette to end at the green color and not extend into the yellow and white bit. To do this, you can slice the palette: // Slice the palette to use a portion of it. var palette_sliced = palettes . niccoli . linearl [ 7 ]. slice ( 0 , 5 ); print ( 'palette_sliced' , palette_sliced ); To understand how slice works, run this code to print the original palette. // Compare to original palette. var palette_full = palettes . niccoli . linearl [ 7 ]; print ( 'palette full' , palette_full ); Look at the Console panel and note that the original list contains seven (7) elements. Now expand the list (click on the carrot) so that you can see the color codes. Note that each color code in the list has an index number . The first color has the index 0 and the last color has the index 6. Now look at your sliced palette in the Console panel. It contains five (5) elements. The two arguments in the .slice() method defined the start and end of this subset. Note that the first number is inclusive and the second one is exclusive . That means when you say .slice(0,5) the result will include the index 0 but exclude the index 5 or greater. Resources for good cartographic practice Here are more resources for choosing color palettes for data visualization: ColorBrewer Scientific color maps Why we use bad color maps and what you can do about it Color in a perceptually uniform way","title":"Community palette module"},{"location":"code/cart/community_palette/#introduction","text":"ee-palettes is a resource created by Gennadii Donchyts, Fedor Baart & Justin Braaten. You can access this resource through the project\u2019s GitHub repository . The README file (the web page that is displayed when you click the GitHub link) thoroughly documents how to work with the resource. The snippets below aim to provide some additional support for EE newbies.","title":"Introduction"},{"location":"code/cart/community_palette/#1-load-the-module","text":"In this community resource, palettes are contained in an Earth Engine module , or a collection of data objects that can be called from other scripts. This follows the modular programming paradigm that seeks to break complex tasks into simple, reusable sub-tasks. Many EE problems require the coder to create a palette to call as a visualization parameter. So rather than force all coders to do this from scratch each time, Gena, Fedor, and Justin wrote up a module that contains many different solutions to this task. Then they very kindly shared this module (for no cost) with the broader EE user community. The snippet below loads the ee-palettes module into a variable called palettes . // Load ee-palettes module. var palettes = require ( 'users/gena/packages:palettes' );","title":"1. Load the module."},{"location":"code/cart/community_palette/#2-define-a-palette","text":"The ee-palettes module contains a dictionary composed of sub-dictionaries, lists, and strings. To see this, run this snippet. // Inspect the module. print ( 'community palettes' , palettes ); In the Console, you should see a dictionary structured like this: Module dictionary Color schemes Palette groups Color lists To select a color list for your palette, use the following pattern: // Pattern for selecting palettes. var palette_name = // You can name this anything you want. module_dictionary . // Whatever you named the variable when you loaded the module. color_scheme . // Schemes organized by source (e.g. ColorBrewer) and sometimes data constraints (e.g. qualitative, sequential, diverging). palette_group // A general range of colors displayed by a palette. [ color_list_index ] // The list of colors used to display the palette (akin to the palette's 'resolution'). ; For example: // Define a palette. var palette = // Whatever you want to name the palette. palettes . // Whatever you named the variable when you loaded the module. cmocean . // Name of the color scheme. Deep // Name of the palette group. [ 7 ] // Index number of color list. ; print ( 'palette' , palette ); Typically, you call the palette in one line: // Define a palette. var palette = palettes . cmocean . Deep [ 7 ]; print ( 'palette' , palette );","title":"2. Define a palette."},{"location":"code/cart/community_palette/#3-invert-the-palette","text":"Sometimes you will need to reverse the order of colors in a color list. For example, the Red-Blue diverging ColorBrewer scheme is inherently ordered from red to blue. But what if you want to use this palette to display temperature anomalies, where negative numbers represent cooler than normal temperatures and positive values represent warmer than normal temperatures? Good cartographic practice aims to make the visual display of data intuitive for the map reader. To use cool colors for cool temperatures and warm colors for warm temperatures, you can reverse the color list. // Define palette and reverse the color list. var palette_reversed = palettes . colorbrewer . RdBu [ 9 ]. reverse (); print ( 'palette reversed' , palette_reversed ); Compare the result from above with the original palette to check your understanding. // Original palette. var palette_original = palettes . colorbrewer . RdBu [ 9 ]. reverse (); print ( 'palette original' , palette_original );","title":"3. Invert the palette."},{"location":"code/cart/community_palette/#4-slice-the-palette","text":"Sometimes you will want to use a portion of the palette, but not the entire palette. For example, imagine that you would like to map concentrations of chlorophyll-a. You find a palette from the niccoli color scheme that transitions from dark blue to green through a yellow-peach to white. For your map, you would like the palette to end at the green color and not extend into the yellow and white bit. To do this, you can slice the palette: // Slice the palette to use a portion of it. var palette_sliced = palettes . niccoli . linearl [ 7 ]. slice ( 0 , 5 ); print ( 'palette_sliced' , palette_sliced ); To understand how slice works, run this code to print the original palette. // Compare to original palette. var palette_full = palettes . niccoli . linearl [ 7 ]; print ( 'palette full' , palette_full ); Look at the Console panel and note that the original list contains seven (7) elements. Now expand the list (click on the carrot) so that you can see the color codes. Note that each color code in the list has an index number . The first color has the index 0 and the last color has the index 6. Now look at your sliced palette in the Console panel. It contains five (5) elements. The two arguments in the .slice() method defined the start and end of this subset. Note that the first number is inclusive and the second one is exclusive . That means when you say .slice(0,5) the result will include the index 0 but exclude the index 5 or greater.","title":"4. Slice the palette."},{"location":"code/cart/community_palette/#resources-for-good-cartographic-practice","text":"Here are more resources for choosing color palettes for data visualization: ColorBrewer Scientific color maps Why we use bad color maps and what you can do about it Color in a perceptually uniform way","title":"Resources for good cartographic practice"},{"location":"code/cart/feature_outlines/","text":"It is often desirable to symbolize the perimeters of features without showing their interiors. You can do this with the .style() method. This involves three steps: Define style parameters. Initialize map layer widget and apply style parameters. Add the map layer widget to the map. // Define style parameters for feature collection layer. var style_fc = { color : 'string' , // Stroke (perimeter) color as string ('white'). width : number , // Stroke (perimeter) width as number (0.5) fillColor : 'FFFFFF00' // Fill (interior) color with opacity channel set to 00. }; // Initialize map layer as a widget. // Call style_regions with .style method. // Label the layer 'Regions;. var fc_layer = ui . Map . Layer ( regions . style ( style_fc ), // Call style parameters with .style method. {}, // Viz parameters as empty object. 'string' , // Name of the layer (string). true // Show by default (true or false) ); // Add map layer widget to left map. left_map . add ( regions_layer ); // Note: you use .add() method, not .addLayer() method.","title":"Show feature outlines without fills"},{"location":"code/cart/gradient_legend/","text":"// Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Construct gradient legend. var legend = cart // module . makeGradientLegend ( // function viz_params , // viz parameters 'Legend title' , // legend title 'bottom-left' // position on map ) ; // Add legend to map. Map . add ( legend );","title":"Gradient legend"},{"location":"code/cart/palettes_mb/","text":"One palette, one band. That is the simple mantra. If your image only contains one band, then there\u2019s no problem. EE will assume you want to display that one. If your image contains more than one band and your viz parameters includes a palette, then EE will throw you an error because it wants to know which band to display with that palette. One way to solve this problem is to specify the band in the viz parameters. // Viz parameter pattern for multiband images and one palette. var viz = { min : number , // The min data value to display. max : number , // The max data value to display. bands : 'band name' , // The band name as a string. palette : palette_name // The palette name (called by variable's name) }; Heads-up : the bands key in the viz dictionary is plural ( bands ) even though you only specify a single band.","title":"Palettes for multiband images"},{"location":"code/cart/qualitative_legend/","text":"// Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Make labels for nominal image values. var qualitative_labels = [ ] ; // Construct legend for HOLC reference map. var legend = cart . makeLegend ( 'Legend label' , // Label for legend palette , // Palette for image qualitative_labels , // labels to display 'bottom-left' // Where you want to put legend on the map. ); // Add legend to map. Map . add ( legend );","title":"Qualitative legend"},{"location":"code/cart/widget_label/","text":"You can create label widgets to place titles, instructions, credits, and other verbal information on maps and panels. The code below illustrates the two basic steps to creating label widgets: Define the style parameters for the label. Many of the style parameters refer to the CSS box model . Initialize the label widget and apply the style parameters. // Initialize style parameters for title labels. // Name the variable: title_style var output_style = { fontSize : 'string' , // Size in unis pixels as string ('24px') fontWeight : 'string' , // Use 'bold' for titles, otherwise you can comment out. padding : 'string' , // Provides breathing room with css box model. Size in pixels as string ('4px') whiteSpace : 'pre' , // If you want to define line breaks manually with \\n, otherwise comment this out. backgroundColor : 'string' // Default is white. Comment out if white works. Change if you want to add highlighter effect. } ; // Initialize a label widget and apply title_style. // Name the variable: title var output_label = ui . Label ({ value : 'the label content' , // Content of the label style : output_style // Style defined in step above. // targetUrl:, // URL as string if you want a hyperlink, otherwise comment out. } ); You can then display a label widget either by printing to console or adding to a map or panel widget. This will print the label to the Console of the Code Editor: print ( output_label ); This will add the label to a panel widget. Adding to a map widget follows same // Add label to panel widget. some_panel_widget . add ( output_label ) You can chain the .add() methods to add more than one widget to the panel.","title":"Label widget"},{"location":"code/cart/widget_map/","text":"The code below will initialize a new map widget. You can add layers to this object, change the map center, and change the basemap, just like you have done with a big m Map object. The only difference now is that you call the name of the map variable you defined, rather than Map, when applying these methods. // Initialize a map widget. var output = ui . Map (); // Set the base map. output . setOptions ( 'SATELLITE' ); // Or alternatively you can chain these two steps. // Initialize a map widget and set base map. var output = ui . Map () . setOptions ( 'SATELLITE' );","title":"Map widget"},{"location":"code/cart/widget_panel/","text":"This is the simplest way to create a panel widget: var output = ui . Panel (); You can style the panel based on the CSS box model . For example, the code below defines the width of the panel (as 20% of the display screen). var output = ui . Panel ({ style : { width : '20%' } });","title":"Panel widget"},{"location":"code/cart/widget_swipe/","text":"A swipe map is a popular way to compare two layers and an alternative to changing the visibility or opacity of two overlying layers. The code below creates a swipe between two maps with a split panel. This involves two steps: Initialize a linker widget. This will allow you to change the map center and zoom in one map that controls the map center and zoom of a linked map. This allows the user to pan and zoom while keeping the geographic extent of the two maps consistent. Initialize a split panel to hold the two maps and setting the wipe option to true . Clear root and then add the split panel to it. // Initialize a map linker widget and link two maps. var map_linker = ui . Map . Linker ([ left_map , right_map ]); // Initialize a spilt panel widget to hold the two linked maps. // Define the orientation as 'horizontal' and wipe as true. var split_panel = ui . SplitPanel ({ firstPanel : map_linker . get ( 0 ), secondPanel : map_linker . get ( 1 ), orientation : 'horizontal' , wipe : true , } ); // Clear root. // Then add split panel to root. ui . root . clear (); ui . root . add ( split_panel );","title":"Swipe maps"},{"location":"code/fc/aggregate_array/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // Inspect all the unique values for one property of features in the collection. // ---------------------------------------------------------------------------- // 1. Get list. var value_list = point_collection . aggregate_array ( 'name' ) . distinct () . sort () ; // 2. Print list. print ( 'list of values' , value_list );","title":"Inspect unique values of one property"},{"location":"code/fc/convert_fc_to_binary/","text":"Introduction This procedure begins with a feature collection and produces a binary raster, or a raster that shows the presence/absence of the features in the feature collection, where the cell contains the value 1 if it was covered by a feature\u2019s geometry and the value 0 if it was not covered. The procedure consists of two tasks and each task consists of three steps. Prep feature collection The first task prepares a feature collection for conversion. Write a function to give a feature a numeric property. Apply the function to every feature in a feature collection. Check to see if step 2 worked as you expected. You should see that each feature in the feature collection has a property and value that correspond with the .set() arguments in the function. // PREPARE FEATURE COLLECTION // Write a function to give a feature a property named 'tag' and a specified value. var tag_features = function ( feature ) { return feature . set ( { tag : 1 } // Property name and specified value ); } ; // Apply function to all features in a feature collection (fc). var tagged_fc = fc . map ( tag_features ); // Change fc to the name of feature collection. // Inspect first record of collection. print ( 'tagged fc' , tagged_fc . first ()); Convert feature collection The second task converts a feature collection into an image band. Write a function that creates a raster initialized with zeros, cast the raster as a \u2018byte\u2019 (which makes the raster \u2018smaller\u2019 with respect to storage), and then \u2018paint\u2019 the values from the specified feature property into the raster cells. Apply the function and specify the feature collection (to convert) and property (to paint). Inspect the result. This will print the result to the Console. Click on the carrot to see the list of bands. Because we made the raster with the .constant() method, the name of the band will be \u201cconstant\u201d. // CONVERT FEATURE COLLECTION TO IMAGE BAND // Create a function to convert feature collection to image. var makeImage = function ( fc , property ) { return ee . Image . constant ( 0 ) // Return a raster with zeros . byte () // Store as byte . paint ( fc , property ); // Paint values at locations from property of feature collection (fc). } ; // Call function with feature collection and property arguments. var image_output = makeImage ( tagged_fc , 'tag' ); // Inspect the result. print ( 'image from fc' , image_output );","title":"Convert to binary image"},{"location":"code/fc/convert_fc_to_binary/#introduction","text":"This procedure begins with a feature collection and produces a binary raster, or a raster that shows the presence/absence of the features in the feature collection, where the cell contains the value 1 if it was covered by a feature\u2019s geometry and the value 0 if it was not covered. The procedure consists of two tasks and each task consists of three steps.","title":"Introduction"},{"location":"code/fc/convert_fc_to_binary/#prep-feature-collection","text":"The first task prepares a feature collection for conversion. Write a function to give a feature a numeric property. Apply the function to every feature in a feature collection. Check to see if step 2 worked as you expected. You should see that each feature in the feature collection has a property and value that correspond with the .set() arguments in the function. // PREPARE FEATURE COLLECTION // Write a function to give a feature a property named 'tag' and a specified value. var tag_features = function ( feature ) { return feature . set ( { tag : 1 } // Property name and specified value ); } ; // Apply function to all features in a feature collection (fc). var tagged_fc = fc . map ( tag_features ); // Change fc to the name of feature collection. // Inspect first record of collection. print ( 'tagged fc' , tagged_fc . first ());","title":"Prep feature collection"},{"location":"code/fc/convert_fc_to_binary/#convert-feature-collection","text":"The second task converts a feature collection into an image band. Write a function that creates a raster initialized with zeros, cast the raster as a \u2018byte\u2019 (which makes the raster \u2018smaller\u2019 with respect to storage), and then \u2018paint\u2019 the values from the specified feature property into the raster cells. Apply the function and specify the feature collection (to convert) and property (to paint). Inspect the result. This will print the result to the Console. Click on the carrot to see the list of bands. Because we made the raster with the .constant() method, the name of the band will be \u201cconstant\u201d. // CONVERT FEATURE COLLECTION TO IMAGE BAND // Create a function to convert feature collection to image. var makeImage = function ( fc , property ) { return ee . Image . constant ( 0 ) // Return a raster with zeros . byte () // Store as byte . paint ( fc , property ); // Paint values at locations from property of feature collection (fc). } ; // Call function with feature collection and property arguments. var image_output = makeImage ( tagged_fc , 'tag' ); // Inspect the result. print ( 'image from fc' , image_output );","title":"Convert feature collection"},{"location":"code/fc/convert_to_image/","text":"This workflow creates a nominal image from a feature collection, or an image where each pixel value represents a class, category, or name. It assumes that each feature in the collection has a property that contains integers, where each integer represents the class, category, or name of the location. The workflow involves two steps: Create a function that takes a feature collection and a property of features in the collection as arguments and then returns an image where each pixel value holds the specified property of the feature at that location. Apply the function by calling it and naming the two arguments. The output will be an image with a single band (named \u2018constant\u2019). Any pixel that does not correspond to a feature in the collection will be masked. // Create a function to convert feature collection to image. var makeImage = function ( fc , property ) { return ee . Image () // Create empty image . byte () // Store as byte . paint ( fc , property ); // Paint values at locations from property of feature collection (fc). } ; // Use function to convert a feature collection to an image. var output = // Name output variable makeImage ( // Call function from above fc_name , // feature collection 'property_name' // property of fc to use as pixel values ) ;","title":"Convert to nominal (class) image"},{"location":"code/fc/filterBounds/","text":"If you have completed the buffer every feature in a collection code, then you can then do the following: // ---------------------------------------------------------------------------- // Filter for features that intersect another feature collection. // ---------------------------------------------------------------------------- var points_in_buffers = point_collection . filterBounds ( buffers ) ; Map . addLayer ( points_in_buffers , { color : 'magenta' }, 'Points in buffers from name filter' , 0 );","title":"Filter for features that interset feature collection"},{"location":"code/fc/filterBounds_geometry/","text":"If you have completed the buffer a feature code, then you can then do the following: // ---------------------------------------------------------------------------- // Filter for features that intersect another feature's geometry. // ---------------------------------------------------------------------------- var points_in_buffer = point_collection . filterBounds ( buffer . geometry ()) ; Map . addLayer ( points_in_buffer , { color : 'red' }, 'Points in buffer' , 0 );","title":"Filter for features that intersect geometry"},{"location":"code/fc/filter_eq/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // Filter collection by a nominal attribute. // ---------------------------------------------------------------------------- var points_filtered_by_name = point_collection . filter ( ee . Filter . eq ( 'name' , 'S' )) ; print ( 'selected by nominal attribute' , points_filtered_by_name ); Map . addLayer ( points_filtered_by_name , { color : 'yellow' }, 'Selected by name' , 0 );","title":"Filter by nominal attribute"},{"location":"code/fc/filter_gt/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // Filter collection by a numeric attribute. // ---------------------------------------------------------------------------- var points_filtered_by_number = point_collection . filter ( ee . Filter . gt ( 'number' , 1 )) ; print ( 'selected by numeric attribute' , points_filtered_by_number ); Map . addLayer ( points_filtered_by_number , { color : 'cyan' }, 'Selected by number' , 0 );","title":"Filter by numeric attribute"},{"location":"code/fc/map_area/","text":"If you have completed buffer every feature in a collection , then you can then do the following: // ---------------------------------------------------------------------------- // To compute area of every feature in a collection: // ---------------------------------------------------------------------------- // 1. Define a function. var compute_area = function ( feature ) { var area_feature = feature . area (); var features_with_area = feature . set ({ area : area_feature }); return features_with_area ; }; // 2. Map function over collection. var buffers_with_area = buffers . map ( compute_area ); print ( 'buffers with area' , buffers_with_area );","title":"Compute area of every feature in collection"},{"location":"code/fc/map_buffer/","text":"If you have completed the vector objects code, then you can then do the following: // ---------------------------------------------------------------------------- // To buffer every feature in a collection: // ---------------------------------------------------------------------------- // 1. Define a function. var buffer_points = function ( point ){ var buffered_point = point . buffer ( 1000 ); return buffered_point ; } ; // 2. Map function over collection. var buffers = points_filtered_by_number . map ( buffer_points ); print ( 'buffer all featuers in collection filtered by number' , buffers ); Map . addLayer ( buffers , { color : 'green' }, 'Buffer all features in collection filtered by number' , 0 );","title":"Buffer every feature in collection"},{"location":"code/fc/map_intersection/","text":"If you have completed the buffer every feature in a collection code, then you can then do the following: // ---------------------------------------------------------------------------- // Intersection between a feature and collection. // ---------------------------------------------------------------------------- // 1. Define a function. var intersect_features = function ( feature ) { var intersection = feature . intersection ( buffer ); return intersection ; } ; // 2. Map function across collection. var intersection_feature_collection = buffers . map ( intersect_features ); print ( 'Intersection from feature collection' , intersection_feature_collection ); Map . addLayer ( intersection_feature_collection , { color : 'black' }, 'Intersection from feature collection' , 0 );","title":"Intersection between a feature and collection"},{"location":"code/fc/print_first/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // Inspect the first feature in the collection. // ---------------------------------------------------------------------------- print ( 'First feature' , point_collection . first ());","title":"Inspect first feature"},{"location":"code/fc/self_check_fc/","text":"Introduction You should learn and employ strategies to self-check your work while you are working on a solution. This involves making requests to visualize a method\u2019s output and either checking the result with known benchmarks or critically thinking: does this look right? Print feature collection to console For feature collections, you can print information to console. For example, the following code will print the first feature in the collection and the number of features in the collection. // Print information about a feature collection. print ( 'fc check' , // Label to print to console. fc . first (), // First record in collection. fc . size () // Number of records in collection. ) ; Print nominal values of one property to console You can run the code above to see the properties associated with each feature in the collection. With this information, you can then summarize the values for one property. When the property contains nominal values (words), you can quickly print a list of all the unique values. // Print unique values of a specified property. print ( 'fc property check' , // Label to print to console. fc . aggregate_array ( 'property name' ) // Return all the values for this property. . distinct () // Filter to return only unique values. . sort () // Sort the list alphabetically. ) ; Print numerical values of one property to console When the property contains numerical values , you can quickly print a summary statistic for a specified property. print ( 'fc numerical property check' , // Label to print to console. fc . aggregate_min ( 'property name' ), // Print min value of specified property. fc . aggregate_max ( 'property name' ) // Print max value of specified property. ) ; Add collection as layer to map To quickly add a feature collection as a map layer, you can define the viz parameters as an empty object and still label the layer and define the shown parameter as false (or 0). // Add feature collection to map as a layer for a quick check. Map . addLayer ( fc , {}, 'fc quick check' , 0 ); Comment out after checking After you have used these methods to check your work, you should comment out the lines to prevent them from running each time you run your code. This will help your code run more efficiently.","title":"Check feature collection outputs during workflow"},{"location":"code/fc/self_check_fc/#introduction","text":"You should learn and employ strategies to self-check your work while you are working on a solution. This involves making requests to visualize a method\u2019s output and either checking the result with known benchmarks or critically thinking: does this look right?","title":"Introduction"},{"location":"code/fc/self_check_fc/#print-feature-collection-to-console","text":"For feature collections, you can print information to console. For example, the following code will print the first feature in the collection and the number of features in the collection. // Print information about a feature collection. print ( 'fc check' , // Label to print to console. fc . first (), // First record in collection. fc . size () // Number of records in collection. ) ;","title":"Print feature collection to console"},{"location":"code/fc/self_check_fc/#print-nominal-values-of-one-property-to-console","text":"You can run the code above to see the properties associated with each feature in the collection. With this information, you can then summarize the values for one property. When the property contains nominal values (words), you can quickly print a list of all the unique values. // Print unique values of a specified property. print ( 'fc property check' , // Label to print to console. fc . aggregate_array ( 'property name' ) // Return all the values for this property. . distinct () // Filter to return only unique values. . sort () // Sort the list alphabetically. ) ;","title":"Print nominal values of one property to console"},{"location":"code/fc/self_check_fc/#print-numerical-values-of-one-property-to-console","text":"When the property contains numerical values , you can quickly print a summary statistic for a specified property. print ( 'fc numerical property check' , // Label to print to console. fc . aggregate_min ( 'property name' ), // Print min value of specified property. fc . aggregate_max ( 'property name' ) // Print max value of specified property. ) ;","title":"Print numerical values of one property to console"},{"location":"code/fc/self_check_fc/#add-collection-as-layer-to-map","text":"To quickly add a feature collection as a map layer, you can define the viz parameters as an empty object and still label the layer and define the shown parameter as false (or 0). // Add feature collection to map as a layer for a quick check. Map . addLayer ( fc , {}, 'fc quick check' , 0 );","title":"Add collection as layer to map"},{"location":"code/fc/self_check_fc/#comment-out-after-checking","text":"After you have used these methods to check your work, you should comment out the lines to prevent them from running each time you run your code. This will help your code run more efficiently.","title":"Comment out after checking"},{"location":"code/fc/table_widget/","text":"So far, we have seen how to print attributes of features and feature collections to the Console. This is helpful when working through a solution and producing an answer to the problem, but less helpful when you want to share the results of your work with a general audience. In Earth Engine, you can show attributes of a feature collection as a table widget , also called a user-interface (ui) object. In the example below, we still just print the table to the Console, but eventually we will add the table to the map layout. The snippet below assumes that you have worked through the starter script for the River Corridor Easement Outreach problem and have the following: You have a feature collection called large_owners (change this name to whatever you called your final result), Each feature in this collection has a property called \u2018OWNER1\u2019, Each feature also has a property called \u2018ACRES\u2019 that you want to sort the table by in descending order. // -------------------------------------------------------------------------------- // Display attributes of feature collection as a table. // -------------------------------------------------------------------------------- // In this example, large_owners contains the parts of parcels that overlap corridor with acres as an attribute. var results = ui . Chart . feature . byFeature ( large_owners , 'OWNER1' ); results . setChartType ( 'Table' ); results . setOptions ( { allowHtml : true , // Formatted values of cells with html tags will be rendered as HTML. pageSize : 50 , // The number of rows to show in each page. frozenColumns : 1 , // The number of columns from the left to 'freeze' when scrolling horizontally. Note that this works in the Console, but not when you open the table in a new browser panel. sort : 'enable' , // Users can click on a header to sort table by that column. sortAscending : false , // True = sort ascending; False = sort descending. sortColumn : 1 // Index of column to sort. 0 is first column, 1 is second. } ) ; results . style (). set ({ stretch : 'both' }); // Print table. print ( 'Land owners to contact' , results );","title":"Display attributes of feature collection as a table"},{"location":"code/fc/union/","text":"If you have completed the buffer every feature in a collection code, then you can then do the following: // ---------------------------------------------------------------------------- // Union features in a feature collection. // ---------------------------------------------------------------------------- var union_feature_collection = buffers . union (); print ( 'Union from feature collection' , union_feature_collection ); Map . addLayer ( union_feature_collection , { color : 'black' }, 'Union from feature collection' , 0 );","title":"Union features in collection"},{"location":"code/features/area/","text":"If you have completed the buffer a feature code, then you can do the following: // ---------------------------------------------------------------------------- // To compute area of a feature. // ---------------------------------------------------------------------------- var area_buffer = buffer . area (); print ( 'area of buffer' , area_buffer ); var buffer_with_area = buffer . set ({ area : area_buffer }); print ( 'buffer with area' , buffer_with_area );","title":"Compute area"},{"location":"code/features/buffer/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // To buffer a feature. // ---------------------------------------------------------------------------- var buffer = point01 . buffer ( 1000 ); print ( 'buffer a feature' , buffer ); Map . addLayer ( buffer , { color : 'blue' }, 'Buffer a feature' , 0 );","title":"Buffer a feature"},{"location":"code/features/intersection/","text":"If you have completed the buffer a feature code, then you can do the following: // ---------------------------------------------------------------------------- // Intersection of two features. // ---------------------------------------------------------------------------- var buffer = point01 . buffer ( 1000 ); var buffer2 = point02 . buffer ( 1000 ); var intersection_features = buffer . intersection ( buffer2 ; print ( 'Intersection of two features' , intersection_features ); Map . addLayer ( intersection_features , { color : 'gray' }, 'Intersection of two features' , 0 );","title":"Intersection of two features"},{"location":"code/features/starter_coords/","text":"// ---------------------------------------------------------------------------- // Vector objects. // ---------------------------------------------------------------------------- // Here are three starter coordinates stored as a dictionary. var coords = { pt1 : [ - 73.168687 , 44.013513 ], pt2 : [ - 73.156242 , 44.006167 ], pt3 : [ - 73.173494 , 44.008142 ] } ; // Here are three geometries constructed from these coordinates. var geometry = ee . Geometry . Point ( coords . pt1 ); var geometry2 = ee . Geometry . Point ( coords . pt2 ); var geometry3 = ee . Geometry . Point ( coords . pt3 ); // Here are three features. var point01 = ee . Feature ( geometry ,{ name : 'S' , number : 1 }); var point02 = ee . Feature ( geometry2 ,{ name : 'O' , number : 2 }); var point03 = ee . Feature ( geometry3 ,{ name : 'S' , number : 3 }); // Here is a feature collection that contains three features. var point_collection = ee . FeatureCollection ([ point01 , point02 , point03 ]); // Inspect the collection. print ( 'Point collection' , point_collection );","title":"Vector objects"},{"location":"code/features/union_right/","text":"If you have completed the vector objects code, you can then do the following: // ---------------------------------------------------------------------------- // Union two features. // ---------------------------------------------------------------------------- var buffer = point01 . buffer ( 1000 ); var buffer2 = point02 . buffer ( 1000 ); var union_features = buffer . union ( buffer2 ); print ( 'Union of two features' , union_features ); Map . addLayer ( union_features , { color : 'gray' }, 'Union from two features' , 0 );","title":"Union two features"},{"location":"code/geometry/calculateLength/","text":"Geometry objects have implicit spatial properties, like length, area, center, etc. Earth Engine provides methods to calculate these for each geometry class. For example, the method .length() calculates the length of a line (or \u2018LineString\u2019) object. To call the method, you append .length() to the variable that stores the line object. The snippet below assumes that you have a line object named \u2018line\u2019. // Calculate the length of a line object. var line_length = line . length () ; // Inspect result. print ( 'route distance' , line_length ); Check your understanding What statement would you write to find the center of the line? Hint: use the Docs tab to look for a method associated with ee.Geometry.LineString that sounds like it would calculate the center of the line.","title":"Calculate line length"},{"location":"code/geometry/constructFeature/","text":"A geometry object represents where , but not what is where. In other words, geometry objects represent geographic locations, but they do not describe conditions or properties of these locations. For example, you can store the location of a town with a geometry object, but not the town\u2019s name, population, area, etc. A feature object stores both the geometry and qualitative and/or quantitative properties of a location. Often, you will hear people who are accustomed to geographic information systems use the word attribute rather than property. For our purposes, they are synonymous. Usually, the attributes of features will be defined as part of a data product. But you can also define and alter attributes as necessary with code. Earth Engine provides a method to make features by calling ee.Feature(). As arguments (between the parentheses), it takes a geometry object and the attributes. The attributes must be defined as a dictionary with a key and a value. You might think of this like a data table: the key is the column title and the value is the row. The example below again assumes that you have a point object named \u2018point\u2019. // Set a property of point. var point_feature = ee . Feature ( point , { name : 'my first point' }); // Print the two different objects to compare. print ( 'Point as geometry' , point , 'Point as feature' , point_feature ); Check your understanding Try to change the name of the feature to 'Gulf of Guinea'. What does the statement now say?","title":"Construct feature from geometry"},{"location":"code/geometry/constructLineGeometry/","text":"A line consists of two or more points. When you stretch a string tightly between two points on a globe, you show the great circle route . This shows the shortest distance across the globe\u2019s surface between the two points. Google Earth Engine uses a spheroid model to represent line objects. The resulting lines represent great circle routes, just like if you were working with a string and a globe. Earth Engine provides a method to construct line objects from two or more points. You call the method with the phrase ee.Geometry.LineString(). It takes a list of points as an argument. // Construct two points. var point = ee . Geometry . Point ([ 0 , 0 ]); var point2 = ee . Geometry . Point ([ - 90 , 60 ]); // Construct line from two points. var line = ee . Geometry . LineString ([ point , point2 ]); // Inspect results. print ( 'Great circle arc' , line ); // Display result as a layer on the map. Map . addLayer ( line , { color : 'red' }, 'Great circle line' ); Check your understanding If this line represents the shortest distance between two points, why does the line bend rather than go straight?","title":"Construct line geometry"},{"location":"code/geometry/constructPointGeometry/","text":"While you can create geometry objects with the drawing buttons in the upper-left of the Map user interface, you can also create them with code. Earth Engine provides a method to make a point from coordinates (x,y). You call the method with the phrase ee.Geometry.Point(). Then you tell it the coordinates in the parentheses. To work, the coordinates need to be in the form of a list. // Construct new point geometry object. var point = ee . Geometry . Point ([ 0 , 0 ]); // Inspect the result. print ( 'Point object' , point ); Check your understanding Which number represents the Prime Meridian Which number represents the Equator?","title":"Construct point geometry"},{"location":"code/geometry/convert_units/","text":"By default, Earth Engine often computes spatial measurements in meters. So length returns meters and area returns square meters. (It is good practice to use the docs tab in the code editor to double-check the units returned by your method.) You can convert these units into meters through simple math operations of number objects. The example below converts meters to kilometers. The snippet below assumes that you have a line object named \u2018line\u2019. // Calculate the length of a line object. var line_length_km = line . length () . divide ( 1000 ) ; // Inspect result. print ( 'route distance kilometers' , line_length_km ); Check your understanding How would you convert meters to miles?","title":"Convert units"},{"location":"code/ic/add_bands/","text":"// Add bands to an image. var output = // Image that you will produce. image_01 // image_01 with one or more bands to add first to output . addBands ( image_02 ) // image_02 with one or more bands to add second to output ; // Example var change_stack = year_one . addBands ( year_two ) ;","title":"Add bands"},{"location":"code/ic/cloud_masks/","text":"Most images in the earth engine data catalog include a quality control band that describes the degree to which each pixel in the image may be affected by clouds. The \u2018image_tools\u2019 module that I wrote includes custom functions that you can call to use the band information and mask pixels that are affected by clouds directly or indirectly (the shadows that clouds cast). This can be helpful for generating cloud-free images for visualization purposes. To use these tools, first load the module and then map the appropriate function over every image in the collection. The snippet below provides general examples. In the examples, the \u2018input\u2019 variable is an image collection. The output will also be an image collection. // Load image_tools module to access cloud masks. var tools = require ( \"users/jhowarth/eePrimer:modules/image_tools.js\" ); // Apply cloud mask to every image in a MODIS collection. var output = input . map ( tools . cloudMask_MODIS ); // Apply cloud mask to every image in a Sentinel 2 collection. var output = input . map ( tools . cloudMask_S2 );","title":"Mapping cloud masks"},{"location":"code/ic/filter_by_dates/","text":"Introduction An image collection often contains a time series , or a number of observations that are collected at some time interval over a duration of time. The interval between observations represents the temporal resolution of the collection and the duration of time represents the temporal extent . In most cases, you will want to work with a subset of images in an image collection based on a time window defined by your research question. You can filter the collection based on time constraints in a number of ways. Filter by date range The ee.Filter.date() method will filter an image collection by a start and end date. The start date is inclusive and the end date is exclusive . This means that a record with the start date will be included in the output, but a record that matches the end date will not. Both dates are strings in the format \u2018YYYY-MM-DD\u2019. For example, this snippet will filter an image collection for all records in January 2000. // Filter image collection with start and end dates. var output = input // Name of output and input . filter ( // Call the filter method. ee . Filter . date ( '2000-01-01' , '2000-02-01' ) // Call this particular filter with start and end dates. ) ; This method is used so frequently that EE provides a shorthand. // Shorthand for filtering image collection with start and end dates. var output = input . filterDate ( '2000-01-01' , '2000-02-01' ); Filter by calendar range Often your research question will concern a season or portion of time that recurs every year. For example, to study sea conditions during a hurricane season in the Atlantic, you would could filter for records between August and October. If you are studying one hurricane season, you could use the date range method. But if you wanted to make comparisons between hurricane seasons, then the date range method becomes laborious. For seasonal windows, it is often helpful to use the ee.Filter.calendarRange() method. This takes three arguments: start, end, and calendar unit. The first two are integers. The last is a string. And to make things fun, the start and end arguments are now both inclusive . For example, this snippet filters an image collection for all records between the years 2000 and 2010. Because the end number is inclusive, 2010 records will be returned in the output. // Filter image collection by year calendar range. var output = input . filter ( ee . Filter . calendarRange ( 2000 , 2010 , 'year' )) This snippet filters an image collection for all records between August and October of any year. Because the end number is inclusive, October records will be returned in the output. // Filter image collection by month calendar range. var output = input . filter ( ee . Filter . calendarRange ( 8 , 10 , 'month' )) You can also chain these filters together. This snippet filters an image collection for all records between August and October for the years between 2000 and 2010. // Filter image collection by year and month calendar ranges. var output = input . filter ( ee . Filter . calendarRange ( 2000 , 2010 , 'year' )) . filter ( ee . Filter . calendarRange ( 8 , 10 , 'month' ))","title":"Filter by dates"},{"location":"code/ic/filter_by_dates/#introduction","text":"An image collection often contains a time series , or a number of observations that are collected at some time interval over a duration of time. The interval between observations represents the temporal resolution of the collection and the duration of time represents the temporal extent . In most cases, you will want to work with a subset of images in an image collection based on a time window defined by your research question. You can filter the collection based on time constraints in a number of ways.","title":"Introduction"},{"location":"code/ic/filter_by_dates/#filter-by-date-range","text":"The ee.Filter.date() method will filter an image collection by a start and end date. The start date is inclusive and the end date is exclusive . This means that a record with the start date will be included in the output, but a record that matches the end date will not. Both dates are strings in the format \u2018YYYY-MM-DD\u2019. For example, this snippet will filter an image collection for all records in January 2000. // Filter image collection with start and end dates. var output = input // Name of output and input . filter ( // Call the filter method. ee . Filter . date ( '2000-01-01' , '2000-02-01' ) // Call this particular filter with start and end dates. ) ; This method is used so frequently that EE provides a shorthand. // Shorthand for filtering image collection with start and end dates. var output = input . filterDate ( '2000-01-01' , '2000-02-01' );","title":"Filter by date range"},{"location":"code/ic/filter_by_dates/#filter-by-calendar-range","text":"Often your research question will concern a season or portion of time that recurs every year. For example, to study sea conditions during a hurricane season in the Atlantic, you would could filter for records between August and October. If you are studying one hurricane season, you could use the date range method. But if you wanted to make comparisons between hurricane seasons, then the date range method becomes laborious. For seasonal windows, it is often helpful to use the ee.Filter.calendarRange() method. This takes three arguments: start, end, and calendar unit. The first two are integers. The last is a string. And to make things fun, the start and end arguments are now both inclusive . For example, this snippet filters an image collection for all records between the years 2000 and 2010. Because the end number is inclusive, 2010 records will be returned in the output. // Filter image collection by year calendar range. var output = input . filter ( ee . Filter . calendarRange ( 2000 , 2010 , 'year' )) This snippet filters an image collection for all records between August and October of any year. Because the end number is inclusive, October records will be returned in the output. // Filter image collection by month calendar range. var output = input . filter ( ee . Filter . calendarRange ( 8 , 10 , 'month' )) You can also chain these filters together. This snippet filters an image collection for all records between August and October for the years between 2000 and 2010. // Filter image collection by year and month calendar ranges. var output = input . filter ( ee . Filter . calendarRange ( 2000 , 2010 , 'year' )) . filter ( ee . Filter . calendarRange ( 8 , 10 , 'month' ))","title":"Filter by calendar range"},{"location":"code/ic/filter_by_image_property/","text":"This pattern is similar to that for filtering feature collections by properties. The general form is: var output = ic . filter ( ee . Filter . __ ( 'property name' , value )) ; For example, this snippet filters a Landsat 8 image collection for all images with less than 20 percent cloud cover. var output = L8_collection . filter ( ee . Filter . lt ( 'CLOUD_COVER' , 20 )) ;","title":"Filter by image property"},{"location":"code/ic/filter_by_location/","text":"// Filter an image collection by a location. var output_ic = input_ic . filterBounds ( geometry ) ;","title":"Filter by location"},{"location":"code/ic/inspect_ic/","text":"This pattern is similar to that for working with feature collections. An image collection usually contains a lot of images, so if you try to inspect the collection with the print() method, EE will often throw you an error because you are asking it to do too much work. Imagine if you asked a librarian to go into the stacks and bring you every issue that has ever been published by a journal or magazine. They too would likely turn red and balk at the request. So to politely get a sense of an image collection\u2019s contents, a good strategy is to print the first record and the number of records in the collection. // Inspect first record and print size (number of images) of collection. print ( 'label' , // Label for this request that is printed to Console. input . first (), // The first record in the image collection. input . size () // The number of records in the image collection. );","title":"Inspect image collection"},{"location":"code/ic/landsat_scaling_function/","text":"// A function to apply scaling factors for Landsat collections. function applyScaleFactors ( image ) { var opticalBands = image . select ( 'SR_B.' ). multiply ( 0.0000275 ). add ( - 0.2 ); var thermalBand = image . select ( 'ST_B.*' ). multiply ( 0.00341802 ). add ( 149.0 ); return image . addBands ( opticalBands , null , true ) . addBands ( thermalBand , null , true ); }","title":"Scale landsat collections"},{"location":"code/ic/load_ic/","text":"This pattern resembles that for loading feature collections and images. You name a variable for the output and use the ee.ImageCollection() method. This takes the asset id as an argument and the asset id must be a string. // Load image collection. var ic_name = ee . ImageCollection ( 'address' );","title":"Load image collection"},{"location":"code/ic/reduce_local_operation/","text":"An image collection often contains stacks of images where the images in the collection overlap each other. You can reduce the stack of images to a single image with a statistical operator. For example, if you have a stack of images in a collection where each image has bands that record the sea surface temperature and chlorophyll-a concentrations of a region over time, you can compute the mean value of each pixel in each band with the .mean() method. // Reduce image collection into an image that represents the mean value for each pixel in the collection. var output = input . mean (); If you explore the EE docs for image collections, you should be able to spot a number of these statistical and math reducers, including: .mean() .median() .max() .min() .product() .sum() All of these take an image collection as an input and reduce it to a single image. They also all do this operation for each band of every image.","title":"Reduce with local operations"},{"location":"code/ic/select_bands/","text":"If you only need to work with a subset of bands from an image (or from every image in an image collection), it is good practice to select the bands early in your workflow. This lightens the load of your computation, which can help reduce your carbon footprint and save you time. Use the .select() method with the band name as an argument. // Select band from an image (or image collection) var output = input . select ( 'band_name' ); To select more than one band from the image, use a list of strings as the argument. // Select multiple bands from an image (or image collection) var output = input . select ([ 'first_band_name' , 'second_band_name' ]); It is often helpful to rename a band after you have selected it. This is not required. It can just be helpful, because the default names for bands are sometimes a bit weird and it can be nice to give them names that are more friendly and intuitive. To give your selected bands new names, add a second argument to the .select() method that defines the new names of the bands. The length of this second argument must match the length of the first. In other words, you need to provide one new name for each band. // Select multiple bands from an image (or image collection) var output = input . select ([ 'first_band_name' , 'second_band_name' ], [ 'first_band_new_name' , 'second_band_new_name' ]);","title":"Select bands"},{"location":"code/ic/sort_cloud_cover/","text":"// Sort image collection by cloud cover property. var output = input . sort ( 'cloud_cover_property' ); // Example for L5. var red_5_sort = red_5 . sort ( 'CLOUD_COVER' );","title":"Sort by cloud cover"},{"location":"code/image/apply_mask/","text":"This workflow involves two rasters: a bottom raster contains values to be displayed or analyzed, a binary raster contains two possible values (zeros or ones). When you apply a mask, you drape the binary raster over the bottom raster. Any location with the value one (1) in the binary raster will be visible in the bottom raster. Any location with the value zero (0) in the binary raster will be masked . A masked value will not be visible when you display the image on a map, nor will it be used in computations when you mask an image in a workflow. // Apply binary raster as a mask on another raster. var image_with_mask = bottom_raster . updateMask ( binary_raster );","title":"Apply mask to image"},{"location":"code/image/chart_area_percent/","text":"Introduction The snippets below creates the chart that is shown in the Land Cover app . Here are the main steps. (1) Convert pixel area into percentage decimal. The first step is to store the percent of the study region area represented by each pixel. To do this, we can divide the pixel area layer by a constant (the area of the study region). The snippet below gets this number from the output of a previous step (when we computed the area of the region). The snippet renames the band \u2018all of town\u2019. This will make the labels look cleaner in the chart at the end. // ---------------------------------------------------------------------------- // Chart areas of regions as a percent of a zone. // ---------------------------------------------------------------------------- // Convert pixel area into precentage decimal. var pixel_percentages = pixel_area_layer . divide ( area_study_region . getNumber ( 'area' )) . rename ( 'all of town' ) ; // Inspect the result. Notice the name of the band in this image. print ( 'pixel_percentage_layer' , pixel_percentages ); (2) Construct an image with dough and cutter bands. The next step is to set up the image that we will chart. The image needs to contain at least one band to use as the dough and one band to use as the cutter. The cutter must be the last band. The snippet below sets up three dough bands: Pixel percentages for an entire raster. Pixel percentages for just the protected lands. Pixel percentages for just the uva lands without any permanent protections. The last band in the image represents the cutters, or the zones that we want to compute the area for. The snippet also renames the bands again to make the labels on the chart at the end look cleaner. // Construct image with three dough bands and one cutter band. var simple_lc_area = pixel_percentages // Band with pixel percentages. . addBands ( pixel_percentages . updateMask ( protected_lands_image ) // Band with pixel percentages masked by protected lands binary. . addBands ( pixel_percentages . updateMask ( uva_not_protected )) // Band with pixel percentages masked by UVA binary without protections. . addBands ( simple_lc ) // Band with simple land cover classes. . rename ( 'protected in town' , 'uva without permanent protections in town' , 'land cover' ) // Labels for added bands. ) ; // Inspect the result. print ( 'simple_lc_area' , simple_lc_area ); (3) Chart zonal statistic for each dough band by cutter. // Chart zonal statistic for each dough band by cutter. // Create list of labels. The first item is a blank placeholder. The rest are labels for the cutters. var lc_labels = [ '' , 'Tree canopy' , 'Water' , 'Old field' , 'Active field' , 'Developed' ] ; // Create dictionary of chart arguments. var chart_params = { image : simple_lc_area , // Image with dough bands and cutter band. classBand : 'land cover' , // Name of the band to use as cutter. region : study_region , // Region to perform operation. reducer : ee . Reducer . sum (), // Type of zonal statistic to calculate. scale : 10 , // Scale to carry out operation. classLabels : lc_labels , // Labels of cutters to use in chart. } ; // Create the chart. var chart = ui . Chart . image . byClass ( chart_params ) // Dictionary of chart arguments. . setChartType ( 'BarChart' ) // Type of chart. . setOptions ({ title : 'Land cover in study region' , // Title for chart. hAxis : { title : 'Percent of study region' , // Title for horizontal axis. titleTextStyle : { italic : false , bold : true }, }, colors : simple_lc_palette , // Colors to use for chart. } ) ; // Inspect chart. print ( chart );","title":"Chart area of regions as percent of a zone"},{"location":"code/image/chart_area_percent/#introduction","text":"The snippets below creates the chart that is shown in the Land Cover app . Here are the main steps.","title":"Introduction"},{"location":"code/image/chart_area_percent/#1-convert-pixel-area-into-percentage-decimal","text":"The first step is to store the percent of the study region area represented by each pixel. To do this, we can divide the pixel area layer by a constant (the area of the study region). The snippet below gets this number from the output of a previous step (when we computed the area of the region). The snippet renames the band \u2018all of town\u2019. This will make the labels look cleaner in the chart at the end. // ---------------------------------------------------------------------------- // Chart areas of regions as a percent of a zone. // ---------------------------------------------------------------------------- // Convert pixel area into precentage decimal. var pixel_percentages = pixel_area_layer . divide ( area_study_region . getNumber ( 'area' )) . rename ( 'all of town' ) ; // Inspect the result. Notice the name of the band in this image. print ( 'pixel_percentage_layer' , pixel_percentages );","title":"(1) Convert pixel area into percentage decimal."},{"location":"code/image/chart_area_percent/#2-construct-an-image-with-dough-and-cutter-bands","text":"The next step is to set up the image that we will chart. The image needs to contain at least one band to use as the dough and one band to use as the cutter. The cutter must be the last band. The snippet below sets up three dough bands: Pixel percentages for an entire raster. Pixel percentages for just the protected lands. Pixel percentages for just the uva lands without any permanent protections. The last band in the image represents the cutters, or the zones that we want to compute the area for. The snippet also renames the bands again to make the labels on the chart at the end look cleaner. // Construct image with three dough bands and one cutter band. var simple_lc_area = pixel_percentages // Band with pixel percentages. . addBands ( pixel_percentages . updateMask ( protected_lands_image ) // Band with pixel percentages masked by protected lands binary. . addBands ( pixel_percentages . updateMask ( uva_not_protected )) // Band with pixel percentages masked by UVA binary without protections. . addBands ( simple_lc ) // Band with simple land cover classes. . rename ( 'protected in town' , 'uva without permanent protections in town' , 'land cover' ) // Labels for added bands. ) ; // Inspect the result. print ( 'simple_lc_area' , simple_lc_area );","title":"(2) Construct an image with dough and cutter bands."},{"location":"code/image/chart_area_percent/#3-chart-zonal-statistic-for-each-dough-band-by-cutter","text":"// Chart zonal statistic for each dough band by cutter. // Create list of labels. The first item is a blank placeholder. The rest are labels for the cutters. var lc_labels = [ '' , 'Tree canopy' , 'Water' , 'Old field' , 'Active field' , 'Developed' ] ; // Create dictionary of chart arguments. var chart_params = { image : simple_lc_area , // Image with dough bands and cutter band. classBand : 'land cover' , // Name of the band to use as cutter. region : study_region , // Region to perform operation. reducer : ee . Reducer . sum (), // Type of zonal statistic to calculate. scale : 10 , // Scale to carry out operation. classLabels : lc_labels , // Labels of cutters to use in chart. } ; // Create the chart. var chart = ui . Chart . image . byClass ( chart_params ) // Dictionary of chart arguments. . setChartType ( 'BarChart' ) // Type of chart. . setOptions ({ title : 'Land cover in study region' , // Title for chart. hAxis : { title : 'Percent of study region' , // Title for horizontal axis. titleTextStyle : { italic : false , bold : true }, }, colors : simple_lc_palette , // Colors to use for chart. } ) ; // Inspect chart. print ( chart );","title":"(3) Chart zonal statistic for each dough band by cutter."},{"location":"code/image/check_module/","text":"// Load module. var check = require ( 'users/jhowarth/eePrimer:modules/checks.js' ); // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). check . meanValue ( // Call function dough , // Image to check bands , // Bands to select in image as a list geometry , // Geometry to use as a cutter resolution , // Pixel resolution (scale) of image (dough) label // Label to print to console above result. ) ; // Example check . meanValue ( red_5_all_clear , bands , geometry , 30 , 'Red 5 check' ) ;","title":"Use module to check image output"},{"location":"code/image/compute_area/","text":"This workflow computes the area of a region as a zonal statistic . A zonal statistic computes a statistic with the values of one layer in one or more zones defined by a second layer. I tend to think of the zones as cookie cutters and the image that provides the statistics as the cookie dough. We will draw this out in class. // Create a layer where each pixel value reports the pixel's area. var pixel_area_layer = ee . Image . pixelArea () ; // Inspect the result. This should be an image with one band named \"area\". print ( 'pixel area layer' , pixel_area_layer ); // Create a dictionary for reducer arguments. var reduce_params = { reducer : ee . Reducer . sum (), // Name of reducer geometry : cutter_features , // Cutter: Feature collection, feature, or geometry that defines the zone. scale : 10 , // Scale to perform operation. This usually needs to be coarser than dough image, otherwise Google complains about the work involved. maxPixels : 1e12 // How many pixels should Google work with before bailing on task? I usually set this at 1e12. } ; // Perform zonal statistic on dough by calling the dictionary defined above. var zonal_area_output = dough . reduceRegion ( reduce_params ); // Inspect results. print ( 'zonal area output' , zonal_area_output )","title":"Compute area of a zone"},{"location":"code/image/construct_image/","text":"Use the ee.Image() method with a string argument (\u2018address\u2019). This will load an image referenced by the address into the variable that you name. // Construct image from address. var output_image = ee . Image ( 'address' );","title":"Construct image from address"},{"location":"code/image/inspect_image/","text":"You can print an image to the Console, though this won\u2019t visualize the image with color. It returns \u2018metadata\u2019 (data about data). Dry as this sounds you can learn some things from it. // Inspect an image. print ( 'label' , data_object ); // Where data_object is the name of the variable that holds the image. In the Console panel, expand the \u2018carrots\u2019 to explore the properties and bands of the image. Please note that an image will alway contain at least one band. The name of the band is in double quotes. The default name of the first band in an image is \u201cb1\u201d (short for band one). You can also inspect an image with the Inspector panel. With the crosshair, click on a location and the value of each band in each image at that location will print to the Inspector panel.","title":"Inspect image"},{"location":"code/image/map_algebra/","text":"When two or more rasters have the same cell size, extent, orientation, and anchor (in other words, when the cells of two rasters align), you can perform algebra with the values in each cell. For example, a simple algebraic expression is: A * B = C. In map algebra , A and B are raster layers and the product C is also a raster layer. Again, the idea is that the computer calculates A * B = C for every location in the raster, always comparing the same cell location between the two input rasters and storing the product in the corresponding cell of the output raster. // Multiply value at each location for two images var raster_C = raster_A . multiply ( raster_B ); The table below lists some of the common algebraic operators. Math operator Method * .multiply() / .divide() + .add() - .subtract() Each of these operators can take either another raster or a constant as an argument. When the argument is another raster, the operator compares the values of each cell between two rasters. When the argument is a constant, the operator scales every value by the constant. This is common when converting between units of measurement. Map algebra can also be performed with logical operators. The two most common examples are intersection (and) and union (or). The table below shows the methods for each. Logical operator Method Intersection .and() Union .or() The output for logical comparisons is a binary raster, where the value 1 identifies where the expression is true and 0 where it is false.","title":"Map algebra"},{"location":"code/image/normalized_difference/","text":"// Compute normalized difference between a first and second band of an image. var output = input . normalizedDifference ([ 'first_band' , 'second_band' ]);","title":"Normalized difference"},{"location":"code/image/reclassify/","text":"// Classify image with defined breaks. var output = input . gte ( first_threshold ) . add ( input . gte ( second_threshold )) . add ( input . gte ( third_threshold )) ;","title":"Reclassify with defined breaks"},{"location":"code/image/reclassify_image/","text":"// Make list of old values. var old_values = [ 0 , 1 , 2 , 3 , 4 , 5 ]; // Make list of new values. The length of this list must be the same as list above. var new_values = [ 1 , 2 , 2 , 1 , 3 , 3 ]; // Replace old values with new values. var reclass_image = old_image . remap ( old_values , new_values );","title":"Generalize (reclassify) image"},{"location":"code/image/reduce_by_regions/","text":"This zonal operation works with an image and a feature collection. It uses the regions defined by each feature in the feature collection to summarize the values of the image. It can sometimes be helpful to think of the analogy of \u2018cookie dough\u2019 and \u2018cookie cutter\u2019 (place cookie cutters onto the dough to summarize dough values). The image provides the dough and the feature collection provides the cutters. The output of this operation is a feature collection, where each feature in the collection has a new property that holds the summary statistic of the image. This new property is named after the reducer used as an argument. // Reduce image values by regions defined by each feature in a feature collection. var output = image_dough // create output from image (dough) . reduceRegions ({ // method to compute a zonal statistic collection : fc , // feature collection to use as cookie cutters reducer : ee . Reducer . ____ (), // name of reducer (for dough values within each cutter) scale : __ , // if possible same as dough, if exceeds memory then 2 or 3 times dough scale tileScale : 1 // if possible 1, but if exceeds memory then 2 or 4 } );","title":"Reduce by regions"},{"location":"code/image/rgb_composites/","text":"Use cases Use this method to symbolize data from three bands with additive color to make an RGB composite. Workflow This involves two steps: define range and bands (collectively called visualization parameters ), visualize as a map layer. Code // 1. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min value of data range. max : 255 , // Max value of data range. bands : [ // Band list: 'band_name' , // Symbolize this band with red channel 'band_name' , // Symbolize this band with green channel 'band_name' // Symbolize this band with blue channel ], } ; // 2. Add as layer to map. map . addLayer ( data_object , // Image with bands to visualize vis , // Range and band list 'layer label' , // Layer label false , // Shown (optional, default is 1) 1 // Opacity (optional, default is 1) ) ; Contrast enhance composites The example above uses the same min and max display range for all three bands in the composite. The histogram for each of band, however, may differ because each band may contain different data ranges. Using a single display range for all three bands will thus likely diminish the contrast of your composite. You can enhance the contrast of RGB composites by defining a different min and max for each band. Your visualization parameters will essentially contain three lists: list of min values, list of max values, list of bands. The first min value and first max value in the list will be applied to the first band, the second min value and second max value will be applied to the second band, and the third min value and third max value will be applied to the third band. // 1. Define 'visualization parameters', or how values map to colors. var vis = { min : [ 0 , 10 , 0 ] // Min value of data range for each band in the list. max : [ 255 , 245 , 245 ], // Max value of data range for each band in the list. bands : [ // Band list: 'band_name' , // Symbolize this band with red channel 'band_name' , // Symbolize this band with green channel 'band_name' // Symbolize this band with blue channel ], } ; // 2. Add as layer to map. map . addLayer ( data_object , // Image with bands to visualize vis , // Range and band list 'layer label' , // Layer label false , // Shown (optional, default is 1) 1 // Opacity (optional, default is 1) ) ;","title":"Symbolize three bands with additive color"},{"location":"code/image/rgb_composites/#use-cases","text":"Use this method to symbolize data from three bands with additive color to make an RGB composite.","title":"Use cases"},{"location":"code/image/rgb_composites/#workflow","text":"This involves two steps: define range and bands (collectively called visualization parameters ), visualize as a map layer.","title":"Workflow"},{"location":"code/image/rgb_composites/#code","text":"// 1. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min value of data range. max : 255 , // Max value of data range. bands : [ // Band list: 'band_name' , // Symbolize this band with red channel 'band_name' , // Symbolize this band with green channel 'band_name' // Symbolize this band with blue channel ], } ; // 2. Add as layer to map. map . addLayer ( data_object , // Image with bands to visualize vis , // Range and band list 'layer label' , // Layer label false , // Shown (optional, default is 1) 1 // Opacity (optional, default is 1) ) ;","title":"Code"},{"location":"code/image/rgb_composites/#contrast-enhance-composites","text":"The example above uses the same min and max display range for all three bands in the composite. The histogram for each of band, however, may differ because each band may contain different data ranges. Using a single display range for all three bands will thus likely diminish the contrast of your composite. You can enhance the contrast of RGB composites by defining a different min and max for each band. Your visualization parameters will essentially contain three lists: list of min values, list of max values, list of bands. The first min value and first max value in the list will be applied to the first band, the second min value and second max value will be applied to the second band, and the third min value and third max value will be applied to the third band. // 1. Define 'visualization parameters', or how values map to colors. var vis = { min : [ 0 , 10 , 0 ] // Min value of data range for each band in the list. max : [ 255 , 245 , 245 ], // Max value of data range for each band in the list. bands : [ // Band list: 'band_name' , // Symbolize this band with red channel 'band_name' , // Symbolize this band with green channel 'band_name' // Symbolize this band with blue channel ], } ; // 2. Add as layer to map. map . addLayer ( data_object , // Image with bands to visualize vis , // Range and band list 'layer label' , // Layer label false , // Shown (optional, default is 1) 1 // Opacity (optional, default is 1) ) ;","title":"Contrast enhance composites"},{"location":"code/image/self_check_image/","text":"Introduction You should learn and employ strategies to self-check your work while you are working on a solution. This involves making requests to visualize a method\u2019s output and either checking the result with known benchmarks or critically thinking: does this look right? Print image to console For images, you can print information to the Console. This will confirm that the output is an image and describe the list of bands that it contains. For each band, you can also inspect the data type. A binary image should only contain integers of the set [0, 1]. A nominal image (that represents categories or names of things) should also only contain integers, but of a larger set. A quantitative image (that represents continuous values with decimal numbers) will likely be a double data type. // Self check image output by printing to Console. print ( 'Self check image output' , // Label to print in Console. image_output // Name of image output to check. ) ; Print min and max values of image to console Another good strategy is to print the minimum and maximum values of an image for a study region. To do this, you need to apply the reduce region method to the image that you can nest as an argument to a print statement. // print min and max values of image with study region. print ( 'image output min and max' , image_output . reduceRegion ( { reducer : ee . Reducer . minMax (), // The reducer for min and max values. geometry : study_region , // A feature collection or geometry that defines your study site. scale : __ , // The scale of image output. maxPixels : 1e12 , // The maximum number of pixels to use in the computation. tileScale : 2 // You can use two (2) to help if run quickly. } ) ); Please note that this computation can be intensive and slow down your script, so after you have run the script to check your work, you should comment out the lines to avoid running them repetitively when you no longer need the information. Add the image as a grayscale layer to the map You can quickly add the image as a grayscale layer to the map. A grayscale will display the data values on a gradient from black to white with black for the lowest value and white for the greatest value. To do this quickly, you can run the code below. // Add image as a grayscale layer to map for a quick peek. Map . addLayer ( image_output , { min : __ , max : __ }, 'Image output check' , 0 ); Please note that you will need to enter the min and max values for the image. The previous code block above is a good way to get this information quickly. If you do not specify the min and max data values to display, the layer will likely have very poor contrast (because the min and max will be defined by the image data type, which is usually a much greater range than that populated with data values). Also note that the default palette is [\u2018black\u2019, \u2018white\u2019], so you do not need to specify a palette to produce a grayscale image.","title":"Check image output during a workflow"},{"location":"code/image/self_check_image/#introduction","text":"You should learn and employ strategies to self-check your work while you are working on a solution. This involves making requests to visualize a method\u2019s output and either checking the result with known benchmarks or critically thinking: does this look right?","title":"Introduction"},{"location":"code/image/self_check_image/#print-image-to-console","text":"For images, you can print information to the Console. This will confirm that the output is an image and describe the list of bands that it contains. For each band, you can also inspect the data type. A binary image should only contain integers of the set [0, 1]. A nominal image (that represents categories or names of things) should also only contain integers, but of a larger set. A quantitative image (that represents continuous values with decimal numbers) will likely be a double data type. // Self check image output by printing to Console. print ( 'Self check image output' , // Label to print in Console. image_output // Name of image output to check. ) ;","title":"Print image to console"},{"location":"code/image/self_check_image/#print-min-and-max-values-of-image-to-console","text":"Another good strategy is to print the minimum and maximum values of an image for a study region. To do this, you need to apply the reduce region method to the image that you can nest as an argument to a print statement. // print min and max values of image with study region. print ( 'image output min and max' , image_output . reduceRegion ( { reducer : ee . Reducer . minMax (), // The reducer for min and max values. geometry : study_region , // A feature collection or geometry that defines your study site. scale : __ , // The scale of image output. maxPixels : 1e12 , // The maximum number of pixels to use in the computation. tileScale : 2 // You can use two (2) to help if run quickly. } ) ); Please note that this computation can be intensive and slow down your script, so after you have run the script to check your work, you should comment out the lines to avoid running them repetitively when you no longer need the information.","title":"Print min and max values of image to console"},{"location":"code/image/self_check_image/#add-the-image-as-a-grayscale-layer-to-the-map","text":"You can quickly add the image as a grayscale layer to the map. A grayscale will display the data values on a gradient from black to white with black for the lowest value and white for the greatest value. To do this quickly, you can run the code below. // Add image as a grayscale layer to map for a quick peek. Map . addLayer ( image_output , { min : __ , max : __ }, 'Image output check' , 0 ); Please note that you will need to enter the min and max values for the image. The previous code block above is a good way to get this information quickly. If you do not specify the min and max data values to display, the layer will likely have very poor contrast (because the min and max will be defined by the image data type, which is usually a much greater range than that populated with data values). Also note that the default palette is [\u2018black\u2019, \u2018white\u2019], so you do not need to specify a palette to produce a grayscale image.","title":"Add the image as a grayscale layer to the map"},{"location":"code/image/symbolize_gradients/","text":"Use cases Use this method to symbolize continuous data , or data that represents a gradient of values (e.g. elevation, temperature, reflectance), from one band of an image. In this case, your goal is to stretch a palette across a range of data values. Workflow To symbolize image data from a single band with color: define color palette, define range, band, and palette (collectively called visualization parameters ), visualize as a map layer. Code // 1. Define color palette. // For nominal data, the number of color codes should equal the number of unique nominal values. var palette_name = [ 'color code' , // Color for min value 'color code' , // Color for max value or intermediate color '...' // Last color symbolizes max value. ]; // 2. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min data value to symbolize. max : 255 , // Max data value to symbolize. bands : [ 'name' ], // Source for data values. palette : palette_name , // Source for palette (range of colors) } ; // 3. Add as layer to map. map . addLayer ( data_object , // Band with values to visualize vis , // Range, source, and palette 'layer label' , // Layer label false , // Shown 1 // Opacity ) ;","title":"Symbolize gradients with a palette"},{"location":"code/image/symbolize_gradients/#use-cases","text":"Use this method to symbolize continuous data , or data that represents a gradient of values (e.g. elevation, temperature, reflectance), from one band of an image. In this case, your goal is to stretch a palette across a range of data values.","title":"Use cases"},{"location":"code/image/symbolize_gradients/#workflow","text":"To symbolize image data from a single band with color: define color palette, define range, band, and palette (collectively called visualization parameters ), visualize as a map layer.","title":"Workflow"},{"location":"code/image/symbolize_gradients/#code","text":"// 1. Define color palette. // For nominal data, the number of color codes should equal the number of unique nominal values. var palette_name = [ 'color code' , // Color for min value 'color code' , // Color for max value or intermediate color '...' // Last color symbolizes max value. ]; // 2. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min data value to symbolize. max : 255 , // Max data value to symbolize. bands : [ 'name' ], // Source for data values. palette : palette_name , // Source for palette (range of colors) } ; // 3. Add as layer to map. map . addLayer ( data_object , // Band with values to visualize vis , // Range, source, and palette 'layer label' , // Layer label false , // Shown 1 // Opacity ) ;","title":"Code"},{"location":"code/image/symbolize_nominal/","text":"Use cases Use this method to symbolize nominal data , or data that represents categories or classes (e.g. land cover categories or elevation classes), from one band of an image. In this case, your goal is to represent each unique category or class with a unique color. Workflow The involves three steps: define palette (set of colors), define range, band, and palette (collectively called visualization parameters ), visualize as a map layer. Code // 1. Define palette. var palette_name = [ 'color code' , // Describe label 'color code' , // Describe label '...' // # color codes should match # class codes ]; // 2. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min class value for palette. max : 1 , // Max class value for palette. bands : [ 'name' ], // Source for class values. palette : palette_name , // Source for palette (range of colors) } ; // 3. Add as layer to map. map . addLayer ( data_object , // Band with values to visualize vis , // Range, source, and palette 'layer label' , // Layer label false , // Shown 1 // Opacity ) ;","title":"Symbolize nominal data with a palette"},{"location":"code/image/symbolize_nominal/#use-cases","text":"Use this method to symbolize nominal data , or data that represents categories or classes (e.g. land cover categories or elevation classes), from one band of an image. In this case, your goal is to represent each unique category or class with a unique color.","title":"Use cases"},{"location":"code/image/symbolize_nominal/#workflow","text":"The involves three steps: define palette (set of colors), define range, band, and palette (collectively called visualization parameters ), visualize as a map layer.","title":"Workflow"},{"location":"code/image/symbolize_nominal/#code","text":"// 1. Define palette. var palette_name = [ 'color code' , // Describe label 'color code' , // Describe label '...' // # color codes should match # class codes ]; // 2. Define 'visualization parameters', or how values map to colors. var vis = { min : 0 , // Min class value for palette. max : 1 , // Max class value for palette. bands : [ 'name' ], // Source for class values. palette : palette_name , // Source for palette (range of colors) } ; // 3. Add as layer to map. map . addLayer ( data_object , // Band with values to visualize vis , // Range, source, and palette 'layer label' , // Layer label false , // Shown 1 // Opacity ) ;","title":"Code"},{"location":"code/image/threshold_image/","text":"This happens when you apply a \u201ctrue or false\u201d criterion to a dataset. For example: true or false, x is equal to 0? The idea is that you ask this question for every cell in a raster and the computer answers the question by giving that cell either a 1 (if true) or 0 (if false). Often, threshold criteria are familiar math criteria. Math criterion Method equal to .eq() not equal to .neq() greater than .gt() greater than or equal to .gte() less than .lt() less than or equal to .lte() The example below shows the syntax for the \u2018equal to\u2019 criterion. If the input is a binary raster, the output will be the inverse. // Threshold image by equality criterion. var output_image = input_image . eq ( 0 );","title":"Threshold an image"},{"location":"code/map/addLayer/","text":"If you have one or more point, line, or polygon objects stored as a variable, you can display the variable on the Map UI as a layer . The layer metaphor goes back to the days when people would place a mylar sheet on top of a reference map and then draw shapes on the mylar. Thus the reference map was the base layer and each mylar sheet put on top of it was a thematic layer . Even though we don\u2019t use mylar anymore, the map layer metaphor is helpful as a conceptual device. In the example below, I assume you have a point object named \u2018point\u2019. Map . addLayer ( point , // Geometry object to show on the map. { color : 'blue' }, // Color to display the objects. 'My first point' // A label for the layer that will appear in the label panel. ); Please note: If you don\u2019t seen the point on your map, it may be because it is not in your current map extent. So you will need to zoom out and pan around to find it. Check your understanding How would you change the display so that the point appears red?","title":"Add layer to map"},{"location":"code/map/centerObject/","text":"You can also define your map extent so that the map centers on a geometry object, like a sample point or city center. In the example below, I assume you have a point object named \u2018point\u2019. // Center on a point. Map . centerObject ( point , 4 ); Check your understanding If you have the choice, should you center the map before you add a layer, or add a layer before you center the map, or do you think that the sequence really does not matter?","title":"Center map on object"},{"location":"code/map/getMapCenter/","text":"When you first open the Code Editor , the Map UI will be centered on a point in Buffalo Valley, OK. You can retrieve the coordinates of a map\u2019s center with this: // Get map center. var center = Map . getCenter (); print ( 'Center point' , center ); Please copy and paste the above code snippet into the Code Editor, then run it, and try to answer the questions below. Check your understanding: Please look at the Console Panel. Which number represents latitude and which represents longitude? What does it mean if a number is negative? Please write down the coordinates that you printed to the Console. Now click the pan button (the little glove in the upper left of the Map UI), then click the map and, while holding your click, drag towards your left. Now run the code again and try to answer the questions below. Check your understanding: Again, please look at the Console Panel. Which number changed the most when you panned? How did it change? Do you think you are moving the map when you pan, or are you moving the window through which you are looking at the map?","title":"Get map center"},{"location":"code/map/getZoom/","text":"By default, the map will always initialize so that you see all of the lower 48 states (plus Puerto Rico) in the map window. After panning the map in the last step, you have changed the region of the world displayed on the map. But you have not changed the amount of detail that the map displays. In the Map UI, zoom level defines the amount of detail displayed on the map. You can retrieve the zoom level used to display a map with this: // Get zoom level of map. var zoom = Map . getZoom (); print ( 'Zoom level' , zoom ); Check your understanding: Use the + and - buttons at the top left of the Map UI and re-run the code snippet to answer these two questions. What zoom level is the most 'zoomed out'? What zoom level is the most 'zoomed in'?","title":"Get zoom"},{"location":"code/map/setBasemap/","text":"By default, the Code Editor map will display with the Google road map. You can change the default basemap type like this: Map . setOptions ( 'HYBRID' ); You may choose from four base map types: \u2018ROADMAP\u2019 \u2018SATELLITE\u2019 \u2018HYBRID\u2019 \u2018TERRAIN\u2019 Check your understanding: What object type does this method take as a parameter?","title":"Set base map"},{"location":"code/map/setCenterZoom/","text":"Together, map center and zoom level define a map\u2019s extent. You can customize the map extent by changing the map center and zoom level like this: // Set map center and zoom. Map . setCenter ( - 73.1812983597342 , 44.013157468373876 , 19 ); Check your understanding: At this zoom level, how many decimal places do you really need to center the map at this location? Hint: remove a decimal from the two coordinates and re-run the script. Repeat until you see first see BiHall move (just a tad).","title":"Set map center and zoom"},{"location":"code/number/construct_number/","text":"The input must be numberic. // Construct a number with ee.Number() method. var output = ee . Number ( input ); // Example var year = ee . Number ( 2012 );","title":"Construct number"},{"location":"code/number/math/","text":"// Math with number objects. var output = input . subtract ( 1 ); var output2 = input . add ( 1 ); // Example var last_year = year . subtract ( 1 ); var new_year = year . add ( 1 );","title":"Math with number objects"},{"location":"concepts/LST/","text":"Definition Please define land surface temperature (LST) and explain how this differs from air surface temperature? LST from Landsat collections In this week\u2019s problem, we will apply a method to create high-resolution LST images from Landsat image collections that was developed and kindly shared by Sophia Ermida . LST and land cover The app below will help you compare land surface temperatures to land cover for anywhere in the world. Please work with the app to explore Middlebury, Brooklyn, Seattle, and at least one other location of your choice. For each, please think through these questions: At the extent of the scene, how does LST relate to land cover? At the scale of a city or town, how does LST vary and how is this related to land cover? What does the histogram tell you about your scene? Why do you need to adjust the min and max display values as your investigation changes location? Since you are displaying a divergent color scheme , how should you aim to set your min and max display values? This link will open the app in a new browser tab.","title":"Land Surface Temperature"},{"location":"concepts/LST/#definition","text":"Please define land surface temperature (LST) and explain how this differs from air surface temperature?","title":"Definition"},{"location":"concepts/LST/#lst-from-landsat-collections","text":"In this week\u2019s problem, we will apply a method to create high-resolution LST images from Landsat image collections that was developed and kindly shared by Sophia Ermida .","title":"LST from Landsat collections"},{"location":"concepts/LST/#lst-and-land-cover","text":"The app below will help you compare land surface temperatures to land cover for anywhere in the world. Please work with the app to explore Middlebury, Brooklyn, Seattle, and at least one other location of your choice. For each, please think through these questions: At the extent of the scene, how does LST relate to land cover? At the scale of a city or town, how does LST vary and how is this related to land cover? What does the histogram tell you about your scene? Why do you need to adjust the min and max display values as your investigation changes location? Since you are displaying a divergent color scheme , how should you aim to set your min and max display values? This link will open the app in a new browser tab.","title":"LST and land cover"},{"location":"concepts/MODIS/","text":"","title":"MODIS"},{"location":"concepts/additive_color/","text":"RGB composites visualize pixel values across three bands with the additive color system. The order of bands (first, second, third) corresponds to the color channel (red, green, blue) used to symbolize pixel values (sometimes called DN, or digital number). The chart below provides a key to the primary and secondary colors produced through the system. A high value in a single band with low values in the other two bands produces the primary colors (red, green, blue). High values in two bands with a low value in a third band produces the secondary colors (yellow, cyan, magenta). Low values in all three bands produce dark colors culminating in black when all values are zero. High values in all three bands produce unsaturated colors that culminate in white when all channels are maxed. You can recreate these colors and explore other intermediary colors with the RGB mixer below. App RGB mixer","title":"Additive color"},{"location":"concepts/additive_color/#app","text":"RGB mixer","title":"App"},{"location":"concepts/band_combinations/","text":"Source: USGS","title":"Band combinations"},{"location":"concepts/bands_image/","text":"A band is a collection of pixels , or an array of values. A band represents a raster data model. One or more bands in a stack makes an image .","title":"Band and Image"},{"location":"concepts/burn_severity/","text":"Introduction Fire intensity: intensity of fire while active Burn severity: how fire intensity affects ecological functions of the burnt area (degree to which fire alters an area) Burn ratio Normalized Burn Ratio (NBR): ratio between NIR and SWIR2 bands. A high NBR indicates healthy vegetation while a low NBR value indicates bare ground and recently burnt areas. Severity classes Burn severity: difference between pre-fire and post-fire NBR. A higher dNBR indicates more severe damage, while negative dNBR may indicate regrowth. Severity level: USGS thresholds for assigning severity classes. Sources un-spider.org EO article","title":"Burn severity"},{"location":"concepts/burn_severity/#introduction","text":"Fire intensity: intensity of fire while active Burn severity: how fire intensity affects ecological functions of the burnt area (degree to which fire alters an area)","title":"Introduction"},{"location":"concepts/burn_severity/#burn-ratio","text":"Normalized Burn Ratio (NBR): ratio between NIR and SWIR2 bands. A high NBR indicates healthy vegetation while a low NBR value indicates bare ground and recently burnt areas.","title":"Burn ratio"},{"location":"concepts/burn_severity/#severity-classes","text":"Burn severity: difference between pre-fire and post-fire NBR. A higher dNBR indicates more severe damage, while negative dNBR may indicate regrowth. Severity level: USGS thresholds for assigning severity classes.","title":"Severity classes"},{"location":"concepts/burn_severity/#sources","text":"un-spider.org EO article","title":"Sources"},{"location":"concepts/cart_roots/","text":"Geographic framework equator prime meridian latitude longitude great circle zoom level representative fraction (RF) scale Vector model Geometry (points, lines, polygons) Attributes (spatial, thematic) Singlepart versus multipart Properties Transformations Terms in italics are synonyms. input operation ( method ) output parameters ( arguments ) data types ( object classes ) Cartographic modeling task ( goal ) task hierarchy workflow ( procedure , program )","title":"Cartographic roots"},{"location":"concepts/cart_roots/#geographic-framework","text":"equator prime meridian latitude longitude great circle zoom level representative fraction (RF) scale","title":"Geographic framework"},{"location":"concepts/cart_roots/#vector-model","text":"Geometry (points, lines, polygons) Attributes (spatial, thematic) Singlepart versus multipart Properties","title":"Vector model"},{"location":"concepts/cart_roots/#transformations","text":"Terms in italics are synonyms. input operation ( method ) output parameters ( arguments ) data types ( object classes )","title":"Transformations"},{"location":"concepts/cart_roots/#cartographic-modeling","text":"task ( goal ) task hierarchy workflow ( procedure , program )","title":"Cartographic modeling"},{"location":"concepts/contrast_enhancement/","text":"","title":"Contrast enhancement"},{"location":"concepts/contrast_three_bands/","text":"","title":"Three band contrast enhancement"},{"location":"concepts/css_box_model/","text":"source","title":"CSS box model"},{"location":"concepts/electromagnetic_spectrum/","text":"Definition Range of energy wavelengths from sun. Illustrations source: serc.carleton.edu","title":"Electromagnetic spectrum"},{"location":"concepts/electromagnetic_spectrum/#definition","text":"Range of energy wavelengths from sun.","title":"Definition"},{"location":"concepts/electromagnetic_spectrum/#illustrations","text":"source: serc.carleton.edu","title":"Illustrations"},{"location":"concepts/features/","text":"coming soon","title":"Feature model"},{"location":"concepts/grayscale/","text":"","title":"Grayscale layer"},{"location":"concepts/landsat_lexicon/","text":"There are a number of different Landsat collections in the Earth Engine Data Catalog. To be able navigate the collection, you should be comfortable with the following lexicon. Missions The Landsat program consists of a series of missions . Each mission deploys one or more sensors on a satellite. Very practically, the mission\u2019s name denotes the launch sequence. Landsat 1 launched July 23, 1972. Landsat 9 launched September 27, 2021. Why are there no images from Landsat 6? Sensors The sensors onboard the Landsat satellites have steadily aimed to improve upon Norwood\u2019s original MSS. MSS : Multispectral Scanner TM : Thematic Mapper ETM+ : Enhanced Thematic Mapper OLI/OLI-2 : Operational Land Imager TIRS/TIRS-2 : Thermal Infrared Sensor Collections There have been two major reprocessing efforts by USGS to improve data quality. Collection 2 is the most recent and has the best geolocation accuracy which improves time series analyses. Tiers Within a collection, Tier 1 data have the highest radiometric and positional quality. USGS recommends using Tier 1 data for all future time-series analysis. Levels Distinguishes the level of data processing applied to products. Level-1 includes processing to improve locational accuracy of data. Level-2 products are built from Level 1, but also provide atmospheric correction to create surface reflectance and surface temperature products. Level-2 science products also include spectral indices derived from surface reflectance products. Level-3 products are built from Level-2 products and include Analysis Ready Data (ARD), including Fractional Snow Covered Area and Burned Area, and Scene-based Inputs, including Provisional Actual Evapotranspiration. Sensor measurements Distinguishes what the pixel values represent. Raw scenes : DN (digital number) values represent scaled, calibrated at-sensor radiance. Top of atmosphere (TOA) : calibrated top-of-atmosphere reflectance. Surface reflectance : atmospherically corrected surface reflectance and land surface temperature.","title":"Landsat lexicon"},{"location":"concepts/landsat_lexicon/#missions","text":"The Landsat program consists of a series of missions . Each mission deploys one or more sensors on a satellite. Very practically, the mission\u2019s name denotes the launch sequence. Landsat 1 launched July 23, 1972. Landsat 9 launched September 27, 2021. Why are there no images from Landsat 6?","title":"Missions"},{"location":"concepts/landsat_lexicon/#sensors","text":"The sensors onboard the Landsat satellites have steadily aimed to improve upon Norwood\u2019s original MSS. MSS : Multispectral Scanner TM : Thematic Mapper ETM+ : Enhanced Thematic Mapper OLI/OLI-2 : Operational Land Imager TIRS/TIRS-2 : Thermal Infrared Sensor","title":"Sensors"},{"location":"concepts/landsat_lexicon/#collections","text":"There have been two major reprocessing efforts by USGS to improve data quality. Collection 2 is the most recent and has the best geolocation accuracy which improves time series analyses.","title":"Collections"},{"location":"concepts/landsat_lexicon/#tiers","text":"Within a collection, Tier 1 data have the highest radiometric and positional quality. USGS recommends using Tier 1 data for all future time-series analysis.","title":"Tiers"},{"location":"concepts/landsat_lexicon/#levels","text":"Distinguishes the level of data processing applied to products. Level-1 includes processing to improve locational accuracy of data. Level-2 products are built from Level 1, but also provide atmospheric correction to create surface reflectance and surface temperature products. Level-2 science products also include spectral indices derived from surface reflectance products. Level-3 products are built from Level-2 products and include Analysis Ready Data (ARD), including Fractional Snow Covered Area and Burned Area, and Scene-based Inputs, including Provisional Actual Evapotranspiration.","title":"Levels"},{"location":"concepts/landsat_lexicon/#sensor-measurements","text":"Distinguishes what the pixel values represent. Raw scenes : DN (digital number) values represent scaled, calibrated at-sensor radiance. Top of atmosphere (TOA) : calibrated top-of-atmosphere reflectance. Surface reflectance : atmospherically corrected surface reflectance and land surface temperature.","title":"Sensor measurements"},{"location":"concepts/mother_of_landsat/","text":"Q: I\u2019ve seen an image of Yosemite National Park\u2019s Half Dome taken by a Landsat 1 prototype. How did that come about? A: When NASA said, \u201cHow do you know this damn thing will work?\u201d [we] came up with the idea of taking the breadboard around [to national parks]. [The Half Dome image] was a tremendous hit. source Virginia Tower Norwood during her second or third year at MIT. Source: MIT Technology Review For Virginia Tower Norwood\u2019s biography, please read this article .","title":"Mother of Landsat"},{"location":"concepts/raster/","text":"Review questions How does a raster store attributes of locations? What data objects does Earth Engine provide to work with geographic information in a raster framework? What are similarities and differences between raster and vector models? How does a raster model display data values with colors ? How does a raster represent points , lines , and zones ? What are similarities and differences between thresholding and generalizing ? What are similarities and differences between map algebra and masks ? What are similarities and differences between computation of area in vector versus raster models? What towns have most to lose if UVA lands are developed? Who wins, who loses when setting 30 x \u201830 goal at state versus town scale?","title":"Raster model"},{"location":"concepts/raster/#review-questions","text":"How does a raster store attributes of locations? What data objects does Earth Engine provide to work with geographic information in a raster framework? What are similarities and differences between raster and vector models? How does a raster model display data values with colors ? How does a raster represent points , lines , and zones ? What are similarities and differences between thresholding and generalizing ? What are similarities and differences between map algebra and masks ? What are similarities and differences between computation of area in vector versus raster models? What towns have most to lose if UVA lands are developed? Who wins, who loses when setting 30 x \u201830 goal at state versus town scale?","title":"Review questions"},{"location":"concepts/reflectance/","text":"Definition The amount of energy leaving a location on the earth\u2019s surface / the amount of energy striking that location, usually expressed as a ratio or percentage. Illustration Source: https://jp.mathworks.com/help/images/hyperspectral-data-correction.html","title":"Reflectance"},{"location":"concepts/reflectance/#definition","text":"The amount of energy leaving a location on the earth\u2019s surface / the amount of energy striking that location, usually expressed as a ratio or percentage.","title":"Definition"},{"location":"concepts/reflectance/#illustration","text":"Source: https://jp.mathworks.com/help/images/hyperspectral-data-correction.html","title":"Illustration"},{"location":"concepts/remote_sensing_systems/","text":"Definition A system for observing conditions on Earth (or other planets) without making direct contact with the observed subject. Remote sensing includes both passive and active systems of observations. Illustration source: Arkarjun (2013)","title":"Remote sensing systems"},{"location":"concepts/remote_sensing_systems/#definition","text":"A system for observing conditions on Earth (or other planets) without making direct contact with the observed subject. Remote sensing includes both passive and active systems of observations.","title":"Definition"},{"location":"concepts/remote_sensing_systems/#illustration","text":"source: Arkarjun (2013)","title":"Illustration"},{"location":"concepts/rgb_composites/","text":"","title":"RGB composites"},{"location":"concepts/scenes/","text":"Review Questions What are scenes ? What are common sources for image scenes and how do they differ? How does a mosaic differ from a composite ? Sources of scenes To get started, we should first discuss how Landsat 8 imagery differs from MODIS Terra imagery (what we used last week to look at global oceans). Try to fill out this table as we discuss the next few sections. SATELLITE SPATIAL RESOLUTION TEMPORAL RESOLUTION MODIS TERRA LANDSAT 8 SENTINEL 2 The app below will grab the scene from each satellite at the center of the map and then draw natural color composites of each image. This link will open the app in a new browser tab. Check your understanding What kinds of questions will Landsat enable us to ask that would be impossible with MODIS? Where are the satellites at this moment? Here\u2019s a map that shows the location of US earth observing satellites (Landsat and MODIS platforms). How does Landsat\u2019s orbit influence scene mosaics? The video below animates how Landsat 8 images the entire surface of the planet. Check your understanding How many days may separate two adjacent scenes? How does MODIS orbit influence scene mosaics? The video below animates how MODIS Aqua images the planet\u2019s entire surface. Check your understanding Why does MODIS imagery often have pointy stripes in equatorial regions?","title":"Image scenes"},{"location":"concepts/scenes/#review-questions","text":"What are scenes ? What are common sources for image scenes and how do they differ? How does a mosaic differ from a composite ?","title":"Review Questions"},{"location":"concepts/scenes/#sources-of-scenes","text":"To get started, we should first discuss how Landsat 8 imagery differs from MODIS Terra imagery (what we used last week to look at global oceans). Try to fill out this table as we discuss the next few sections. SATELLITE SPATIAL RESOLUTION TEMPORAL RESOLUTION MODIS TERRA LANDSAT 8 SENTINEL 2 The app below will grab the scene from each satellite at the center of the map and then draw natural color composites of each image. This link will open the app in a new browser tab. Check your understanding What kinds of questions will Landsat enable us to ask that would be impossible with MODIS?","title":"Sources of scenes"},{"location":"concepts/scenes/#where-are-the-satellites-at-this-moment","text":"Here\u2019s a map that shows the location of US earth observing satellites (Landsat and MODIS platforms).","title":"Where are the satellites at this moment?"},{"location":"concepts/scenes/#how-does-landsats-orbit-influence-scene-mosaics","text":"The video below animates how Landsat 8 images the entire surface of the planet. Check your understanding How many days may separate two adjacent scenes?","title":"How does Landsat's orbit influence scene mosaics?"},{"location":"concepts/scenes/#how-does-modis-orbit-influence-scene-mosaics","text":"The video below animates how MODIS Aqua images the planet\u2019s entire surface. Check your understanding Why does MODIS imagery often have pointy stripes in equatorial regions?","title":"How does MODIS orbit influence scene mosaics?"},{"location":"concepts/sentinel/","text":"Sentinel missions Sentinel collections","title":"Sentinel"},{"location":"concepts/spectral_indices/","text":"Normalized difference (first - second) / (first + second) The normalized burn ratio (NBR) is a normalized ratio so we are dividing the difference of two bands by their sum. NBR uses the SWIR 2 and NIR bands. The Burn Severity Index is derived from the change in NBR before and after a fire. The classes shown below are standards proposed by the USGS. The map shown at the top uses the thresholds that are not scaled (column on right).","title":"Spectral indices"},{"location":"concepts/spectral_indices/#normalized-difference","text":"(first - second) / (first + second) The normalized burn ratio (NBR) is a normalized ratio so we are dividing the difference of two bands by their sum. NBR uses the SWIR 2 and NIR bands. The Burn Severity Index is derived from the change in NBR before and after a fire. The classes shown below are standards proposed by the USGS. The map shown at the top uses the thresholds that are not scaled (column on right).","title":"Normalized difference"},{"location":"concepts/spectral_signature/","text":"Definition How reflectance changes with wavelength for different land cover types. Illustration source: Hartley Bulcock Tools Spectral signature app S2 band spec sheet","title":"Spectral signatures"},{"location":"concepts/spectral_signature/#definition","text":"How reflectance changes with wavelength for different land cover types.","title":"Definition"},{"location":"concepts/spectral_signature/#illustration","text":"source: Hartley Bulcock","title":"Illustration"},{"location":"concepts/spectral_signature/#tools","text":"Spectral signature app S2 band spec sheet","title":"Tools"},{"location":"concepts/what_is_landsat/","text":"The Landsat Program is a series of Earth-observing satellite missions jointly managed by NASA and the U.S. Geological Survey. Landsat represents the world\u2019s longest continuously acquired collection of space-based moderate-resolution land remote sensing data. Source more background","title":"What is Landsat?"},{"location":"lessons/changes_in_the_night/","text":"Introduction The US military designed the Operational Line-Scan System (OLS) of the Defense Meteorological Program (DMSP) to observe weather patterns with satellites. Unexpectedly, the system also proved capable of detecting visible and near-infrared (VNIR) emission sources at night. What do you think are sources of bright lights at night? What do you think we could learn by studying changes in these lights over time? Concepts Band and Image RGB composites Additive color Activity With the app below, please select from the \u2018Explore a pattern\u2019 list. For each pattern: Click on locations in the pattern to explore it and draw charts of the pixel values for each band. Use the additive color keys to describe the changes symbolized by the charts. For each pattern, identify at least one analogous place (where you see the same pattern) in another part of the world and write down the placename so that we can use the search bar to return to the place. Explore the map to find another pattern not described in the pattern list. App This link opens the app in a new browser tab. References Mapping City Lights With Nighttime Data from the DMSP Operational Linescan System","title":"Changes in the night"},{"location":"lessons/changes_in_the_night/#introduction","text":"The US military designed the Operational Line-Scan System (OLS) of the Defense Meteorological Program (DMSP) to observe weather patterns with satellites. Unexpectedly, the system also proved capable of detecting visible and near-infrared (VNIR) emission sources at night. What do you think are sources of bright lights at night? What do you think we could learn by studying changes in these lights over time?","title":"Introduction"},{"location":"lessons/changes_in_the_night/#concepts","text":"Band and Image RGB composites Additive color","title":"Concepts"},{"location":"lessons/changes_in_the_night/#activity","text":"With the app below, please select from the \u2018Explore a pattern\u2019 list. For each pattern: Click on locations in the pattern to explore it and draw charts of the pixel values for each band. Use the additive color keys to describe the changes symbolized by the charts. For each pattern, identify at least one analogous place (where you see the same pattern) in another part of the world and write down the placename so that we can use the search bar to return to the place. Explore the map to find another pattern not described in the pattern list.","title":"Activity"},{"location":"lessons/changes_in_the_night/#app","text":"This link opens the app in a new browser tab.","title":"App"},{"location":"lessons/changes_in_the_night/#references","text":"Mapping City Lights With Nighttime Data from the DMSP Operational Linescan System","title":"References"},{"location":"lessons/compare_ols_L5/","text":"Introduction Your goal is to write a script that will allow you to compare changes in nighttime lights across three time periods with false color composites for each time period. The app below illustrates the map you will produce. This link opens the app in a new browser tab. Prerequisites This lesson follows changes in the night and natural/false color . Code blocks The script draws on the following new code blocks: Construct number Math with number objects Symbolize three bands with additive color Add bands to an image Sort by cloud cover Check image with module Dropbox Please report requested checks and a link to the script in this dropbox . Starter script // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // problem: rgbLights_practice.js // your name: // date: // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // -------------------------------------------------------------------------------- // Define Study Periods for Nighttime Lights collection. // -------------------------------------------------------------------------------- // Load image collection from \"NOAA/DMSP-OLS/NIGHTTIME_LIGHTS\". // // Please use this name for the output: // ic var ic = // Define three target years (2012, 2002, 1992) as ee.Number() objects. // // Please use these names for the outputs: // red_year (for 2012 target) // green_year (for 2002 target) // blue_year (for 1992 target) // Filter image collection to create a separate image collections for three study periods, // where a study period begins one year before and ends one year after the target years defined in last step. // // Please use these names for the outputs: // red_year_ic (based on red_year target) // green_year_ic (based on green_year target) // blue_year_ic (based on blue_year target) // CHECK 1 // Inspect each year and report how many images you have in each . print ( 'CHECK 1' ); // -------------------------------------------------------------------------------- // Reduce each study period to a single image. // -------------------------------------------------------------------------------- // Reduce each image collection to the mean value of each pixel // Select the 'stable_lights' band // Rename the band to convery the sequence in time each image represents ('last', 'middle', 'first') // // Please use these names for the outputs: // mean_red_year (based on red_year_ic) should have band 'last' // mean_green_year (based on green_year_ic) should have band name 'middle' // mean_blue_year (based on blue_year_ic) should have band name 'first' // CHECK 2 // Inspect the result for each year and report the # of bands and data type for the new images. // Hint: they should all have the same number of bands and the same data type. print ( 'CHECK 2' ); // -------------------------------------------------------------------------------- // Construct a new image that contains three bands. // -------------------------------------------------------------------------------- // Construct a three band image by adding the 'middle' and 'first' bands to the 2013 image. // // Please use this names for the output: // lights_change_image // CHECK 3 // Inspect your output and report the band name and data type for each band index (0, 1, 2) in the output. print ( 'CHECK 3' ); // --------------------------------------------------------------------- // Compose map // --------------------------------------------------------------------- // Define a point of interest. var geometry = ee . Geometry . Point ([ 120.603693 , 31.310461 ]); // Center map on point of interest and zoom level 12. // Set basemap to satellite with labels. // Add three-band image to map with display range from 0 to 63. // Label the layer 'Changes in the night' // --------------------------------------------------------------------- // Load and filter Landsat 5 image collection. // --------------------------------------------------------------------- // Load Landsat 5 collection from ee address 'LANDSAT/LT05/C02/T1_L2' // and filter collection by point of interest. // // Please use this name for the output: // L5 // Create a new image collection for each study period (as defined previously). // // Please use these names for the outputs: // red_5 (based on red_year target) // green_5 (based on green_year target) // blue_5 (based on blue_year target) // CHECK 4 // Inspect results and report the size of each image collection print ( 'CHECK 4' ); // --------------------------------------------------------------------- // Sort images by cloud cover and pick the first in the list. // --------------------------------------------------------------------- // Sort each image collection by 'CLOUD_COVER'. // // Please use these names for the outputs: // red_5_sort (based on red_5 study period) // green_5_sort (based on green_5 study period) // blue_5_sort (based on blue_5 study period) // Check to see that the last step worked as you expected. // The images in the collection should now be sorted from least cloudy to most cloudy. // Select the first image in each sorted collection. // // Please use these names for the outputs: // red_5_all_clear (based on red_5_sort) // green_5_all_clear (based on green_5_sort) // blue_5_all_clear (based on blue_5_sort) // CHECK 5 // Inspect the results and report the id of the image for each time period. print ( 'CHECK 5' ); // --------------------------------------------------------------------- // Display each image as a SWIR false color composite. // --------------------------------------------------------------------- // Define a list of bands to use for a SWIR false color composite. // // Please use this name for the outputs: // bands // Define viz paramters of the composite. // Call the variable you made in the last step to define the bands. // Set display range to be 8000 - 17000. // // Please use this name for the outputs: // viz // Add SWIR false color composte layer for each all_clear image to map. // The most recent image should be the 'top' L5 layer and the oldest image should be the 'bottom' L5 layer. // CHECK 6 // Use the meanValue() function to check your results from the imported module to check your final results. // For each time period, you should be able to report the mean value at your point of interest for each band in your composite. print ( 'CHECK 6' ); var check = require ( 'users/jhowarth/eePrimer:modules/checks.js' );","title":"Compare OLS with L5"},{"location":"lessons/compare_ols_L5/#introduction","text":"Your goal is to write a script that will allow you to compare changes in nighttime lights across three time periods with false color composites for each time period. The app below illustrates the map you will produce. This link opens the app in a new browser tab.","title":"Introduction"},{"location":"lessons/compare_ols_L5/#prerequisites","text":"This lesson follows changes in the night and natural/false color .","title":"Prerequisites"},{"location":"lessons/compare_ols_L5/#code-blocks","text":"The script draws on the following new code blocks: Construct number Math with number objects Symbolize three bands with additive color Add bands to an image Sort by cloud cover Check image with module","title":"Code blocks"},{"location":"lessons/compare_ols_L5/#dropbox","text":"Please report requested checks and a link to the script in this dropbox .","title":"Dropbox"},{"location":"lessons/compare_ols_L5/#starter-script","text":"// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // problem: rgbLights_practice.js // your name: // date: // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // -------------------------------------------------------------------------------- // Define Study Periods for Nighttime Lights collection. // -------------------------------------------------------------------------------- // Load image collection from \"NOAA/DMSP-OLS/NIGHTTIME_LIGHTS\". // // Please use this name for the output: // ic var ic = // Define three target years (2012, 2002, 1992) as ee.Number() objects. // // Please use these names for the outputs: // red_year (for 2012 target) // green_year (for 2002 target) // blue_year (for 1992 target) // Filter image collection to create a separate image collections for three study periods, // where a study period begins one year before and ends one year after the target years defined in last step. // // Please use these names for the outputs: // red_year_ic (based on red_year target) // green_year_ic (based on green_year target) // blue_year_ic (based on blue_year target) // CHECK 1 // Inspect each year and report how many images you have in each . print ( 'CHECK 1' ); // -------------------------------------------------------------------------------- // Reduce each study period to a single image. // -------------------------------------------------------------------------------- // Reduce each image collection to the mean value of each pixel // Select the 'stable_lights' band // Rename the band to convery the sequence in time each image represents ('last', 'middle', 'first') // // Please use these names for the outputs: // mean_red_year (based on red_year_ic) should have band 'last' // mean_green_year (based on green_year_ic) should have band name 'middle' // mean_blue_year (based on blue_year_ic) should have band name 'first' // CHECK 2 // Inspect the result for each year and report the # of bands and data type for the new images. // Hint: they should all have the same number of bands and the same data type. print ( 'CHECK 2' ); // -------------------------------------------------------------------------------- // Construct a new image that contains three bands. // -------------------------------------------------------------------------------- // Construct a three band image by adding the 'middle' and 'first' bands to the 2013 image. // // Please use this names for the output: // lights_change_image // CHECK 3 // Inspect your output and report the band name and data type for each band index (0, 1, 2) in the output. print ( 'CHECK 3' ); // --------------------------------------------------------------------- // Compose map // --------------------------------------------------------------------- // Define a point of interest. var geometry = ee . Geometry . Point ([ 120.603693 , 31.310461 ]); // Center map on point of interest and zoom level 12. // Set basemap to satellite with labels. // Add three-band image to map with display range from 0 to 63. // Label the layer 'Changes in the night' // --------------------------------------------------------------------- // Load and filter Landsat 5 image collection. // --------------------------------------------------------------------- // Load Landsat 5 collection from ee address 'LANDSAT/LT05/C02/T1_L2' // and filter collection by point of interest. // // Please use this name for the output: // L5 // Create a new image collection for each study period (as defined previously). // // Please use these names for the outputs: // red_5 (based on red_year target) // green_5 (based on green_year target) // blue_5 (based on blue_year target) // CHECK 4 // Inspect results and report the size of each image collection print ( 'CHECK 4' ); // --------------------------------------------------------------------- // Sort images by cloud cover and pick the first in the list. // --------------------------------------------------------------------- // Sort each image collection by 'CLOUD_COVER'. // // Please use these names for the outputs: // red_5_sort (based on red_5 study period) // green_5_sort (based on green_5 study period) // blue_5_sort (based on blue_5 study period) // Check to see that the last step worked as you expected. // The images in the collection should now be sorted from least cloudy to most cloudy. // Select the first image in each sorted collection. // // Please use these names for the outputs: // red_5_all_clear (based on red_5_sort) // green_5_all_clear (based on green_5_sort) // blue_5_all_clear (based on blue_5_sort) // CHECK 5 // Inspect the results and report the id of the image for each time period. print ( 'CHECK 5' ); // --------------------------------------------------------------------- // Display each image as a SWIR false color composite. // --------------------------------------------------------------------- // Define a list of bands to use for a SWIR false color composite. // // Please use this name for the outputs: // bands // Define viz paramters of the composite. // Call the variable you made in the last step to define the bands. // Set display range to be 8000 - 17000. // // Please use this name for the outputs: // viz // Add SWIR false color composte layer for each all_clear image to map. // The most recent image should be the 'top' L5 layer and the oldest image should be the 'bottom' L5 layer. // CHECK 6 // Use the meanValue() function to check your results from the imported module to check your final results. // For each time period, you should be able to report the mean value at your point of interest for each band in your composite. print ( 'CHECK 6' ); var check = require ( 'users/jhowarth/eePrimer:modules/checks.js' );","title":"Starter script"},{"location":"lessons/corridor_easement_list/","text":"Introduction The State of Vermont\u2019s Climate Action Plan anticipates that the next century will bring broad shifts in local patterns of precipitation and temperature. Annual temperatures are projected to increase over 2\u00b0F by 2050 and between 4 to 9\u00b0F by 2100. The number of days with extreme heat (defined by a high temperature of at least 90\u00b0F) are projected to double in frequency to nine days a year by 2050 and then increase to between 15 and 45 days a year by the century\u2019s end. Annual precipitation is projected to increase one to two inches by 2050 and between four and nine inches by 2100. Much of this increased precipitation will be delivered in higher intensity storms, as extreme precipitation events (those with more than two inches of rain in a 24-hour period) are expected to increase. These changes will directly impact the ecological function and habitat of lands for native plant and animal species. As a result, the goal of protecting 30 percent of terrestrial lands and waters by 2030 has been widely recognized as a pillar of climate change planning. The United Nations Convention on Biodiversity adopted 30 by \u201830 goal as one of ten \u2018milestones\u2019 in a global framework for managing nature through 2030. President Biden\u2019s 2021 \u201cExecutive Order on Tackling the Climate Crisis at Home and Abroad\u201d also adopted the 30 by \u201830 goal. In Vermont, the River Corridor Easement Program is one instrument available to compensate private land owners for extending protections on their lands. Your goal this week is to write a script in Earth Engine that identifies property owners with at least 50 acres of land within a designated river corridor for any watershed in Vermont. This tool could be used by municipal Conservation Commissions and private Land Trusts to contact land owners and initiate conversations that help us get closer to reaching 30 by \u201830 and subsequent 50 by \u201850 conservation goals. The app below illustrates that basic tool that you will make. Here is a link to the app that will open in a separate window. New code The list below introduces new methods that you will use to solve this problem. Each snippet introduces a method with a toy case. Your task is to apply these new methods with methods you have learned previously to complete all sections of the starter script below. Vector objects Inspect first feature Inspect unique values of one property Filter by nominal attribute Filter by numerical attribute Buffer a feature Compute area Intersection Buffer every feature in collection Starter script // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_02_practice_problem.js // NAME: Your name here please // DATE: Today's date // PURPOSE: Identify opportunities for river corridor easements. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Use the drawing tool to drop a point on the Battell Bridge. // -------------------------------------------------------------------------------- // 1. Load watersheds and filter for Vermont and study region. // -------------------------------------------------------------------------------- // Load watersheds from Earth Engine data catalog. Search for 'HUC12'. // Print first feature in collection in order to examine the table structure. // Filter the feature collection by attribute to keep only watersheds in the state of Vermont. // Create a study region by selecting the Vermont watershed that intersects the point of interest. // -------------------------------------------------------------------------------- // 2. Set up map and add watershed layers. // -------------------------------------------------------------------------------- // Center map study region and set zoom 11. // Set base map to satellite image with labels. // Add layer of Vermont watersheds to map. Use color 'LightCyan' and 0.5 opacity. // Add layer of study region to map. Use color 'PaleTurquoise' // ================================================================================ // Dataset addresses that are not in Earth Engine Data Catalog. // ================================================================================ var datasets = { parcels : 'projects/conservation-atlas/assets/cadastre/VTPARCELS_poly_standardized_parcels_SP_v1' , river_corridors : 'projects/conservation-atlas/assets/hydrology/WaterHydro_RiverCorridors_poly' , } ; // -------------------------------------------------------------------------------- // 3. Show river corridor that overlaps the study region. // -------------------------------------------------------------------------------- // Construct a feature collection from the river corridors address. // Filter the collection for features that intersect the study region. // Union the features within the collection. // Add the result from the last step to the map. Use color 'LightSkyBlue'. // -------------------------------------------------------------------------------- // 4. Show property parcels that overlap the river corridor in the study region. // -------------------------------------------------------------------------------- // Construct a feature collection from the parcels address. // Filter the feature collection for features that intersect the river corridors in the study region. // Filter the feature collection by attribute to keep features that are parcels (not roads, water, etc). /* Please note: to do the last step (filter by attribute), you will need to: 1. Find the property keys for the features and figure out which one describes the parcel property type. 2. Then you need to find the list of values for this key (so that you can see how the data distinguishes owned parcels from roads, water, etc). */ // Add result from last step as layer to map. Use color 'Gainsboro'. // -------------------------------------------------------------------------------- // 5. Show intersections of parcels and river corridors in the study region. // -------------------------------------------------------------------------------- // Create function to intersect each parcel (from step 3) by river corridor (from step 3). // Please ask for help if you get stuck here. // Apply the function to every parcel in the featue collection. // Add the result to the map. Use color 'LightSkyBlue'. // -------------------------------------------------------------------------------- // 6. Show features that meet an area criterion. // -------------------------------------------------------------------------------- // Write a function to compute area of each feature and append result to each feature as the property 'ACRES'. // Filter the result of above for all features that are greater than 50 acres. // Add the result as a layer to map. Use color 'Orchid'. // -------------------------------------------------------------------------------- // 7. Display results as a table. // -------------------------------------------------------------------------------- // We will discuss this last step in class tomorrow.","title":"River corridor easement outreach"},{"location":"lessons/corridor_easement_list/#introduction","text":"The State of Vermont\u2019s Climate Action Plan anticipates that the next century will bring broad shifts in local patterns of precipitation and temperature. Annual temperatures are projected to increase over 2\u00b0F by 2050 and between 4 to 9\u00b0F by 2100. The number of days with extreme heat (defined by a high temperature of at least 90\u00b0F) are projected to double in frequency to nine days a year by 2050 and then increase to between 15 and 45 days a year by the century\u2019s end. Annual precipitation is projected to increase one to two inches by 2050 and between four and nine inches by 2100. Much of this increased precipitation will be delivered in higher intensity storms, as extreme precipitation events (those with more than two inches of rain in a 24-hour period) are expected to increase. These changes will directly impact the ecological function and habitat of lands for native plant and animal species. As a result, the goal of protecting 30 percent of terrestrial lands and waters by 2030 has been widely recognized as a pillar of climate change planning. The United Nations Convention on Biodiversity adopted 30 by \u201830 goal as one of ten \u2018milestones\u2019 in a global framework for managing nature through 2030. President Biden\u2019s 2021 \u201cExecutive Order on Tackling the Climate Crisis at Home and Abroad\u201d also adopted the 30 by \u201830 goal. In Vermont, the River Corridor Easement Program is one instrument available to compensate private land owners for extending protections on their lands. Your goal this week is to write a script in Earth Engine that identifies property owners with at least 50 acres of land within a designated river corridor for any watershed in Vermont. This tool could be used by municipal Conservation Commissions and private Land Trusts to contact land owners and initiate conversations that help us get closer to reaching 30 by \u201830 and subsequent 50 by \u201850 conservation goals. The app below illustrates that basic tool that you will make. Here is a link to the app that will open in a separate window.","title":"Introduction"},{"location":"lessons/corridor_easement_list/#new-code","text":"The list below introduces new methods that you will use to solve this problem. Each snippet introduces a method with a toy case. Your task is to apply these new methods with methods you have learned previously to complete all sections of the starter script below. Vector objects Inspect first feature Inspect unique values of one property Filter by nominal attribute Filter by numerical attribute Buffer a feature Compute area Intersection Buffer every feature in collection","title":"New code"},{"location":"lessons/corridor_easement_list/#starter-script","text":"// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_02_practice_problem.js // NAME: Your name here please // DATE: Today's date // PURPOSE: Identify opportunities for river corridor easements. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Use the drawing tool to drop a point on the Battell Bridge. // -------------------------------------------------------------------------------- // 1. Load watersheds and filter for Vermont and study region. // -------------------------------------------------------------------------------- // Load watersheds from Earth Engine data catalog. Search for 'HUC12'. // Print first feature in collection in order to examine the table structure. // Filter the feature collection by attribute to keep only watersheds in the state of Vermont. // Create a study region by selecting the Vermont watershed that intersects the point of interest. // -------------------------------------------------------------------------------- // 2. Set up map and add watershed layers. // -------------------------------------------------------------------------------- // Center map study region and set zoom 11. // Set base map to satellite image with labels. // Add layer of Vermont watersheds to map. Use color 'LightCyan' and 0.5 opacity. // Add layer of study region to map. Use color 'PaleTurquoise' // ================================================================================ // Dataset addresses that are not in Earth Engine Data Catalog. // ================================================================================ var datasets = { parcels : 'projects/conservation-atlas/assets/cadastre/VTPARCELS_poly_standardized_parcels_SP_v1' , river_corridors : 'projects/conservation-atlas/assets/hydrology/WaterHydro_RiverCorridors_poly' , } ; // -------------------------------------------------------------------------------- // 3. Show river corridor that overlaps the study region. // -------------------------------------------------------------------------------- // Construct a feature collection from the river corridors address. // Filter the collection for features that intersect the study region. // Union the features within the collection. // Add the result from the last step to the map. Use color 'LightSkyBlue'. // -------------------------------------------------------------------------------- // 4. Show property parcels that overlap the river corridor in the study region. // -------------------------------------------------------------------------------- // Construct a feature collection from the parcels address. // Filter the feature collection for features that intersect the river corridors in the study region. // Filter the feature collection by attribute to keep features that are parcels (not roads, water, etc). /* Please note: to do the last step (filter by attribute), you will need to: 1. Find the property keys for the features and figure out which one describes the parcel property type. 2. Then you need to find the list of values for this key (so that you can see how the data distinguishes owned parcels from roads, water, etc). */ // Add result from last step as layer to map. Use color 'Gainsboro'. // -------------------------------------------------------------------------------- // 5. Show intersections of parcels and river corridors in the study region. // -------------------------------------------------------------------------------- // Create function to intersect each parcel (from step 3) by river corridor (from step 3). // Please ask for help if you get stuck here. // Apply the function to every parcel in the featue collection. // Add the result to the map. Use color 'LightSkyBlue'. // -------------------------------------------------------------------------------- // 6. Show features that meet an area criterion. // -------------------------------------------------------------------------------- // Write a function to compute area of each feature and append result to each feature as the property 'ACRES'. // Filter the result of above for all features that are greater than 50 acres. // Add the result as a layer to map. Use color 'Orchid'. // -------------------------------------------------------------------------------- // 7. Display results as a table. // -------------------------------------------------------------------------------- // We will discuss this last step in class tomorrow.","title":"Starter script"},{"location":"lessons/global_fires/","text":"Introduction Fires are always present on our planet. If you look any day of the year and any time of the day, you will always find a fire burning somewhere. How do fires differ physically, ecologically, and culturally? How can satellite imagery help us understand these differences and their relationships to each other? FIRMS fire map Background spectral signatures additive color burn severity MODIS band dictionary spectral indices awesome spectral indices Burn severity explorer Your goal this week is to write a script that functions similar to the app shown below. This link will open the app in a new browser tab. Deliverables There are two deliverables for this lesson: Please complete Form 1 to interpret global fires. You should be able to use either the app above or the script that you write to answer the questions in this form. Please complete Form 2 to submit six (6) checkpoints and a link to your script. Please submit answers to both forms by 9:30am on Thursday (11/16) . You will have until 5pm on Friday (11/17) to revise your script so that it produces correct answers. (Please re-submit the Form 2 with your corrections). New code snippets cloud masks normalized difference ratios reclassify with defined breaks Starter script /* Title: Burn severity explorer Author: Your name Date: Today's date */ // -------------------------------------------------------- // 1. Define study window and region. // -------------------------------------------------------- // Initial poi. var geometry = ee . Geometry . Point ([ - 122.29400855 , 37.14993396 ]); // Define year (2020) and month (8) as ee number objects. // Please name the outputs: study_year and study_month (respectively) // Construct a feature collection from \"WWF/HydroSHEDS/v1/Basins/hybas_7\". // // Please name the output: regions // Create a study region by filtering regions that intersect geometry variable. // // Please name the output: // study_region // Center map on study_region at zoom level 10. // Set base map to 'TERRAIN'. // -------------------------------------------------------- // Visualize global fire scars for the study window. // -------------------------------------------------------- // Create list of bands to work with. var bands = [ 'SWIR' , 'NIR' , 'Green' ]; // Create a list of the MODIS bands (from \"MODIS/061/MOD09A1\") that are equivalent to the bands list. // // Name the variable: // MODIS_bands // Load image_tools module to access cloud masks. var tools = require ( \"users/jhowarth/eePrimer:modules/image_tools.js\" ); // Load image collection from \"MODIS/061/MOD09A1\". // Filter by calendar year for study year variable. // Filter by calendar data with month variable as start and month plus one as end. // Apply the cloud filter (tools.cloudMask_MODIS) to every image in the collection. // Reduce each pixel value to the pixel's minimum in the collection. // Apply the scaling factor (0.0001) to the image. // Select the MODIS bands from the image and rename these bands as defined in first step of section. // // Please name the output: // after_fire_MODIS // Define viz parameters for false color swir. var viz = { min : [ 0 , 0 , 0 ], max : [ 0.6 , 0.65 , 0.5 ], bands : bands , gamma : 1.2 }; // Add image as layer to map with viz parameters. // Name the layer 'MODIS for study window'. // ******************************************************* // CHECK 1 // ******************************************************* // Load module. var check = require ( 'users/jhowarth/eePrimer:modules/checks.js' ); // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // after_fire_MODIS, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 1000, // Pixel resolution (scale) of image (dough) // 'CHECK 1' // Label to print to console above result. // ) // ; // -------------------------------------------------------- // 2. Display global fires for a study window. // -------------------------------------------------------- // Load FIRMS image collection from 'FIRMS' address. // Filter for calendar year with start and end as defined above. // Filter for calendar month with start as defined above and end as above plus 1. // Select the 'T21' band. // Reduce each pixel value to each pixel's maximum in the collection. // Convert from Kelvin to Celsius. // // Please name the ouput: // fire // Define viz parameters for the fire image. // Stretch values from 50 to 225 and use yellow, orange, and red colors to make the palette. // Add the fire layer with the fire viz and label the layer 'Fires'. // Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Construct a gradient legend with the fire viz parameters. // Title the legend 'Brightness temp (C) of fires in study month' so that 'in study month' prints on a new line. // Place the legend in the bottom left corner of the map. // Add legend to map. // ******************************************************* // CHECK 2 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // fire, // Image to check // 'T21', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 1000, // Pixel resolution (scale) of image (dough) // 'CHECK 2' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // 3. Investigate at finer scale with Sentinel 2. // ------------------------------------------------------------- // Make list of S2 bands that match band list. // // Name variable: // S2_bands // Write a function that takes year and month arguments // and chains the following: // Construct image collection from \"COPERNICUS/S2_SR_HARMONIZED\". // Filter collection for images that intersect study_region. // Filter collection by calendar year (from argument). // Filter collection by calendar month, where start is month from argument and end is this month plus one. // Apply tools.cloudMask_S2 to every image in the collection // Reduce collection to minimum value at each pixel. // Scale the pixel values by 0.0001. // Select the S2_bands and rename them regular band names (defined earlier). // Call function to make after fire image with study window year and month. // // Name the variable: after_fire_S2 // Call function to make before fire image with study window same month but year minus one. // // Name the variable: before_fire _S2 // Add after fire and before fire layers to map as false color SWIR. // Apply the same viz parameters that you used with layer image. // Name each layer 'S2 after' or 'S2 before' as appropriate. // ******************************************************* // CHECK 3 & 4 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // after_fire_S2, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 3' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // before_fire_S2, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 4' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // 4. Estimate burn severity index for before and after images. // ------------------------------------------------------------- // Compute burn ratio (normalized difference between NIR and SWIR) for before_fire_S2 and after_fire_S2 // Name the outputs: // br_before_S2 // br_after_S2 // Subtract br_after_S2 from br_before_S2. // // Name the output: dbr (for delta burn ratio) // Load community palettes. var palettes = require ( 'users/gena/packages:palettes' ); // Store the colorbrewer.BrBG[11] palette. // Reverse the color ramp so brown shows higher values and teal shows lower values. // // Name the variable: dbr_palette // Define viz parameters stretched from -0.75 to 0.75. // Display with palette. // // Name the variable: dbr_viz // Create a land mask from \"NASA/NASADEM_HGT/001\". // All pixels with elevation value greater than 0 should be 1. // All other pixels should be 0. var landMask = ee . Image ( \"NASA/NASADEM_HGT/001\" ) . select ( 'elevation' ) . gt ( 0 ); // Add dbr as layer to map and apply land mask, // display with dbr_viz, // name the layer 'Difference normalized burn ratio', // make not visible by default. // Construct a gradient legend with the dbr viz parameters. // Title the legend 'Difference normalized burn ratio'. // Place the legend in the bottom left corner of the map. // // Please name the output: dbr_legend // Add legend to map. // ******************************************************* // CHECK 5 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // dbr, // Image to check // 'nd', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 5' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // Classify burn severity index based on USGS thresholds. // ------------------------------------------------------------- // Classify burn severity image according to USGS thresholds. // // Name output: burn_severity_class var burn_severity_class = dbr . gte ( - 0.25 ) . add ( dbr . gte ( - 0.1 )) . add ( dbr . gte ( 0.1 )) . add ( dbr . gte ( 0.27 )) . add ( dbr . gte ( 0.44 )) . add ( dbr . gte ( 0.66 )) ; // Define burn severity class palette (give this to them). var burn_severity_palette = [ '#778735' , '#a7c04f' , '#07e444' , '#f6fc0d' , '#f7b140' , '#f86819' , '#a601d4' ] ; // Define vis parameters for classified burn severity index. // The min value should be 0 and the max should be 6. // Apply the palette defined above. // // Name the variable: burn_severity_viz // Add burn severity classes layer to map with land mask. // Apply burn_severity_viz and label the layer 'Classified values from S2'. // Create a list of labels for classes. // The length of this list must equal the length of class values. var burn_severity_labels = [ 'High post-fire regrowth' , 'Low post-fire regrowth' , 'Unburned' , 'Low Severity' , 'Moderate-low Severity' , 'Moderate-high Severity' , 'High Severity' ]; // Make a qualitative legend for burn severity layer. // Place the legend in the bottom-left. // // Name the legend: severity_legend // Add severity legend to the map. // ******************************************************* // CHECK 6 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // burn_severity_class, // Image to check // 'nd', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 6' // Label to print to console above result. // ) // ;","title":"A world on fire"},{"location":"lessons/global_fires/#introduction","text":"Fires are always present on our planet. If you look any day of the year and any time of the day, you will always find a fire burning somewhere. How do fires differ physically, ecologically, and culturally? How can satellite imagery help us understand these differences and their relationships to each other? FIRMS fire map","title":"Introduction"},{"location":"lessons/global_fires/#background","text":"spectral signatures additive color burn severity MODIS band dictionary spectral indices awesome spectral indices","title":"Background"},{"location":"lessons/global_fires/#burn-severity-explorer","text":"Your goal this week is to write a script that functions similar to the app shown below. This link will open the app in a new browser tab.","title":"Burn severity explorer"},{"location":"lessons/global_fires/#deliverables","text":"There are two deliverables for this lesson: Please complete Form 1 to interpret global fires. You should be able to use either the app above or the script that you write to answer the questions in this form. Please complete Form 2 to submit six (6) checkpoints and a link to your script. Please submit answers to both forms by 9:30am on Thursday (11/16) . You will have until 5pm on Friday (11/17) to revise your script so that it produces correct answers. (Please re-submit the Form 2 with your corrections).","title":"Deliverables"},{"location":"lessons/global_fires/#new-code-snippets","text":"cloud masks normalized difference ratios reclassify with defined breaks","title":"New code snippets"},{"location":"lessons/global_fires/#starter-script","text":"/* Title: Burn severity explorer Author: Your name Date: Today's date */ // -------------------------------------------------------- // 1. Define study window and region. // -------------------------------------------------------- // Initial poi. var geometry = ee . Geometry . Point ([ - 122.29400855 , 37.14993396 ]); // Define year (2020) and month (8) as ee number objects. // Please name the outputs: study_year and study_month (respectively) // Construct a feature collection from \"WWF/HydroSHEDS/v1/Basins/hybas_7\". // // Please name the output: regions // Create a study region by filtering regions that intersect geometry variable. // // Please name the output: // study_region // Center map on study_region at zoom level 10. // Set base map to 'TERRAIN'. // -------------------------------------------------------- // Visualize global fire scars for the study window. // -------------------------------------------------------- // Create list of bands to work with. var bands = [ 'SWIR' , 'NIR' , 'Green' ]; // Create a list of the MODIS bands (from \"MODIS/061/MOD09A1\") that are equivalent to the bands list. // // Name the variable: // MODIS_bands // Load image_tools module to access cloud masks. var tools = require ( \"users/jhowarth/eePrimer:modules/image_tools.js\" ); // Load image collection from \"MODIS/061/MOD09A1\". // Filter by calendar year for study year variable. // Filter by calendar data with month variable as start and month plus one as end. // Apply the cloud filter (tools.cloudMask_MODIS) to every image in the collection. // Reduce each pixel value to the pixel's minimum in the collection. // Apply the scaling factor (0.0001) to the image. // Select the MODIS bands from the image and rename these bands as defined in first step of section. // // Please name the output: // after_fire_MODIS // Define viz parameters for false color swir. var viz = { min : [ 0 , 0 , 0 ], max : [ 0.6 , 0.65 , 0.5 ], bands : bands , gamma : 1.2 }; // Add image as layer to map with viz parameters. // Name the layer 'MODIS for study window'. // ******************************************************* // CHECK 1 // ******************************************************* // Load module. var check = require ( 'users/jhowarth/eePrimer:modules/checks.js' ); // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // after_fire_MODIS, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 1000, // Pixel resolution (scale) of image (dough) // 'CHECK 1' // Label to print to console above result. // ) // ; // -------------------------------------------------------- // 2. Display global fires for a study window. // -------------------------------------------------------- // Load FIRMS image collection from 'FIRMS' address. // Filter for calendar year with start and end as defined above. // Filter for calendar month with start as defined above and end as above plus 1. // Select the 'T21' band. // Reduce each pixel value to each pixel's maximum in the collection. // Convert from Kelvin to Celsius. // // Please name the ouput: // fire // Define viz parameters for the fire image. // Stretch values from 50 to 225 and use yellow, orange, and red colors to make the palette. // Add the fire layer with the fire viz and label the layer 'Fires'. // Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Construct a gradient legend with the fire viz parameters. // Title the legend 'Brightness temp (C) of fires in study month' so that 'in study month' prints on a new line. // Place the legend in the bottom left corner of the map. // Add legend to map. // ******************************************************* // CHECK 2 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // fire, // Image to check // 'T21', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 1000, // Pixel resolution (scale) of image (dough) // 'CHECK 2' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // 3. Investigate at finer scale with Sentinel 2. // ------------------------------------------------------------- // Make list of S2 bands that match band list. // // Name variable: // S2_bands // Write a function that takes year and month arguments // and chains the following: // Construct image collection from \"COPERNICUS/S2_SR_HARMONIZED\". // Filter collection for images that intersect study_region. // Filter collection by calendar year (from argument). // Filter collection by calendar month, where start is month from argument and end is this month plus one. // Apply tools.cloudMask_S2 to every image in the collection // Reduce collection to minimum value at each pixel. // Scale the pixel values by 0.0001. // Select the S2_bands and rename them regular band names (defined earlier). // Call function to make after fire image with study window year and month. // // Name the variable: after_fire_S2 // Call function to make before fire image with study window same month but year minus one. // // Name the variable: before_fire _S2 // Add after fire and before fire layers to map as false color SWIR. // Apply the same viz parameters that you used with layer image. // Name each layer 'S2 after' or 'S2 before' as appropriate. // ******************************************************* // CHECK 3 & 4 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // after_fire_S2, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 3' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // before_fire_S2, // Image to check // bands, // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 4' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // 4. Estimate burn severity index for before and after images. // ------------------------------------------------------------- // Compute burn ratio (normalized difference between NIR and SWIR) for before_fire_S2 and after_fire_S2 // Name the outputs: // br_before_S2 // br_after_S2 // Subtract br_after_S2 from br_before_S2. // // Name the output: dbr (for delta burn ratio) // Load community palettes. var palettes = require ( 'users/gena/packages:palettes' ); // Store the colorbrewer.BrBG[11] palette. // Reverse the color ramp so brown shows higher values and teal shows lower values. // // Name the variable: dbr_palette // Define viz parameters stretched from -0.75 to 0.75. // Display with palette. // // Name the variable: dbr_viz // Create a land mask from \"NASA/NASADEM_HGT/001\". // All pixels with elevation value greater than 0 should be 1. // All other pixels should be 0. var landMask = ee . Image ( \"NASA/NASADEM_HGT/001\" ) . select ( 'elevation' ) . gt ( 0 ); // Add dbr as layer to map and apply land mask, // display with dbr_viz, // name the layer 'Difference normalized burn ratio', // make not visible by default. // Construct a gradient legend with the dbr viz parameters. // Title the legend 'Difference normalized burn ratio'. // Place the legend in the bottom left corner of the map. // // Please name the output: dbr_legend // Add legend to map. // ******************************************************* // CHECK 5 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // dbr, // Image to check // 'nd', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 5' // Label to print to console above result. // ) // ; // ------------------------------------------------------------- // Classify burn severity index based on USGS thresholds. // ------------------------------------------------------------- // Classify burn severity image according to USGS thresholds. // // Name output: burn_severity_class var burn_severity_class = dbr . gte ( - 0.25 ) . add ( dbr . gte ( - 0.1 )) . add ( dbr . gte ( 0.1 )) . add ( dbr . gte ( 0.27 )) . add ( dbr . gte ( 0.44 )) . add ( dbr . gte ( 0.66 )) ; // Define burn severity class palette (give this to them). var burn_severity_palette = [ '#778735' , '#a7c04f' , '#07e444' , '#f6fc0d' , '#f7b140' , '#f86819' , '#a601d4' ] ; // Define vis parameters for classified burn severity index. // The min value should be 0 and the max should be 6. // Apply the palette defined above. // // Name the variable: burn_severity_viz // Add burn severity classes layer to map with land mask. // Apply burn_severity_viz and label the layer 'Classified values from S2'. // Create a list of labels for classes. // The length of this list must equal the length of class values. var burn_severity_labels = [ 'High post-fire regrowth' , 'Low post-fire regrowth' , 'Unburned' , 'Low Severity' , 'Moderate-low Severity' , 'Moderate-high Severity' , 'High Severity' ]; // Make a qualitative legend for burn severity layer. // Place the legend in the bottom-left. // // Name the legend: severity_legend // Add severity legend to the map. // ******************************************************* // CHECK 6 // ******************************************************* // Use meanValue() function to print mean value of an image (dough) with a geometry (cutter). // check.meanValue( // Call function // burn_severity_class, // Image to check // 'nd', // Bands to select in image as a list // geometry, // Geometry to use as a cutter // 10, // Pixel resolution (scale) of image (dough) // 'CHECK 6' // Label to print to console above result. // ) // ;","title":"Starter script"},{"location":"lessons/global_forest_loss/","text":"source Introduction Over the last twenty years, how has the rate of deforestation changed in different regions of the world? How can satellite imagery distinguish different human and environmental factors that cause deforestation? How does the size and shape of study regions influence the calculation of deforestation? Background Motivation Earth observatory stories High-Resolution Global Maps of 21 st -Century Forest Cover Change Hansen Global Forest Change (2000 - 2021) Global Forest Change Global forest loss explorer Your goal this week is to write a script that reproduces the app shown below. This link will open the app in a new browser tab. Deliverables (part 1) Before the start of lab on Wednesday (11/30), please complete Form 1 to submit answers for the four checkpoints and a link to your script. Starter script (part 1) // NAME: Global forest loss Part 1 // AUTHOR: your name // DATE: today's date // Construct image from address: \"UMD/hansen/global_forest_change_2021_v1_9\" // Name the output: dataset // Threshold image so that land is 1 and everything else is 0 // Name the output: land_mask // Add land mask as layer. // Make land 'black' and not land 'DarkSlateGray'. // Name the layer 'land'. // Create viz parameters for 'treecover2000' band. // Display range from 0 to 100. // Use 'black' to display min and '#00992B' to display max. // Name the variable: tree_viz // Add dataset to map as a layer and apply land_mask and tree_viz. // Name the layer 'tree cover' and make not shown by default. // Define the loss palette. (Give this to them). var loss_palette = [ '#19faaf' , '#4ff398' , '#6cec81' , '#82e46c' , '#95dc57' , '#a5d344' , '#b4ca32' , '#c2c020' , '#cfb610' , '#daab04' , '#e59f06' , '#ef9313' , '#f78620' , '#fd782d' , '#ff693a' , '#ff5a47' , '#ff4b55' , '#ff3b63' , '#ff2b72' , '#fb1d81' , '#f11590' , '#e5179f' ] ; // Define viz parameters for 'lossyear' band // Display range from 0 to 21 and call loss_palette. // Name the variable: loss_viz. // Add dataset as layer to map and apply loss_viz. // Name the layer 'tree loss year' and make shown by default. // Select swir2, nir, and red bands from the last year and rename SWIR2, NIR, and Red. // Name the variable: last_composite. // Add last_composite as a layer to the map and apply land_mask as a mask. // Display as a SWIR2 false color composite with display range from 0 to 100. // Name the layer 'Last image' and make shown by default. // Compute the normalized difference in vegetation index (NDVI) with the 'NIR' and 'Red' bands from the last_composite. // Name the variable: last_ndvi. // Load the community palettes from 'users/gena/packages:palettes'. var palettes = require ( 'users/gena/packages:palettes' ); // Define viz parameters for the NDVI layer. // Display the colorbrewer.PRGn[11] community palette stretched from -0.8 to 0.8. // Name the variable: ndvi_viz // Add last_ndvi as a layer and apply the land_mask and ndvi_viz. // Name the layer 'Last ndvi' and do not show layer by default. // Compute the normalized burn ratio from last_composite. // Name the variable: last_nbr // Define viz paramters for the normalized burn ratio. // Apply the colorbrewer.BrBG[11] palette and stretch from -0.8 to 0.8. // Name the variable: nbr_viz // Add last_nbr as a layer to the map and apply land_mask as a mask and nbr_viz. // Name the layer 'Last burn ratio' and make not shown by default. // Construct a feature collection from address: \"FAO/GAUL/2015/level1\". // Name the variable: regions // Add regions to the map as a layer. // Display the regions with a white color and name the layer 'Regions'. // Do not show layer by default. // Construct a feature collection from address: \"WCMC/WDPA/current/polygons\" // Filter the feature collection for features where the 'STATUS' is not 'proposed' // And where the 'IUCN_CAT' is not 'VI'. // Name the variable: protected_lands // Add protected lands as a layer to the map. Display with color: '#17E551'. // Label the layer 'Protected Lands' and do not show by default. // ******************************************************* // CHECKS // ******************************************************* // Please uncomment all the lines below and run. // (If you followed the naming prompts, the code below should run.) // var check = require('users/jhowarth/eePrimer:modules/checks.js'); // // var poi = ee.Geometry.Point([-53.56837074828233, -6.485893364659269]); // // check.meanValue( // Call function // dataset, // Image to check // ['treecover2000', 'lossyear', 'datamask'], // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 1' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // last_ndvi, // Image to check // 'nd', // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 2' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // last_nbr, // Image to check // 'nd', // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 3' // Label to print to console above result. // ) // ; // // print('CHECK 4', 'size', protected_lands.size()); Starter scripts (part 2) Making the Global Forest Loss app involves the following steps: Initialize widgets for the layout (a side panel and a split panel that holds two linked maps). Initialize and add widgets to the side panel (for the title, instructions, cart panel, and credits). Make and add image layers for the left map (land, tree cover, tree loss year). Make and add image layers for the right map (last image, last ndvi, last burn ratio). Make and add reference feature layers for both maps (regions, protected lands). Prepare an image to chart change over time. Select a study region and center map on it. Chart change over time in selected region and add to side panel. Write functions to make app interactive. In the sections below, I have posted a starter script for each step of the solution. Some of the chunks require you to fill in missing code blocks. Other chunks are complete and should run as long as you follow the naming conventions. 1. Initialize widgets for the layout. This step works with widgets (user interface objects). Widgets are the basic elements for creating panels, labels, charts, even maps and map layers. New code: Panel widget Map widget Swipe maps // NAME: Global forest loss app // AUTHOR: Jeff Howarth // DATE: 1/28/2022 // ---------------------------------------------------------------------------- // 1. Initialize layout // ---------------------------------------------------------------------------- // Initialize side panel. Define width as 20%. // Name the variable: side_panel // Initialize a map widget for the left map. // Set basemap to SATELLITE. // Name the variable: left_map // Initialize a map widget for the right map. // Set basemap to SATELLITE. // Name the variable: right_map. // Initialize a map linker widget and link left_map and right_map. // Name the variable: map linker // Initialize a split panel widget to hold the two linked maps. // Define the orientation as 'horizontal' and wipe as true. // Name the variable: split_panel // Clear root. // Then add side panel and split panel to root. 2. Initialize widgets for the side panel. New code: Label widgets // ---------------------------------------------------------------------------- // 2. Initialize and add widgets to the side panel. // ---------------------------------------------------------------------------- // Initialize style parameters for title labels. // Name the variable: title_style // Initialize a label widget for the title and apply title_style. // Name the variable: title // Initialize style parameters for instructions. // Name the variable: instructions_style // Initialize a label widget for instructions and apply instructions_style. // Name the variable: chart_instructions // Initialize a panel widget to put the chart. // Name the variable: chart_panel // Define style parameters for region labels. // Name the variable: region_style // Initialize label widget for level 1 name and apply style parameters. // Name the variable: a1_label // Initialize label widget for level 0 name and apply style parameters. // Name the variable: a0_label // Define style parameters for credits. // Name the variable: credits_style // Initialize label widget for credits and apply credits style. // Name the variable: credits // Add label widgets to the side_panel. 3. Make and add image layers for left map. The code below creates and displays the global forest data layers from part 1 onto the left map. // ---------------------------------------------------------------------------- // 3. Make and add image layers for left map. // ---------------------------------------------------------------------------- // Construct image from address: \"UMD/hansen/global_forest_change_2021_v1_9\" // Name the output: dataset var dataset = ee . Image ( 'UMD/hansen/global_forest_change_2021_v1_9' ); // Threshold image so that land is 1 and everything else is 0 // Name the output: land_mask var land_mask = dataset . select ( 'datamask' ). eq ( 1 ); // Add land mask as layer. // Make land 'black' and not land 'DarkSlateGray'. // Name the layer 'land'. left_map . addLayer ( land_mask , { palette : [ 'DarkSlateGray' , 'Black' ]}, 'Land' ); // Create viz parameters for 'treecover2000' band. // Display range from 0 to 100. // Use 'black' to display min and '#00992B' to display max. // Name the variable: tree_viz var tree_viz = { bands : [ 'treecover2000' ], min : 0 , max : 100 , palette : [ 'black' , '#00992B' ] }; // Add dataset to map as a layer and apply land_mask and tree_viz. // Name the layer 'tree cover' and make not shown by default. left_map . addLayer ( dataset . updateMask ( land_mask ), tree_viz , 'tree cover' , 0 ); // Define the loss palette. (Give this to them). var loss_palette = [ '#19faaf' , '#4ff398' , '#6cec81' , '#82e46c' , '#95dc57' , '#a5d344' , '#b4ca32' , '#c2c020' , '#cfb610' , '#daab04' , '#e59f06' , '#ef9313' , '#f78620' , '#fd782d' , '#ff693a' , '#ff5a47' , '#ff4b55' , '#ff3b63' , '#ff2b72' , '#fb1d81' , '#f11590' , '#e5179f' ] ; // Define viz parameters for 'lossyear' band // Display range from 0 to 21 and call loss_palette. // Name the variable: loss_viz. var loss_viz = { bands : [ 'lossyear' ], min : 0 , max : 21 , palette : loss_palette }; // Add dataset as layer to map and apply loss_viz. // Name the layer 'tree loss year' and make shown by default. left_map . addLayer ( dataset , loss_viz , 'tree loss year' ); 4. Make and add image layers for right map. The code below creates and displays the global forest data layers from part 1 onto the right map. // ---------------------------------------------------------------------------- // 4. Make and add image layers for the right map. // ---------------------------------------------------------------------------- // Select swir2, nir, and red bands from the last year and rename SWIR2, NIR, and Red. // Name the variable: last_composite. var last_composite = dataset . select ([ 'last_b70' , 'last_b40' , 'last_b30' ], [ 'SWIR2' , 'NIR' , 'Red' ]); // Add last_composite as a layer to the map and apply land_mask as a mask. // Display as a SWIR2 false color composite with display range from 0 to 100. // Name the layer 'Last image' and make shown by default. right_map . addLayer ( last_composite . updateMask ( land_mask ), { min : 0 , max : 100 }, 'Last image' , 1 ); // Compute the normalized difference in vegetation index (NDVI) with the 'NIR' and 'Red' bands from the last_composite. // Name the variable: last_ndvi. var last_ndvi = last_composite . normalizedDifference ([ 'NIR' , 'Red' ]); // Load the community palettes from 'users/gena/packages:palettes'. // Name the variable: palettes. var palettes = require ( 'users/gena/packages:palettes' ); // Define viz parameters for the NDVI layer. // Display the colorbrewer.PRGn[11] community palette stretched from -0.8 to 0.8. // Name the variable: ndvi_viz var ndvi_viz = { min : - 0.8 , max : 0.8 , palette : palettes . colorbrewer . PRGn [ 11 ] }; // Add last_ndvi as a layer and apply the land_mask and ndvi_viz. // Name the layer 'Last ndvi' and do not show layer by default. right_map . addLayer ( last_ndvi . updateMask ( land_mask ), ndvi_viz , 'Last ndvi' , 0 ); // Compute the normalized burn ratio from last_composite. // Name the variable: last_nbr var last_nbr = last_composite . normalizedDifference ([ 'NIR' , 'SWIR2' ]); // Define viz paramters for the normalized burn ratio. // Apply the colorbrewer.BrBG[11] palette and stretch from -0.8 to 0.8. // Name the variable: nbr_viz var nbr_viz = { min : - 0.8 , max : 0.8 , palette : palettes . colorbrewer . BrBG [ 11 ] }; // Add last_nbr as a layer to the map and apply land_mask as a mask and nbr_viz. // Name the layer 'Last burn ratio' and make not shown by default. right_map . addLayer ( last_nbr . updateMask ( land_mask ), nbr_viz , 'Last burn ratio' , 0 ); 5. Make and add reference layers New code: Show feature outlines without fill // ---------------------------------------------------------------------------- // 5. Make reference feature layers for both maps. // ---------------------------------------------------------------------------- // Construct a feature collection from address: \"FAO/GAUL/2015/level1\". // Name the variable: regions var regions = ee . FeatureCollection ( \"FAO/GAUL/2015/level1\" ); // Define style parameters for region layer. // Name the variable: style_regions // Initialize map layer as a widget. // Call style_regions with .style method. // Label the layer 'regions_layer'. // Add map layer widget to left map. // Write a function to make map layer as a widget. // We will discuss on Thursday. var makeLayer = function ( fc , style , name , show ) { return ui . Map . Layer ( fc . style ( style ), {}, name , show ); }; // Call function to add regions layer as a widget to the right map. right_map . add ( makeLayer ( regions , style_regions , 'Regions' , 1 )); // Construct a feature collection from address: \"WCMC/WDPA/current/polygons\" // Filter the feature collection for features where the 'STATUS' is not 'proposed' // And where the 'IUCN_CAT' is not 'VI'. // Name the variable: protected_lands // Define style parameters for the protected lands layer. // Use color '#17E551' for strokes. // Name the variable: pro_lands_style // Call makeLayer() to add protected lands with pro_lands_style to both left and right maps. // Label the layers 'Protected Lands' and do not show by default. 6. Prepare image to chart change over time. Most of this step you have done previously and involves converting a feature collection into a binary image. The last step involves create an image for charting and I give you the code for this. We will discuss how this works in class tomorrow. // ---------------------------------------------------------------------------- // 6. Prepare image to chart change over time. // ---------------------------------------------------------------------------- // Write a function to give a feature a property named 'tag' and a specified value. // Apply function to all features in a feature collection (fc). // Create a function to convert feature collection to binary image. // Use function to convert a feature collection to an image. // Name the variable: pro_binary // Create image with a band stack to chart. // Last band must be integers that define classes. // Uncomment the sbippet below - it should run if you followed naming conventions. // We will discuss in class on Thursday. // var loss = ee.Image.pixelArea().multiply(0.000001) // .addBands(ee.Image.pixelArea().multiply(0.000001).updateMask(pro_binary)) // .addBands(dataset.select(['lossyear']) // ); 7. Select a study region and center map on it. // ---------------------------------------------------------------------------- // 7. Select a study region and center map on it. // ---------------------------------------------------------------------------- // Define poi. // Name the variable: poi var poi = ee . Geometry . Point ([ - 56.64116785106647 , - 13.042136580282266 ]); // Filter regions by poi to define study region // Name the variable: study_region // Center the left map on the selected study region at zoom level 6. // Define style parameters for selected region (show outlines without interiors). // Make the outlines 'yellow' and the width 1. // Name the variable: style_selected_region. // Add the selected study region as layer to both left and right maps and apply style parameters. // Print region names to labels (initialized previously). // Please uncomment the code block below. It should run if you have followed the naming conventions. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // cart.printFeaturePropertyLabel(study_region, 'ADM1_NAME', a1_label); // cart.printFeaturePropertyLabel(study_region, 'ADM0_NAME', a0_label); // Add labels to the chart panel. 8. Chart change over time in selected region and add to side panel. If you have followed the naming conventions, then the code block below should run. We will discuss how this steps work on Thursday. // ---------------------------------------------------------------------------- // 8. Chart change over time in selected region and add to side panel. // ---------------------------------------------------------------------------- // Define labels for the integer clases. // Name the variable: loss_labels var loss_labels = [ '2000' , '2001' , '2002' , '2003' , '2004' , '2005' , '2006' , '2007' , '2008' , '2009' , '2010' , '2011' , '2012' , '2013' , '2014' , '2015' , '2016' , '2017' , '2018' , '2019' , '2020' , '2021' ] ; // Define chart computation parameters. // Name the variable: loss_chart_params var loss_chart_params = { image : loss , // Image stack with classBand : 'lossyear' , // Band that defines nominal zones. region : study_region , // Cutter feature reducer : ee . Reducer . sum (), // Reducer for zonal statistic scale : 900 , // Scale for reducer classLabels : loss_labels , // Labels for values in dough band xLabels : [ // 'loss in whole region' , 'loss in protected areas' ] } ; // Define chart style parameters. // Name the variable: loss_chart_style var loss_chart_style = { colors : loss_palette , legend : { position : 'none' }, vAxis : { title : 'area (sq km)' , titleTextStyle : { italic : true , bold : false } }, } ; // Make the chart with the computation parameters and set the style options. // Name the variable: chart var chart = ui . Chart . image . byClass ( loss_chart_params ) . setOptions ( loss_chart_style ); // Add the chart to the chart panel chart_panel . add ( chart ); 9. Write functions to make app interactive. The code block below should run if you have followed the naming conventions. We will discuss on Thursday, but don\u2019t get too nervous about this chunk. I will not ask you to create an app that can handle map clicks as part of an IP. // ---------------------------------------------------------------------------- // 9. Write functions to make app interactive. // ---------------------------------------------------------------------------- // Initialize a configuration object to store temporary variables. // Name the variable: config var config = {}; // Write a function to store study region in config object. // Name the variable: makeStudyRegion var makeStudyRegion = function () { config . region = regions . filterBounds ( config . poi ); }; // Write a function that returns a chart using study region in config object. // Name the variable: makeLossChart var makeLossChart = function () { // Define chart computation parameters. var loss_chart_params = { image : loss , // Image stack with classBand : 'lossyear' , // Band that defines nominal zones. region : config . region , // Cutter feature reducer : ee . Reducer . sum (), // Reducer for zonal statistic scale : 900 , // Scale for reducer classLabels : loss_labels , // Labels for values in dough band xLabels : [ // 'loss in whole region' , 'loss in protected areas' ] } ; // Define chart style parameters. var loss_chart_style = { colors : loss_palette , legend : { position : 'none' }, vAxis : { title : 'area (sq km)' , titleTextStyle : { italic : true , bold : false } }, } ; return ui . Chart . image . byClass ( loss_chart_params ) . setOptions ( loss_chart_style ); }; // Write a function to update the chart based on study region from config object. // Name the variable: updateChart var updateChart = function () { chart_panel . clear (); // Clear widgets from the panel. cart . printFeaturePropertyLabel ( config . region , 'ADM1_NAME' , a1_label ); // Print admin1 property to label. cart . printFeaturePropertyLabel ( config . region , 'ADM0_NAME' , a0_label ); // Print admin0 property to label. chart_panel . add ( a1_label ). add ( a0_label ); // Add labels to chart panel chart_panel . add ( makeLossChart ()); // Add makeLossChart() to panel }; // Write a function to update the map with study region from config object. // Name the variable: updateMap var updateMap = function () { left_map . centerObject ( config . region , 6 ); left_map . layers (). set ( 5 , makeLayer ( config . region , style_selected_region , 'Selected region' , 1 )); right_map . layers (). set ( 5 , makeLayer ( config . region , style_selected_region , 'Selected region' , 1 )); }; // Write a function that adds the clicked point to the config object // and updates the chart and map based on the new study region. // Name the variable: handleMapClick var handleMapClick = function ( coordinates ) { config . poi = ee . Geometry . Point ([ coordinates . lon , coordinates . lat ]); makeStudyRegion (); updateChart (); updateMap (); }; // Call the function when user clicks the left map. left_map . onClick ( handleMapClick ); Deliverables (part 2) Please submit a link to your completed code in this DROPBOX by 9am on Thursday (12/1). We will discuss how to publish you code as an app in class. Explore global forest loss For each region listed below, please use the App and try to answer these questions: how has the location of deforestation changed over time in the region? are the proximate drivers of deforestation from wildfire, farming, ranging, mining, or another kind of land use? how has the location of deforestation in protected areas changed over time? Regions: Mato Grosso, Brazil Para, Brazil Rondonia, Brazil Madre de Dios, Peru Equateur, Democratic Republic of Congo Kasai, Democratic Republic of Congo Kalimantan Timur, Indonesia Nimrod, Oregon Panama City, Florida Deliverables (part 3) Please choose one region in the world to investigate (not from the list above) and respond to the prompts in this form . Thank you.","title":"Global forest loss"},{"location":"lessons/global_forest_loss/#introduction","text":"Over the last twenty years, how has the rate of deforestation changed in different regions of the world? How can satellite imagery distinguish different human and environmental factors that cause deforestation? How does the size and shape of study regions influence the calculation of deforestation?","title":"Introduction"},{"location":"lessons/global_forest_loss/#background","text":"Motivation Earth observatory stories High-Resolution Global Maps of 21 st -Century Forest Cover Change Hansen Global Forest Change (2000 - 2021) Global Forest Change","title":"Background"},{"location":"lessons/global_forest_loss/#global-forest-loss-explorer","text":"Your goal this week is to write a script that reproduces the app shown below. This link will open the app in a new browser tab.","title":"Global forest loss explorer"},{"location":"lessons/global_forest_loss/#deliverables-part-1","text":"Before the start of lab on Wednesday (11/30), please complete Form 1 to submit answers for the four checkpoints and a link to your script.","title":"Deliverables (part 1)"},{"location":"lessons/global_forest_loss/#starter-script-part-1","text":"// NAME: Global forest loss Part 1 // AUTHOR: your name // DATE: today's date // Construct image from address: \"UMD/hansen/global_forest_change_2021_v1_9\" // Name the output: dataset // Threshold image so that land is 1 and everything else is 0 // Name the output: land_mask // Add land mask as layer. // Make land 'black' and not land 'DarkSlateGray'. // Name the layer 'land'. // Create viz parameters for 'treecover2000' band. // Display range from 0 to 100. // Use 'black' to display min and '#00992B' to display max. // Name the variable: tree_viz // Add dataset to map as a layer and apply land_mask and tree_viz. // Name the layer 'tree cover' and make not shown by default. // Define the loss palette. (Give this to them). var loss_palette = [ '#19faaf' , '#4ff398' , '#6cec81' , '#82e46c' , '#95dc57' , '#a5d344' , '#b4ca32' , '#c2c020' , '#cfb610' , '#daab04' , '#e59f06' , '#ef9313' , '#f78620' , '#fd782d' , '#ff693a' , '#ff5a47' , '#ff4b55' , '#ff3b63' , '#ff2b72' , '#fb1d81' , '#f11590' , '#e5179f' ] ; // Define viz parameters for 'lossyear' band // Display range from 0 to 21 and call loss_palette. // Name the variable: loss_viz. // Add dataset as layer to map and apply loss_viz. // Name the layer 'tree loss year' and make shown by default. // Select swir2, nir, and red bands from the last year and rename SWIR2, NIR, and Red. // Name the variable: last_composite. // Add last_composite as a layer to the map and apply land_mask as a mask. // Display as a SWIR2 false color composite with display range from 0 to 100. // Name the layer 'Last image' and make shown by default. // Compute the normalized difference in vegetation index (NDVI) with the 'NIR' and 'Red' bands from the last_composite. // Name the variable: last_ndvi. // Load the community palettes from 'users/gena/packages:palettes'. var palettes = require ( 'users/gena/packages:palettes' ); // Define viz parameters for the NDVI layer. // Display the colorbrewer.PRGn[11] community palette stretched from -0.8 to 0.8. // Name the variable: ndvi_viz // Add last_ndvi as a layer and apply the land_mask and ndvi_viz. // Name the layer 'Last ndvi' and do not show layer by default. // Compute the normalized burn ratio from last_composite. // Name the variable: last_nbr // Define viz paramters for the normalized burn ratio. // Apply the colorbrewer.BrBG[11] palette and stretch from -0.8 to 0.8. // Name the variable: nbr_viz // Add last_nbr as a layer to the map and apply land_mask as a mask and nbr_viz. // Name the layer 'Last burn ratio' and make not shown by default. // Construct a feature collection from address: \"FAO/GAUL/2015/level1\". // Name the variable: regions // Add regions to the map as a layer. // Display the regions with a white color and name the layer 'Regions'. // Do not show layer by default. // Construct a feature collection from address: \"WCMC/WDPA/current/polygons\" // Filter the feature collection for features where the 'STATUS' is not 'proposed' // And where the 'IUCN_CAT' is not 'VI'. // Name the variable: protected_lands // Add protected lands as a layer to the map. Display with color: '#17E551'. // Label the layer 'Protected Lands' and do not show by default. // ******************************************************* // CHECKS // ******************************************************* // Please uncomment all the lines below and run. // (If you followed the naming prompts, the code below should run.) // var check = require('users/jhowarth/eePrimer:modules/checks.js'); // // var poi = ee.Geometry.Point([-53.56837074828233, -6.485893364659269]); // // check.meanValue( // Call function // dataset, // Image to check // ['treecover2000', 'lossyear', 'datamask'], // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 1' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // last_ndvi, // Image to check // 'nd', // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 2' // Label to print to console above result. // ) // ; // // check.meanValue( // Call function // last_nbr, // Image to check // 'nd', // Bands to select in image as a list // poi, // Geometry to use as a cutter // 900, // Pixel resolution (scale) of image (dough) // 'CHECK 3' // Label to print to console above result. // ) // ; // // print('CHECK 4', 'size', protected_lands.size());","title":"Starter script (part 1)"},{"location":"lessons/global_forest_loss/#starter-scripts-part-2","text":"Making the Global Forest Loss app involves the following steps: Initialize widgets for the layout (a side panel and a split panel that holds two linked maps). Initialize and add widgets to the side panel (for the title, instructions, cart panel, and credits). Make and add image layers for the left map (land, tree cover, tree loss year). Make and add image layers for the right map (last image, last ndvi, last burn ratio). Make and add reference feature layers for both maps (regions, protected lands). Prepare an image to chart change over time. Select a study region and center map on it. Chart change over time in selected region and add to side panel. Write functions to make app interactive. In the sections below, I have posted a starter script for each step of the solution. Some of the chunks require you to fill in missing code blocks. Other chunks are complete and should run as long as you follow the naming conventions.","title":"Starter scripts (part 2)"},{"location":"lessons/global_forest_loss/#deliverables-part-2","text":"Please submit a link to your completed code in this DROPBOX by 9am on Thursday (12/1). We will discuss how to publish you code as an app in class.","title":"Deliverables (part 2)"},{"location":"lessons/global_forest_loss/#explore-global-forest-loss","text":"For each region listed below, please use the App and try to answer these questions: how has the location of deforestation changed over time in the region? are the proximate drivers of deforestation from wildfire, farming, ranging, mining, or another kind of land use? how has the location of deforestation in protected areas changed over time? Regions: Mato Grosso, Brazil Para, Brazil Rondonia, Brazil Madre de Dios, Peru Equateur, Democratic Republic of Congo Kasai, Democratic Republic of Congo Kalimantan Timur, Indonesia Nimrod, Oregon Panama City, Florida","title":"Explore global forest loss"},{"location":"lessons/global_forest_loss/#deliverables-part-3","text":"Please choose one region in the world to investigate (not from the list above) and respond to the prompts in this form . Thank you.","title":"Deliverables (part 3)"},{"location":"lessons/global_oceans/","text":"Global oceans Motivating questions How does the elevation of the ocean\u2019s floor relate to phytoplankton blooms? How does sea surface temperature relate to phytoplankton blooms? How does one season differ from a long term average? How do these anomalies in sea surface temperature relate to hurricanes? How does SST relate to El Ni\u00f1o and La Ni\u00f1a events ( hint )? How do sea surface temperatures and phytoplankton blooms change over seasons( hint )? Map goal This week, we will use Earth Engine to compile map layers that allow us to explore these kinds of questions about global ocean systems. Here is a link to the app that will open in a separate window. Background resources Global Circulation Part one: differential heating Part two: the three cells Part three: the Coriolis effect and winds Ocean\u2019s Green Machines SST and Hurricanes Earth Engine datasets \u2018NOAA/NGDC/ETOPO1\u2019 \u2018NASA/OCEANDATA/MODIS-Terra/L3SMI\u2019 MODIS Starter script (1) Please try to complete this script and submit a link to your code in this DROPBOX by the end of class. If you have worked until the end of lab and have not finished the script, please still submit a link to your script to the dropbox and try to complete the script before lecture tomorrow. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_05_practice_problem.js // NAME: Jeff Howarth // DATE: 10/12/22 // PURPOSE: Explore sst and chlorophyll-a of global oceans. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // ------------------------------------------------------------------------ // 1. BATHYMETRY // ------------------------------------------------------------------------ // Load bathymetry dataset from 'NOAA/NGDC/ETOPO1' and select for 'bedrock' band. // Create a land mask. // Load the community palette module. // Select a community palette. // Please use cmocean.Deep[7]. // Please be sure deeper is darker colors and shallower elevations are lighter colors. // Config viz parameters. Stretch color palette over -5000 - 0 data values. // Add layer to map with land mask. // // ------------------------------------------------------------------------ // // 2. OCEAN WATER // // ------------------------------------------------------------------------ // Load 'NASA/OCEANDATA/MODIS-Terra/L3SMI'. Select 'sst' and 'chlor_a' bands. // Inspect first record and print size (number of images) of collection. // To filter image collection by calendarRange to make long term record. // 1. Define variables to make August and November the start and end months. // 2. Define 2000 and 2020 as start and end years for long term record. // 3. Filter by calendar range and call the startYear, endYear, startMonth, and endMonth variables defined above. // Reduce image collection into an image that represents the long term average (average value for each pixel). // Select community palette for sst. Please use colorbrewer.Spectral[11]. // Please make sure cool to warm colors map to cool to warm temperatures. // Define vis parameters for sst. Stretch palette from 10 to 30 data values. // Draw layer of sst long term average with land mask. // Select community palette for chorophyll-a. // Please use only the first six colors of niccoli.linearl[7]. // Define vis parameters for chlorophyl. Stretch values from 0 to 1 data values. // Draw chlorophyll-a layer with land mask. // ------------------------------------------------------------------------ // 3. ANOMALIES // ------------------------------------------------------------------------ // Define 2017 as the target year (or short term record). // Filter for the target year (defined above). // Inspect results. // Take the mean of the target and subtract the mean of the long term record. // Select community palette for sst anomaly. // Please use colorbrewer.RdBu[9]. // Please be sure cooler to warmer colors match cooler to warmer temperatures. // Define viz parameters. Stretch palette from -2 to 2 data values. // Select community palette for chlorophyll-a anomaly. // Define viz parameters. Stretch palette from -0.1 to 0.1. // Add anomaly layers to map. New code snippets The list below introduces new methods that you need to solve this problem. Community palette module Palettes for multiband images Load image collection Select bands Inspect image collection Filter by dates Reduce with local operations Starter script (2) This script is already complete for you. You should be able to append it to the end of your part 1 script, update the name for the target year, and run. // ======================================================================== // 4. HURRICANES // ======================================================================== // Load and filter feature collection for target year (defined previously). // Also remove all features with 'max_wind_kts' equal to -99. var canes_fc = ee . FeatureCollection ( \"NOAA/NHC/HURDAT2/atlantic\" ) . filter ( ee . Filter . eq ( 'year' , targetYear )) // rename targetYear to match your script from part 1 . filter ( ee . Filter . neq ( 'max_wind_kts' , - 99 )) ; // Print the first feature in the collection. // Also print the 'min' and 'max' values of the 'max_wind_kts' property in the collection. print ( 'canes' , canes_fc . first (), 'min' , canes_fc . aggregate_min ( 'max_wind_kts' ), 'max' , canes_fc . aggregate_max ( 'max_wind_kts' ) ) ; // Center the map on the feature collection at zoom level 4. Map . centerObject ( canes_fc , 4 ); // Set base map to satellite with labels. Map . setOptions ( 'HYBRID' ); // Add feature collection to map as layer with color 'Ivory'. Map . addLayer ( canes_fc , { color : 'Ivory' }, 'Hurricane points' , 0 ); // ------------------------------------------------------------------------ // Prep and display canes as lines. // ------------------------------------------------------------------------ // Import a module for visualizing the hurricane data from 'users/jhowarth/eePrimer:modules/caneViz.js'. var caneViz = require ( 'users/jhowarth/eePrimer:modules/caneViz.js' ); // Call makeCaneLines() function from module to create hurricane track lines for each named storm. var caneLines = caneViz . makeCaneLines ( canes_fc ); // Print a list of the named storms for the target year. print ( 'cane lines' , caneLines . aggregate_array ( 'name' ) . distinct () . sort () ) ; // Add the track lines to the map as a layer. Display with 'DarkSlateGray' color. Map . addLayer ( caneLines , { color : 'DarkSlateGray' }, 'Cane lines' , 1 ); // ------------------------------------------------------------------------ // Set up map // ------------------------------------------------------------------------ // Display feature collection as graduated circles. // Step 1. Define viz paramters. // Call makeGraduatedCircles() function from module with four arguments. var caneGradSize = caneViz . makeGraduatedCircles ( targetYear , // Feature collection to symbolize. 6 , // Size of smallest point symbol. 'DarkRed' , // Color of point symbols. 0.5 // Opacity of point symbols. ); // Step 2. Create feature view layer. var gradCircles = caneViz . makeCaneLayer ( \"NOAA/NHC/HURDAT2/atlantic_FeatureView\" , // Address of featureView layer. caneGradSize , // Viz parameters for graduated cricles (you made this in the last step). 'Hurricane class' // Label for the layer. ) ; // Step 3. Add featureView layer to map. Map . add ( gradCircles ); // Note: use .add() with the object you made above as argument (rather than addLayer().","title":"Global oceans"},{"location":"lessons/global_oceans/#global-oceans","text":"","title":"Global oceans"},{"location":"lessons/global_oceans/#motivating-questions","text":"How does the elevation of the ocean\u2019s floor relate to phytoplankton blooms? How does sea surface temperature relate to phytoplankton blooms? How does one season differ from a long term average? How do these anomalies in sea surface temperature relate to hurricanes? How does SST relate to El Ni\u00f1o and La Ni\u00f1a events ( hint )? How do sea surface temperatures and phytoplankton blooms change over seasons( hint )?","title":"Motivating questions"},{"location":"lessons/global_oceans/#map-goal","text":"This week, we will use Earth Engine to compile map layers that allow us to explore these kinds of questions about global ocean systems. Here is a link to the app that will open in a separate window.","title":"Map goal"},{"location":"lessons/global_oceans/#background-resources","text":"Global Circulation Part one: differential heating Part two: the three cells Part three: the Coriolis effect and winds Ocean\u2019s Green Machines SST and Hurricanes Earth Engine datasets \u2018NOAA/NGDC/ETOPO1\u2019 \u2018NASA/OCEANDATA/MODIS-Terra/L3SMI\u2019 MODIS","title":"Background resources"},{"location":"lessons/global_oceans/#starter-script-1","text":"Please try to complete this script and submit a link to your code in this DROPBOX by the end of class. If you have worked until the end of lab and have not finished the script, please still submit a link to your script to the dropbox and try to complete the script before lecture tomorrow. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_05_practice_problem.js // NAME: Jeff Howarth // DATE: 10/12/22 // PURPOSE: Explore sst and chlorophyll-a of global oceans. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // ------------------------------------------------------------------------ // 1. BATHYMETRY // ------------------------------------------------------------------------ // Load bathymetry dataset from 'NOAA/NGDC/ETOPO1' and select for 'bedrock' band. // Create a land mask. // Load the community palette module. // Select a community palette. // Please use cmocean.Deep[7]. // Please be sure deeper is darker colors and shallower elevations are lighter colors. // Config viz parameters. Stretch color palette over -5000 - 0 data values. // Add layer to map with land mask. // // ------------------------------------------------------------------------ // // 2. OCEAN WATER // // ------------------------------------------------------------------------ // Load 'NASA/OCEANDATA/MODIS-Terra/L3SMI'. Select 'sst' and 'chlor_a' bands. // Inspect first record and print size (number of images) of collection. // To filter image collection by calendarRange to make long term record. // 1. Define variables to make August and November the start and end months. // 2. Define 2000 and 2020 as start and end years for long term record. // 3. Filter by calendar range and call the startYear, endYear, startMonth, and endMonth variables defined above. // Reduce image collection into an image that represents the long term average (average value for each pixel). // Select community palette for sst. Please use colorbrewer.Spectral[11]. // Please make sure cool to warm colors map to cool to warm temperatures. // Define vis parameters for sst. Stretch palette from 10 to 30 data values. // Draw layer of sst long term average with land mask. // Select community palette for chorophyll-a. // Please use only the first six colors of niccoli.linearl[7]. // Define vis parameters for chlorophyl. Stretch values from 0 to 1 data values. // Draw chlorophyll-a layer with land mask. // ------------------------------------------------------------------------ // 3. ANOMALIES // ------------------------------------------------------------------------ // Define 2017 as the target year (or short term record). // Filter for the target year (defined above). // Inspect results. // Take the mean of the target and subtract the mean of the long term record. // Select community palette for sst anomaly. // Please use colorbrewer.RdBu[9]. // Please be sure cooler to warmer colors match cooler to warmer temperatures. // Define viz parameters. Stretch palette from -2 to 2 data values. // Select community palette for chlorophyll-a anomaly. // Define viz parameters. Stretch palette from -0.1 to 0.1. // Add anomaly layers to map.","title":"Starter script (1)"},{"location":"lessons/global_oceans/#new-code-snippets","text":"The list below introduces new methods that you need to solve this problem. Community palette module Palettes for multiband images Load image collection Select bands Inspect image collection Filter by dates Reduce with local operations","title":"New code snippets"},{"location":"lessons/global_oceans/#starter-script-2","text":"This script is already complete for you. You should be able to append it to the end of your part 1 script, update the name for the target year, and run. // ======================================================================== // 4. HURRICANES // ======================================================================== // Load and filter feature collection for target year (defined previously). // Also remove all features with 'max_wind_kts' equal to -99. var canes_fc = ee . FeatureCollection ( \"NOAA/NHC/HURDAT2/atlantic\" ) . filter ( ee . Filter . eq ( 'year' , targetYear )) // rename targetYear to match your script from part 1 . filter ( ee . Filter . neq ( 'max_wind_kts' , - 99 )) ; // Print the first feature in the collection. // Also print the 'min' and 'max' values of the 'max_wind_kts' property in the collection. print ( 'canes' , canes_fc . first (), 'min' , canes_fc . aggregate_min ( 'max_wind_kts' ), 'max' , canes_fc . aggregate_max ( 'max_wind_kts' ) ) ; // Center the map on the feature collection at zoom level 4. Map . centerObject ( canes_fc , 4 ); // Set base map to satellite with labels. Map . setOptions ( 'HYBRID' ); // Add feature collection to map as layer with color 'Ivory'. Map . addLayer ( canes_fc , { color : 'Ivory' }, 'Hurricane points' , 0 ); // ------------------------------------------------------------------------ // Prep and display canes as lines. // ------------------------------------------------------------------------ // Import a module for visualizing the hurricane data from 'users/jhowarth/eePrimer:modules/caneViz.js'. var caneViz = require ( 'users/jhowarth/eePrimer:modules/caneViz.js' ); // Call makeCaneLines() function from module to create hurricane track lines for each named storm. var caneLines = caneViz . makeCaneLines ( canes_fc ); // Print a list of the named storms for the target year. print ( 'cane lines' , caneLines . aggregate_array ( 'name' ) . distinct () . sort () ) ; // Add the track lines to the map as a layer. Display with 'DarkSlateGray' color. Map . addLayer ( caneLines , { color : 'DarkSlateGray' }, 'Cane lines' , 1 ); // ------------------------------------------------------------------------ // Set up map // ------------------------------------------------------------------------ // Display feature collection as graduated circles. // Step 1. Define viz paramters. // Call makeGraduatedCircles() function from module with four arguments. var caneGradSize = caneViz . makeGraduatedCircles ( targetYear , // Feature collection to symbolize. 6 , // Size of smallest point symbol. 'DarkRed' , // Color of point symbols. 0.5 // Opacity of point symbols. ); // Step 2. Create feature view layer. var gradCircles = caneViz . makeCaneLayer ( \"NOAA/NHC/HURDAT2/atlantic_FeatureView\" , // Address of featureView layer. caneGradSize , // Viz parameters for graduated cricles (you made this in the last step). 'Hurricane class' // Label for the layer. ) ; // Step 3. Add featureView layer to map. Map . add ( gradCircles ); // Note: use .add() with the object you made above as argument (rather than addLayer().","title":"Starter script (2)"},{"location":"lessons/longest_haul/","text":"The Shortest Route for the Longest Haul Introduction Currently, the longest non-stop commercial flight connects Singapore (SIN) to Newark (EWR). Does the flight path follow the great circle route between the two airports? In theory, this route represents the shortest geographic distance between these two locations. To begin to answer this question, please create a map that shows the great circle route between the two airports. Please copy and paste the starter script (below) into the code editor and then save it to your repo. Please write one or more lines of code to \u2018answer\u2019 each prompt. We will discuss both your solution (script) and our original question (does the flight follow the great circle route?) in our next class meeting. At that time, I will also show you how to submit your scripts and document your coursework this semester. Code key To solve this problem, please reference the following new code snippets: Get map center Get zoom Set map center and zoom Construct point geometry Add layer to map Center map on object Construct a feature Construct line geometry Calculate line length Convert units Concepts To understand these code snippets, we will discuss these concepts in lecture. Starter script // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Problem: The longest haul // Date: Today's date // Student: Your name here please // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Construct point geometry objects for SIN and EWR. // Construct line geometry object between the two airports. // Calculate the length of the route. // Make a feature called 'longest_haul' with the line geometry and the distance of the flight as an attribute. // Inspect the result by printing to Console. // Center the map as follows: // latitude: -0.9678795804756186 // longitude: -82.83931467161779 // zoom level: 3 // Change the base map to hybrid. // Add the line feature to the map, display it with red, and label the layer 'Longest haul route'. Airport support To help you find the airports, here is a screen shot that shows the location of SIN: And here is another that shows the location of EWR. Discussion Drawing lines on Mercator The True Size of SIN-EWR on sphere SIN-EWR last weekend Geographic friction EWR-SIN last weekend Geopolitical friction","title":"Shortest route for the longest haul"},{"location":"lessons/longest_haul/#the-shortest-route-for-the-longest-haul","text":"","title":"The Shortest Route for the Longest Haul"},{"location":"lessons/longest_haul/#introduction","text":"Currently, the longest non-stop commercial flight connects Singapore (SIN) to Newark (EWR). Does the flight path follow the great circle route between the two airports? In theory, this route represents the shortest geographic distance between these two locations. To begin to answer this question, please create a map that shows the great circle route between the two airports. Please copy and paste the starter script (below) into the code editor and then save it to your repo. Please write one or more lines of code to \u2018answer\u2019 each prompt. We will discuss both your solution (script) and our original question (does the flight follow the great circle route?) in our next class meeting. At that time, I will also show you how to submit your scripts and document your coursework this semester.","title":"Introduction"},{"location":"lessons/longest_haul/#code-key","text":"To solve this problem, please reference the following new code snippets: Get map center Get zoom Set map center and zoom Construct point geometry Add layer to map Center map on object Construct a feature Construct line geometry Calculate line length Convert units","title":"Code key"},{"location":"lessons/longest_haul/#concepts","text":"To understand these code snippets, we will discuss these concepts in lecture.","title":"Concepts"},{"location":"lessons/longest_haul/#starter-script","text":"// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Problem: The longest haul // Date: Today's date // Student: Your name here please // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Construct point geometry objects for SIN and EWR. // Construct line geometry object between the two airports. // Calculate the length of the route. // Make a feature called 'longest_haul' with the line geometry and the distance of the flight as an attribute. // Inspect the result by printing to Console. // Center the map as follows: // latitude: -0.9678795804756186 // longitude: -82.83931467161779 // zoom level: 3 // Change the base map to hybrid. // Add the line feature to the map, display it with red, and label the layer 'Longest haul route'.","title":"Starter script"},{"location":"lessons/longest_haul/#airport-support","text":"To help you find the airports, here is a screen shot that shows the location of SIN: And here is another that shows the location of EWR.","title":"Airport support"},{"location":"lessons/longest_haul/#discussion","text":"Drawing lines on Mercator The True Size of SIN-EWR on sphere SIN-EWR last weekend Geographic friction EWR-SIN last weekend Geopolitical friction","title":"Discussion"},{"location":"lessons/lst/","text":"Introduction This week we will estimate land surface temperature from Landsat imagery and then compare how parts of a region differ from the whole region. Our key technical questions this week include: What are scenes ? What are common sources for image scenes and how do they differ? How does a mosaic differ from a composite ? Link to background materials Our thematic questions include: What is land surface temperature (LST)? What is a spatial anomaly and how does this differ from the anomalies that we derived last week? How are spatial anomalies related to the principle of distributive justice ? Link to background materials Readings In prep for this week\u2019s problem, please read this article from The New York Times . Map goal Here is a link to the app that will open in a separate window. Dropbox By the end of lab, please submit a link to your solution in this DROPBOX . If you have worked through the end of lab and have not completed the solution, please try to complete the work and submit a link that updates your solution to the same dropbox before the start of lecture tomorrow. New methods The starter script below draws on many methods that you have learned previously. Here is a list of the new methods that you will need for this problem: Filter image collection by image property Reduce image by regions Convert feature collection to image Construct a gradient legend Construct a qualitative legend Check image output during workflow Check feature collection during workflow Starter script // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Problem: LST and HOLC in US Cities // Date: 10/19/2022 // Author: Your name here please // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // ------------------------------------------------------------------- // 1. LOAD HOLC ZONES FOR A STUDY REGION // ------------------------------------------------------------------- // Import feature collection of HOLC maps from 'projects/ee-primer/assets/holc_numeric_grades' var holc = ee . FeatureCollection ( 'projects/ee-primer/assets/holc_numeric_grades' ); // Inspect feature collection. // Print first feature in collection. // And print list of US cities in 'city' property of the feature collection. // This list should show every distinct city name sorted alphabetically. // Your result should contain 196 elements (city names). // Filter feature collection for a study region. // To start, please filter for features where 'city' property is 'Baltimore'. // ------------------------------------------------------------------- // 2. Load Landsat image collection. // ------------------------------------------------------------------- // Import module for LST computation from 'users/sofiaermida/landsat_smw_lst:modules/Landsat_LST.js' var LandsatLST = require ( 'users/sofiaermida/landsat_smw_lst:modules/Landsat_LST.js' ); // Apply module to produce image collection. // Use the 'L8' collection. // Start: 2020-07-01'. End: '2022-09-01' var collection = LandsatLST . collection ( 'L8' , // landsat collection '2020-07-01' , // start date '2022-09-01' , // end date fc // name of study region to filter images ) ; // Print first image in collection and size of collection. // If you used Baltimore as study city, // the collection should contain 42 images. // ------------------------------------------------------------------- // 3. Filter Landsat image collection. // ------------------------------------------------------------------- // Filter the collection from last step (output of LandsatLST) // for images collected in summer months (July and August) // with cloud cover less that 10 percent. // Also select only the 'LST' band from the images. // Print first image and size of collection. // If you used Baltimore as study city, // the collection should now contain 4 images. // -------------------------------------------------------------------------------- // 4. Reduce image collection to image. // -------------------------------------------------------------------------------- // Write a function that does two things: // (1) reduce summer month collection to mean image (grr!) // (2) convert units of reduced image from Kelvin to Fahrenheit. // // K to F conversion involves three steps: // (1) Subtract 273.15 from Kelvin temperature // (2) Multiply by 1.8 // (3) Add 32 // Apply function to summer month image collection. // Print your result. // The output should now be an Image rather than an Image collection. // If you check for min and max values, your min should be 68.2 and your max 118.8. // -------------------------------------------------------------------- // 5. Reduce image by regions. // -------------------------------------------------------------------- // First you want to find the mean temp in each part of your study city. // To do this, reduce mean summer temperature values within each study city HOLC feature. // where HOLC features are cookie cutters, // and mean summer month LST is dough. // // Set the scale so that it is the same as the 'dough'. // Inspect the result. // This should be a feature collection. // For Baltimore, you should have 59 features in collection. // Each feature will have a new property called 'mean' that holds mean temp of feature. // The min value should be 92.4 and the max should be 110.7. // Now you want to represent the 'whole' study region as a single feature. // To do this, union all HOLC features in study city. // Inspect result. You should have a Feature Collection with 1 Feature in it. // Now you can find the mean temp in your whole study region. // Reduce mean summer month land surface temperature within union of holc features // Inspect result. This should be a feature collection with one feature. // Again, the feature should have a property 'mean' // that holds the mean LST in the whole study region. // The min and max should be 102.1. // -------------------------------------------------------------------- // 6. Convert vector to raster. // -------------------------------------------------------------------- // Create a function to convert feature collection to image. // Apply function to convert mean summer temperature in each HOLC feature into an image. // Inspect result. Output should now be an Image. // The min should be 110.7 and the max should be 92.4. // Apply function to convert mean summer temperature for the union of HOLC features into an image. // Inspect result. Again, output should now be an Image. // The min and max should both be 102.1. // -------------------------------------------------------------------- // 7. Compute spatial anomalies as percent difference. // -------------------------------------------------------------------- // Use map algebra to estimate the 'percent difference' between parts and whole: // percent_diff = (mean_part - mean_whole) / mean_whole * 100 // If you check, the min should be -9.5 and the max 8.5. // -------------------------------------------------------------------- // 8. Compose map. // -------------------------------------------------------------------- // Load modules. var customBase = require ( 'users/jhowarth/eePrimer:modules/darkBase.js' ); var palettes = require ( 'users/gena/packages:palettes' ); // Set base map to 'darkBase'. Map . setOptions ( 'darkBase' , { 'darkBase' : customBase . darkBase }); // Center map on study city HOLC collection at zoom level 11. // Configure visualization parameters for percent different image. // Show values from -7 to 7 // and use crameri.vik[25] from ee-palettes. // Add percent difference image as layer to map. // Label layer 'Percent difference from average'. // Make layer display 0.5 opacity by default. // -------------------------------------------------------------------- // 9. Construct HOLC zone reference map and masked layers. // -------------------------------------------------------------------- // Convert HOLC study city feature collection into an image. // Use the 'grade' property to paint image values. // Inspect the result from above. // Add layer of percent differences that only shows the redlined zones. // Label this layer 'Redlined neighborhoods'. // Add layer of percent differences that only shows the non-redlined zones. // Label this layer 'Not redlined neighborhoods'. // Define vis parameters for HOLC zones. // Here is a palette based on the original HOLC color scheme: // ['#74a161','#7caeb6','#d5c958','#d97867'] // Add HOLC reference layer to map with vis parameters defined in last step. // -------------------------------------------------------------------- // 10. Add map keys (legends) for map layers. // -------------------------------------------------------------------- // Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Construct gradient legend for percent difference of LST layer. // Add legend to map. // Make labels for HOLC grades. var holc_grades = [ 'A: Best' , 'B: Still desirable' , 'C: Definitely declining' , 'D: Hazardous' ] ; // Construct qualitative legend for HOLC reference map. // Add legend to map. Further resources This topic was also covered by National Public Radio soon after publication of this academic research article . In fall 2020, Zach Levitt \u201820.5 developed the earth engine app shown below as an independent study with me in Geography. The study compared graphical presentations of data in news and academic outlets, reproduced the presentations in one or more source, and piloted designs to support novice readers. Zach is currently a graphics editor at The New York Times . Here is a link to the app that will open in a separate window.","title":"Land surface temperature"},{"location":"lessons/lst/#introduction","text":"This week we will estimate land surface temperature from Landsat imagery and then compare how parts of a region differ from the whole region. Our key technical questions this week include: What are scenes ? What are common sources for image scenes and how do they differ? How does a mosaic differ from a composite ? Link to background materials Our thematic questions include: What is land surface temperature (LST)? What is a spatial anomaly and how does this differ from the anomalies that we derived last week? How are spatial anomalies related to the principle of distributive justice ? Link to background materials","title":"Introduction"},{"location":"lessons/lst/#readings","text":"In prep for this week\u2019s problem, please read this article from The New York Times .","title":"Readings"},{"location":"lessons/lst/#map-goal","text":"Here is a link to the app that will open in a separate window.","title":"Map goal"},{"location":"lessons/lst/#dropbox","text":"By the end of lab, please submit a link to your solution in this DROPBOX . If you have worked through the end of lab and have not completed the solution, please try to complete the work and submit a link that updates your solution to the same dropbox before the start of lecture tomorrow.","title":"Dropbox"},{"location":"lessons/lst/#new-methods","text":"The starter script below draws on many methods that you have learned previously. Here is a list of the new methods that you will need for this problem: Filter image collection by image property Reduce image by regions Convert feature collection to image Construct a gradient legend Construct a qualitative legend Check image output during workflow Check feature collection during workflow","title":"New methods"},{"location":"lessons/lst/#starter-script","text":"// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Problem: LST and HOLC in US Cities // Date: 10/19/2022 // Author: Your name here please // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // ------------------------------------------------------------------- // 1. LOAD HOLC ZONES FOR A STUDY REGION // ------------------------------------------------------------------- // Import feature collection of HOLC maps from 'projects/ee-primer/assets/holc_numeric_grades' var holc = ee . FeatureCollection ( 'projects/ee-primer/assets/holc_numeric_grades' ); // Inspect feature collection. // Print first feature in collection. // And print list of US cities in 'city' property of the feature collection. // This list should show every distinct city name sorted alphabetically. // Your result should contain 196 elements (city names). // Filter feature collection for a study region. // To start, please filter for features where 'city' property is 'Baltimore'. // ------------------------------------------------------------------- // 2. Load Landsat image collection. // ------------------------------------------------------------------- // Import module for LST computation from 'users/sofiaermida/landsat_smw_lst:modules/Landsat_LST.js' var LandsatLST = require ( 'users/sofiaermida/landsat_smw_lst:modules/Landsat_LST.js' ); // Apply module to produce image collection. // Use the 'L8' collection. // Start: 2020-07-01'. End: '2022-09-01' var collection = LandsatLST . collection ( 'L8' , // landsat collection '2020-07-01' , // start date '2022-09-01' , // end date fc // name of study region to filter images ) ; // Print first image in collection and size of collection. // If you used Baltimore as study city, // the collection should contain 42 images. // ------------------------------------------------------------------- // 3. Filter Landsat image collection. // ------------------------------------------------------------------- // Filter the collection from last step (output of LandsatLST) // for images collected in summer months (July and August) // with cloud cover less that 10 percent. // Also select only the 'LST' band from the images. // Print first image and size of collection. // If you used Baltimore as study city, // the collection should now contain 4 images. // -------------------------------------------------------------------------------- // 4. Reduce image collection to image. // -------------------------------------------------------------------------------- // Write a function that does two things: // (1) reduce summer month collection to mean image (grr!) // (2) convert units of reduced image from Kelvin to Fahrenheit. // // K to F conversion involves three steps: // (1) Subtract 273.15 from Kelvin temperature // (2) Multiply by 1.8 // (3) Add 32 // Apply function to summer month image collection. // Print your result. // The output should now be an Image rather than an Image collection. // If you check for min and max values, your min should be 68.2 and your max 118.8. // -------------------------------------------------------------------- // 5. Reduce image by regions. // -------------------------------------------------------------------- // First you want to find the mean temp in each part of your study city. // To do this, reduce mean summer temperature values within each study city HOLC feature. // where HOLC features are cookie cutters, // and mean summer month LST is dough. // // Set the scale so that it is the same as the 'dough'. // Inspect the result. // This should be a feature collection. // For Baltimore, you should have 59 features in collection. // Each feature will have a new property called 'mean' that holds mean temp of feature. // The min value should be 92.4 and the max should be 110.7. // Now you want to represent the 'whole' study region as a single feature. // To do this, union all HOLC features in study city. // Inspect result. You should have a Feature Collection with 1 Feature in it. // Now you can find the mean temp in your whole study region. // Reduce mean summer month land surface temperature within union of holc features // Inspect result. This should be a feature collection with one feature. // Again, the feature should have a property 'mean' // that holds the mean LST in the whole study region. // The min and max should be 102.1. // -------------------------------------------------------------------- // 6. Convert vector to raster. // -------------------------------------------------------------------- // Create a function to convert feature collection to image. // Apply function to convert mean summer temperature in each HOLC feature into an image. // Inspect result. Output should now be an Image. // The min should be 110.7 and the max should be 92.4. // Apply function to convert mean summer temperature for the union of HOLC features into an image. // Inspect result. Again, output should now be an Image. // The min and max should both be 102.1. // -------------------------------------------------------------------- // 7. Compute spatial anomalies as percent difference. // -------------------------------------------------------------------- // Use map algebra to estimate the 'percent difference' between parts and whole: // percent_diff = (mean_part - mean_whole) / mean_whole * 100 // If you check, the min should be -9.5 and the max 8.5. // -------------------------------------------------------------------- // 8. Compose map. // -------------------------------------------------------------------- // Load modules. var customBase = require ( 'users/jhowarth/eePrimer:modules/darkBase.js' ); var palettes = require ( 'users/gena/packages:palettes' ); // Set base map to 'darkBase'. Map . setOptions ( 'darkBase' , { 'darkBase' : customBase . darkBase }); // Center map on study city HOLC collection at zoom level 11. // Configure visualization parameters for percent different image. // Show values from -7 to 7 // and use crameri.vik[25] from ee-palettes. // Add percent difference image as layer to map. // Label layer 'Percent difference from average'. // Make layer display 0.5 opacity by default. // -------------------------------------------------------------------- // 9. Construct HOLC zone reference map and masked layers. // -------------------------------------------------------------------- // Convert HOLC study city feature collection into an image. // Use the 'grade' property to paint image values. // Inspect the result from above. // Add layer of percent differences that only shows the redlined zones. // Label this layer 'Redlined neighborhoods'. // Add layer of percent differences that only shows the non-redlined zones. // Label this layer 'Not redlined neighborhoods'. // Define vis parameters for HOLC zones. // Here is a palette based on the original HOLC color scheme: // ['#74a161','#7caeb6','#d5c958','#d97867'] // Add HOLC reference layer to map with vis parameters defined in last step. // -------------------------------------------------------------------- // 10. Add map keys (legends) for map layers. // -------------------------------------------------------------------- // Load cart module. var cart = require ( 'users/jhowarth/eePrimer:modules/cart.js' ); // Construct gradient legend for percent difference of LST layer. // Add legend to map. // Make labels for HOLC grades. var holc_grades = [ 'A: Best' , 'B: Still desirable' , 'C: Definitely declining' , 'D: Hazardous' ] ; // Construct qualitative legend for HOLC reference map. // Add legend to map.","title":"Starter script"},{"location":"lessons/lst/#further-resources","text":"This topic was also covered by National Public Radio soon after publication of this academic research article . In fall 2020, Zach Levitt \u201820.5 developed the earth engine app shown below as an independent study with me in Geography. The study compared graphical presentations of data in news and academic outlets, reproduced the presentations in one or more source, and piloted designs to support novice readers. Zach is currently a graphics editor at The New York Times . Here is a link to the app that will open in a separate window.","title":"Further resources"},{"location":"lessons/natural_false_color/","text":"Introduction In this lesson, you will construct natural and false color composites with the Landsat 5 collection . Before you get started, please write down the descriptions for the first six bands of images in this collection. Concepts Band and Image Contrast enhancement RGB composites Additive color Band combinations App Please use this app for the activities described below. It may be helpful to open the app in a new browser tab with this link . Construct a natural color composite With the app below, please load an image and then do the following: Select the \u2018SR_B3\u2019 band for the red channel and enhance the contrast. Select the \u2018SR_B2\u2019 band for the green channel and enhance the contrast. Select the \u2018SR_B1\u2019 band for the blue channel and enhance the contrast. Click the Add RGB composite button. Why does the resulting image look like a \u2018natural\u2019 image? Construct a (NIR) false color composite. Repeat the steps, but this time use: \u2018SR_B4\u2019 for the red channel, \u2018SR_B3\u2019 for the green channel, \u2018SR_B2\u2019 for the blue channel. Then click the Add RGB composite button. Why does the resulting image look different? Zoom into the map and click a location to chart the pixel values in each band. Why does a location look bright red? What land cover does this represent? Why does a location look white? What land cover does this represent? Construct a (SWIR) false color composite. Repeat the steps, but this time use: \u2018SR_B5\u2019 for the red channel, \u2018SR_B4\u2019 for the green channel, \u2018SR_B2\u2019 for the blue channel. Then click the Add RGB composite button. Why does the resulting image look different? Zoom into the map and click a location to chart the pixel values in each band. Why does a location look bright green? What land cover does this represent? Why does a location look magenta? What land cover does this represent? What does a location look bright blue? What land cover does this represent?","title":"Natural and false color"},{"location":"lessons/natural_false_color/#introduction","text":"In this lesson, you will construct natural and false color composites with the Landsat 5 collection . Before you get started, please write down the descriptions for the first six bands of images in this collection.","title":"Introduction"},{"location":"lessons/natural_false_color/#concepts","text":"Band and Image Contrast enhancement RGB composites Additive color Band combinations","title":"Concepts"},{"location":"lessons/natural_false_color/#app","text":"Please use this app for the activities described below. It may be helpful to open the app in a new browser tab with this link .","title":"App"},{"location":"lessons/natural_false_color/#construct-a-natural-color-composite","text":"With the app below, please load an image and then do the following: Select the \u2018SR_B3\u2019 band for the red channel and enhance the contrast. Select the \u2018SR_B2\u2019 band for the green channel and enhance the contrast. Select the \u2018SR_B1\u2019 band for the blue channel and enhance the contrast. Click the Add RGB composite button. Why does the resulting image look like a \u2018natural\u2019 image?","title":"Construct a natural color composite"},{"location":"lessons/natural_false_color/#construct-a-nir-false-color-composite","text":"Repeat the steps, but this time use: \u2018SR_B4\u2019 for the red channel, \u2018SR_B3\u2019 for the green channel, \u2018SR_B2\u2019 for the blue channel. Then click the Add RGB composite button. Why does the resulting image look different? Zoom into the map and click a location to chart the pixel values in each band. Why does a location look bright red? What land cover does this represent? Why does a location look white? What land cover does this represent?","title":"Construct a (NIR) false color composite."},{"location":"lessons/natural_false_color/#construct-a-swir-false-color-composite","text":"Repeat the steps, but this time use: \u2018SR_B5\u2019 for the red channel, \u2018SR_B4\u2019 for the green channel, \u2018SR_B2\u2019 for the blue channel. Then click the Add RGB composite button. Why does the resulting image look different? Zoom into the map and click a location to chart the pixel values in each band. Why does a location look bright green? What land cover does this represent? Why does a location look magenta? What land cover does this represent? What does a location look bright blue? What land cover does this represent?","title":"Construct a (SWIR) false color composite."},{"location":"lessons/protected_lands/","text":"Area-based conservation goals at the town scale Introduction In January 2022, Amy Sheldon of Middlebury introduced H.606 to the Committee on Natural Resources, Fish, and Wildlife in Montpelier, Vermont. \u201cAn act relating to community resilience and biodiversity protection\u201d would require the State of Vermont to protect 30 percent of lands and waters in the state by 2030 and 50 percent by 2050. The conserved land would permanently protect natural landcover with different degrees of human use and resource extraction and include a mixture of ecological reserve areas, biodiversity conservation areas, natural resource management areas, and sustainable forest management areas. For much of Vermont, natural land cover is forested land cover or early-successional forest cover, in contrast to open, bare, and impervious land cover maintained by agriculture and development. The bill passed the House in March and then passed the Senate two months later. On May 12, 2022, the bill was delivered to Governor Scott for his signature. On June 2, the Governor vetoed the bill and wrote: \u201cVermont has a long history of effective land conservation that has significantly contributed to the state\u2019s vibrant, resilient working landscape of farms and forests, vast natural areas, and world class opportunities for outdoor recreation. This is a result of flexible and innovative tools like our current use program and the payment-for-ecosystem-services model. These programs are critical to achieving our conservation priorities because they combine conservation planning with incentives \u2013 making it more attractive and affordable for Vermont families to keep and conserve their land, farms and forests. \u201cOver the course of the legislative session, the Agency of Natural Resources testified multiple times against this bill. Among the objections, the Agency pointed to the conservation goals established in H.606 are unnecessarily tied to \u2013 and unreasonably limited to \u2013 permanent protection. The Agency has repeatedly said that permanent preservation has not been, and cannot be, the state\u2019s exclusive conservation tool and this bill, intentional or not, would diminish the existing and successful conservation tools we have. Based on the objections outlined above, I am returning this legislation without my signature pursuant to Chapter II, Section 11 of the Vermont Constitution.\u201d Your goal for this week is to develop a tool to assess the landcover of lands with permanent protections and lands registered in the Current Use and Value Appraisal Program within any Vermont town and create a chart to evaluate 30 by \u201830 and 50 by \u201850 goals at the town scale. The app below illustrates the basic tool that you will make using the case of Middlebury, Vermont. Here is a link to the app that will open in a separate window. Starter script // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_03_practice_problem.js // NAME: Your name here please // DATE: Today's date // PURPOSE: Town-scale analysis of protected and current use land cover. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Use the drawing tool to drop a point on Battell Bridge. // ---------------------------------------------------------------------------- // Data dictionary for problem. // ---------------------------------------------------------------------------- var data = { towns : 'projects/conservation-atlas/assets/cadastre/Boundary_TWNBNDS_poly' , lc : 'users/jhowarth/middCC/LandLandcov_BaseLC2016' , ag : 'projects/conservation-atlas/assets/landCover/lcAg_2016' , pro : 'projects/conservation-atlas/assets/protections/VT_Protected_Lands_Database' , uva : 'projects/conservation-atlas/assets/cadastre/Use_Value_Appraisal_Parcels' } ; // ---------------------------------------------------------------------------- // Create a map centered on a study region. // ---------------------------------------------------------------------------- // Load town feature collection from data.towns. // Filter study region by a poi. // Center on study region at zoom level 11. // Set base map to satellite with labels. // Add study region as a layer to map. // ---------------------------------------------------------------------------- // Display raster image as a map layer. // ---------------------------------------------------------------------------- // Construct a land cover image from data.land_cover. // Inspect image. // Define a palette to display land cover values. var lc_palette = [ '#A1E67E' , // 1. Tree Canopy '#F2F179' , // 2. Grass/Shrub '#f7f7f7' , // 3. Bare soil '#95C6E6' , // 4. Water '#525252' , // 5. Buildings 'white' , // 6. Roads '#cccccc' , // 7. 0ther pavement 'white' , // 8. Railroads ]; // Define visualization parameters. // Add layer to map. // ---------------------------------------------------------------------------- // Convert feature collection to a binary image. // ---------------------------------------------------------------------------- // Load a feature collection from data.ag. // Inspect first record of collection. // Add layer to map. // Create a function that gives each feature the property 'tag' with the value 1. // Apply function to all features in collection. // Inspect first record of collection. // Create a function to convert feature collection to image. // Apply function to feature collection. // Add binary image to map. // ---------------------------------------------------------------------------- // Map algebra: local operations between two raster layers. // ---------------------------------------------------------------------------- // Invert binary. // Erase inverted binary from landcover // Add a color for agriculture to palette list. // Inspect the palette. // Define vis parameters. // Add layer to map. // ---------------------------------------------------------------------------- // Generalize nominal attributes (aka reclassify). // ---------------------------------------------------------------------------- // Reclassify land cover // OLD VALUE OLD NAME NEW NAME NEW VALUE // 0 Ag Active Field 4 // 1 Tree canopy Forest 1 // 2 Grass/Shrub Old field 3 // 3 Bare soil Developed 5 // 4 Water Water 2 // 5 Buildings Developed 5 // 6 Roads Developed 5 // 7 Other pavement Developed 5 // 8 Railroads Developed 5 // Create list of old values. // Create list of new values. // Reclassify new values from old values. // Define palette for generalized layer. // Define vis parameters for generalized layer. // Add generalized layer to map. // ---------------------------------------------------------------------------- // Convert feature collection to binary image and apply as a mask layer. // ---------------------------------------------------------------------------- // Construct feature collection from data.pro. // Tag features. // Make image from tagged features. // Inspect result. // Add binary layer to map. // Add generalized layer to map with protected lands mask. // ---------------------------------------------------------------------------- // Convert feature collection to binary image and apply mask layers. // ---------------------------------------------------------------------------- // Construct feature collection from data.uva. // Tag features. // Make image from tagged features. // Inspect result. // Add binary layer to map. // Add generalized layer to map with protected lands mask. // Create a layer that represents UVA lands without permanent protections. // ---------------------------------------------------------------------------- // Compute area of a zone. // ---------------------------------------------------------------------------- // Create layer of pixel areas. // Inspect result. // Create parameters of zonal statistic. // Compute zonal statistic. // Inspect result. // ---------------------------------------------------------------------------- // Chart area of regions as a percent of zone. // ---------------------------------------------------------------------------- // Represent the percentage of the study region's area represented by the area of each pixel. // In other words, divide the area of each pixel by the area of the study region. // Rename this layer 'all of town'. // Inspect results. // Construct image with three dough bands and one cutter band. // Band 1 = pixel percentages // Band 2 = pixel percentages masked by protected lands // Band 3 = pixel percentages masked by uva lands without permanent protections // Band 4 = generalized land cover classes. // Inspect result. // ---------------------------------------------------------------------------- // Chart zonal statistic of each dough band with cutter zones. // ---------------------------------------------------------------------------- var lc_labels = [ '' , 'Tree canopy' , 'Water' , 'Old field' , 'Active field' , 'Developed' ] ; var chart_params = { image : simple_lc_area , classBand : 'land cover' , region : study_region , reducer : ee . Reducer . sum (), scale : 10 , classLabels : lc_labels , } ; var chart = ui . Chart . image . byClass ( chart_params ) . setChartType ( 'BarChart' ) . setOptions ({ title : 'Land cover in study region' , hAxis : { title : 'Percent of study region' , titleTextStyle : { italic : false , bold : true }, }, colors : simple_lc_palette , } ) ; New code snippets The list below introduces new methods that you need to solve this problem. Construct image from address Inspect image Add image as layer to map Convert feature collection to binary image Threshold an image Map algebra Generalize (reclassify) image Apply mask to image Compute area of a zone Chart area of regions as percent of zone","title":"Area-based goals at town scale"},{"location":"lessons/protected_lands/#area-based-conservation-goals-at-the-town-scale","text":"","title":"Area-based conservation goals at the town scale"},{"location":"lessons/protected_lands/#introduction","text":"In January 2022, Amy Sheldon of Middlebury introduced H.606 to the Committee on Natural Resources, Fish, and Wildlife in Montpelier, Vermont. \u201cAn act relating to community resilience and biodiversity protection\u201d would require the State of Vermont to protect 30 percent of lands and waters in the state by 2030 and 50 percent by 2050. The conserved land would permanently protect natural landcover with different degrees of human use and resource extraction and include a mixture of ecological reserve areas, biodiversity conservation areas, natural resource management areas, and sustainable forest management areas. For much of Vermont, natural land cover is forested land cover or early-successional forest cover, in contrast to open, bare, and impervious land cover maintained by agriculture and development. The bill passed the House in March and then passed the Senate two months later. On May 12, 2022, the bill was delivered to Governor Scott for his signature. On June 2, the Governor vetoed the bill and wrote: \u201cVermont has a long history of effective land conservation that has significantly contributed to the state\u2019s vibrant, resilient working landscape of farms and forests, vast natural areas, and world class opportunities for outdoor recreation. This is a result of flexible and innovative tools like our current use program and the payment-for-ecosystem-services model. These programs are critical to achieving our conservation priorities because they combine conservation planning with incentives \u2013 making it more attractive and affordable for Vermont families to keep and conserve their land, farms and forests. \u201cOver the course of the legislative session, the Agency of Natural Resources testified multiple times against this bill. Among the objections, the Agency pointed to the conservation goals established in H.606 are unnecessarily tied to \u2013 and unreasonably limited to \u2013 permanent protection. The Agency has repeatedly said that permanent preservation has not been, and cannot be, the state\u2019s exclusive conservation tool and this bill, intentional or not, would diminish the existing and successful conservation tools we have. Based on the objections outlined above, I am returning this legislation without my signature pursuant to Chapter II, Section 11 of the Vermont Constitution.\u201d Your goal for this week is to develop a tool to assess the landcover of lands with permanent protections and lands registered in the Current Use and Value Appraisal Program within any Vermont town and create a chart to evaluate 30 by \u201830 and 50 by \u201850 goals at the town scale. The app below illustrates the basic tool that you will make using the case of Middlebury, Vermont. Here is a link to the app that will open in a separate window.","title":"Introduction"},{"location":"lessons/protected_lands/#starter-script","text":"// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // TITLE: week_03_practice_problem.js // NAME: Your name here please // DATE: Today's date // PURPOSE: Town-scale analysis of protected and current use land cover. // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ // Use the drawing tool to drop a point on Battell Bridge. // ---------------------------------------------------------------------------- // Data dictionary for problem. // ---------------------------------------------------------------------------- var data = { towns : 'projects/conservation-atlas/assets/cadastre/Boundary_TWNBNDS_poly' , lc : 'users/jhowarth/middCC/LandLandcov_BaseLC2016' , ag : 'projects/conservation-atlas/assets/landCover/lcAg_2016' , pro : 'projects/conservation-atlas/assets/protections/VT_Protected_Lands_Database' , uva : 'projects/conservation-atlas/assets/cadastre/Use_Value_Appraisal_Parcels' } ; // ---------------------------------------------------------------------------- // Create a map centered on a study region. // ---------------------------------------------------------------------------- // Load town feature collection from data.towns. // Filter study region by a poi. // Center on study region at zoom level 11. // Set base map to satellite with labels. // Add study region as a layer to map. // ---------------------------------------------------------------------------- // Display raster image as a map layer. // ---------------------------------------------------------------------------- // Construct a land cover image from data.land_cover. // Inspect image. // Define a palette to display land cover values. var lc_palette = [ '#A1E67E' , // 1. Tree Canopy '#F2F179' , // 2. Grass/Shrub '#f7f7f7' , // 3. Bare soil '#95C6E6' , // 4. Water '#525252' , // 5. Buildings 'white' , // 6. Roads '#cccccc' , // 7. 0ther pavement 'white' , // 8. Railroads ]; // Define visualization parameters. // Add layer to map. // ---------------------------------------------------------------------------- // Convert feature collection to a binary image. // ---------------------------------------------------------------------------- // Load a feature collection from data.ag. // Inspect first record of collection. // Add layer to map. // Create a function that gives each feature the property 'tag' with the value 1. // Apply function to all features in collection. // Inspect first record of collection. // Create a function to convert feature collection to image. // Apply function to feature collection. // Add binary image to map. // ---------------------------------------------------------------------------- // Map algebra: local operations between two raster layers. // ---------------------------------------------------------------------------- // Invert binary. // Erase inverted binary from landcover // Add a color for agriculture to palette list. // Inspect the palette. // Define vis parameters. // Add layer to map. // ---------------------------------------------------------------------------- // Generalize nominal attributes (aka reclassify). // ---------------------------------------------------------------------------- // Reclassify land cover // OLD VALUE OLD NAME NEW NAME NEW VALUE // 0 Ag Active Field 4 // 1 Tree canopy Forest 1 // 2 Grass/Shrub Old field 3 // 3 Bare soil Developed 5 // 4 Water Water 2 // 5 Buildings Developed 5 // 6 Roads Developed 5 // 7 Other pavement Developed 5 // 8 Railroads Developed 5 // Create list of old values. // Create list of new values. // Reclassify new values from old values. // Define palette for generalized layer. // Define vis parameters for generalized layer. // Add generalized layer to map. // ---------------------------------------------------------------------------- // Convert feature collection to binary image and apply as a mask layer. // ---------------------------------------------------------------------------- // Construct feature collection from data.pro. // Tag features. // Make image from tagged features. // Inspect result. // Add binary layer to map. // Add generalized layer to map with protected lands mask. // ---------------------------------------------------------------------------- // Convert feature collection to binary image and apply mask layers. // ---------------------------------------------------------------------------- // Construct feature collection from data.uva. // Tag features. // Make image from tagged features. // Inspect result. // Add binary layer to map. // Add generalized layer to map with protected lands mask. // Create a layer that represents UVA lands without permanent protections. // ---------------------------------------------------------------------------- // Compute area of a zone. // ---------------------------------------------------------------------------- // Create layer of pixel areas. // Inspect result. // Create parameters of zonal statistic. // Compute zonal statistic. // Inspect result. // ---------------------------------------------------------------------------- // Chart area of regions as a percent of zone. // ---------------------------------------------------------------------------- // Represent the percentage of the study region's area represented by the area of each pixel. // In other words, divide the area of each pixel by the area of the study region. // Rename this layer 'all of town'. // Inspect results. // Construct image with three dough bands and one cutter band. // Band 1 = pixel percentages // Band 2 = pixel percentages masked by protected lands // Band 3 = pixel percentages masked by uva lands without permanent protections // Band 4 = generalized land cover classes. // Inspect result. // ---------------------------------------------------------------------------- // Chart zonal statistic of each dough band with cutter zones. // ---------------------------------------------------------------------------- var lc_labels = [ '' , 'Tree canopy' , 'Water' , 'Old field' , 'Active field' , 'Developed' ] ; var chart_params = { image : simple_lc_area , classBand : 'land cover' , region : study_region , reducer : ee . Reducer . sum (), scale : 10 , classLabels : lc_labels , } ; var chart = ui . Chart . image . byClass ( chart_params ) . setChartType ( 'BarChart' ) . setOptions ({ title : 'Land cover in study region' , hAxis : { title : 'Percent of study region' , titleTextStyle : { italic : false , bold : true }, }, colors : simple_lc_palette , } ) ;","title":"Starter script"},{"location":"lessons/protected_lands/#new-code-snippets","text":"The list below introduces new methods that you need to solve this problem. Construct image from address Inspect image Add image as layer to map Convert feature collection to binary image Threshold an image Map algebra Generalize (reclassify) image Apply mask to image Compute area of a zone Chart area of regions as percent of zone","title":"New code snippets"},{"location":"lessons/seaside_town_in_the_desert/","text":"Introduction The goal today is to visualize changes over the last 30 years in the vicinity of Muynak, Uzbekistan by making a gif animation similar to Google\u2019s Time Lapse . Dropbox Please paste a copy of your final script in the dropbox below by 5pm on Friday 11/11. As announced in class on Wed, we will not distribute copies of the script in class on Thursday and there will be no correction phase this week. DROPBOX Background What is Landsat? Mother of Landsat Landsat lexicon New code Landsat scaling function Starter script /* TITLE: Changes near Muynak, Uzbekistan with Landsat AUTHOR: Jeff Howarth UPDATE: 11/9/2022 */ // ----------------------------------------------------------------------------------- // Define study region // ----------------------------------------------------------------------------------- // Create a point on Muynak, Uzbekistan and buffer by 400 kilometers // Name the output: roi // Short for \"region of interest\" // Center the map on the roi and zoom level 7. // Change the base layer to 'TERRAIN'. // ----------------------------------------------------------------------------------- // Test a workflow // ----------------------------------------------------------------------------------- // Write a function to apply scaling factors for Landsat collections. // Step 1: Chain a workflow that does the following steps: // 1. Constructs an image collection from the 'LANDSAT/LT05/C02/T1_L2' address // 2. Filters by roi // 3. Filters by calendar year 1989 // 4. Filters by 'CLOUD_COVER' less than 20 // 5. Maps the applyScaleFactors function over the image collection // 6. Sorts by 'CLOUD_COVER' // Name the output: test // Step 2: Print the following to console: // 1. The year as a string, // 2. The size of the collection, // 3. All of the images in the collection. // Step 3: Reduce the image collection by median value of each pixel. // Name the output: median_test // Step 4: Print the year as string and the image to Console. // Step 5: Define viz parameters for the image collection to display a natural color composite from 0.0 and 0.3. // Name the variable: viz_nc // Step 6: Add the median_test layer to the map // Apply viz_nc // Name the layer 'test' // Define a swir false color composite that ranges from 0.0 to 0.5. // ----------------------------------------------------------------------------------- // Generalize workflow as a function. // ----------------------------------------------------------------------------------- // Define a global variable for cloud cover criterion as 25. // Name the variable: cloud // Write a function that implements your test workflow and takes three arguments: // ic: image collection address // yr: year for calendar range // vp: viz parameters // // This function should implement: // Step 1 // Step 2 // Step 3 // Step 4 // Step 6 // // And it should return the image. // Name the function: // doItAll // ----------------------------------------------------------------------------------- // Apply the function to make a time series. // ----------------------------------------------------------------------------------- // Use the following collections and years with viz_fc: // \"LANDSAT/LT05/C02/T1_L2\", 1989 // \"LANDSAT/LT05/C02/T1_L2\", 1994 // \"LANDSAT/LT05/C02/T1_L2\", 2000 // \"LANDSAT/LE07/C02/T1_L2\", 2004 // \"LANDSAT/LE07/C02/T1_L2\", 2009 // // Name each output to identify Landsat mission and image year: // For example: L4_1989 // Create another viz parameters for L8 and later missions // Name the variable: viz_L8_fc // Use the following collections and years with viz_fc: // \"LANDSAT/LC08/C02/T1_L2\", 2014 // \"LANDSAT/LC08/C02/T1_L2\", 2019 // \"LANDSAT/LC09/C02/T1_L2\", 2022 // Following naming convention as above. Make animation If you followed the naming conventions in the starter script, then the script below should print a gif animation to the Console. // ----------------------------------------------------------------------------------- // MAKE ANIMATION // ----------------------------------------------------------------------------------- // Combine time series of images that use viz_fc into an image collection; var time_series_01 = ee . ImageCollection . fromImages ([ L5_1989 , L5_1994 , L5_2000 , L7_2004 , L7_2009 ] ); // Combine time series of images that use viz_fc_L* into a second image collection; var time_series_02 = ee . ImageCollection . fromImages ( [ L8_2014 , L8_2019 , L9_2022 ] ); // Define a function to convert an image to an RGB image and copy // properties from the original image to the new RGB image. var convert_1 = function ( img ) { return img . visualize ( viz_fc ). copyProperties ( img , img . propertyNames ()); }; var convert_2 = function ( img ) { return img . visualize ( viz_fc_L8 ). copyProperties ( img , img . propertyNames ()); }; // Map over the image collection to convert each image to an RGB visualization // using the previously defined visualization function. var time_series_rgb = time_series_01 . map ( convert_1 ); var time_series2_rgb = time_series_02 . map ( convert_2 ); // Merge two image collections var time_series_merge = time_series_rgb . merge ( time_series2_rgb ); // Check to see if everything worked. print ( \"Going ok?\" , time_series_merge ); // Define arguments for animation function parameters. var videoArgs = { dimensions : 720 , framesPerSecond : 0.5 , region : roi }; // Print thumbnail to the Console. print ( ui . Thumbnail ( time_series_merge , videoArgs ));","title":"Seaside town in the desert"},{"location":"lessons/seaside_town_in_the_desert/#introduction","text":"The goal today is to visualize changes over the last 30 years in the vicinity of Muynak, Uzbekistan by making a gif animation similar to Google\u2019s Time Lapse .","title":"Introduction"},{"location":"lessons/seaside_town_in_the_desert/#dropbox","text":"Please paste a copy of your final script in the dropbox below by 5pm on Friday 11/11. As announced in class on Wed, we will not distribute copies of the script in class on Thursday and there will be no correction phase this week. DROPBOX","title":"Dropbox"},{"location":"lessons/seaside_town_in_the_desert/#background","text":"What is Landsat? Mother of Landsat Landsat lexicon","title":"Background"},{"location":"lessons/seaside_town_in_the_desert/#new-code","text":"Landsat scaling function","title":"New code"},{"location":"lessons/seaside_town_in_the_desert/#starter-script","text":"/* TITLE: Changes near Muynak, Uzbekistan with Landsat AUTHOR: Jeff Howarth UPDATE: 11/9/2022 */ // ----------------------------------------------------------------------------------- // Define study region // ----------------------------------------------------------------------------------- // Create a point on Muynak, Uzbekistan and buffer by 400 kilometers // Name the output: roi // Short for \"region of interest\" // Center the map on the roi and zoom level 7. // Change the base layer to 'TERRAIN'. // ----------------------------------------------------------------------------------- // Test a workflow // ----------------------------------------------------------------------------------- // Write a function to apply scaling factors for Landsat collections. // Step 1: Chain a workflow that does the following steps: // 1. Constructs an image collection from the 'LANDSAT/LT05/C02/T1_L2' address // 2. Filters by roi // 3. Filters by calendar year 1989 // 4. Filters by 'CLOUD_COVER' less than 20 // 5. Maps the applyScaleFactors function over the image collection // 6. Sorts by 'CLOUD_COVER' // Name the output: test // Step 2: Print the following to console: // 1. The year as a string, // 2. The size of the collection, // 3. All of the images in the collection. // Step 3: Reduce the image collection by median value of each pixel. // Name the output: median_test // Step 4: Print the year as string and the image to Console. // Step 5: Define viz parameters for the image collection to display a natural color composite from 0.0 and 0.3. // Name the variable: viz_nc // Step 6: Add the median_test layer to the map // Apply viz_nc // Name the layer 'test' // Define a swir false color composite that ranges from 0.0 to 0.5. // ----------------------------------------------------------------------------------- // Generalize workflow as a function. // ----------------------------------------------------------------------------------- // Define a global variable for cloud cover criterion as 25. // Name the variable: cloud // Write a function that implements your test workflow and takes three arguments: // ic: image collection address // yr: year for calendar range // vp: viz parameters // // This function should implement: // Step 1 // Step 2 // Step 3 // Step 4 // Step 6 // // And it should return the image. // Name the function: // doItAll // ----------------------------------------------------------------------------------- // Apply the function to make a time series. // ----------------------------------------------------------------------------------- // Use the following collections and years with viz_fc: // \"LANDSAT/LT05/C02/T1_L2\", 1989 // \"LANDSAT/LT05/C02/T1_L2\", 1994 // \"LANDSAT/LT05/C02/T1_L2\", 2000 // \"LANDSAT/LE07/C02/T1_L2\", 2004 // \"LANDSAT/LE07/C02/T1_L2\", 2009 // // Name each output to identify Landsat mission and image year: // For example: L4_1989 // Create another viz parameters for L8 and later missions // Name the variable: viz_L8_fc // Use the following collections and years with viz_fc: // \"LANDSAT/LC08/C02/T1_L2\", 2014 // \"LANDSAT/LC08/C02/T1_L2\", 2019 // \"LANDSAT/LC09/C02/T1_L2\", 2022 // Following naming convention as above.","title":"Starter script"},{"location":"lessons/seaside_town_in_the_desert/#make-animation","text":"If you followed the naming conventions in the starter script, then the script below should print a gif animation to the Console. // ----------------------------------------------------------------------------------- // MAKE ANIMATION // ----------------------------------------------------------------------------------- // Combine time series of images that use viz_fc into an image collection; var time_series_01 = ee . ImageCollection . fromImages ([ L5_1989 , L5_1994 , L5_2000 , L7_2004 , L7_2009 ] ); // Combine time series of images that use viz_fc_L* into a second image collection; var time_series_02 = ee . ImageCollection . fromImages ( [ L8_2014 , L8_2019 , L9_2022 ] ); // Define a function to convert an image to an RGB image and copy // properties from the original image to the new RGB image. var convert_1 = function ( img ) { return img . visualize ( viz_fc ). copyProperties ( img , img . propertyNames ()); }; var convert_2 = function ( img ) { return img . visualize ( viz_fc_L8 ). copyProperties ( img , img . propertyNames ()); }; // Map over the image collection to convert each image to an RGB visualization // using the previously defined visualization function. var time_series_rgb = time_series_01 . map ( convert_1 ); var time_series2_rgb = time_series_02 . map ( convert_2 ); // Merge two image collections var time_series_merge = time_series_rgb . merge ( time_series2_rgb ); // Check to see if everything worked. print ( \"Going ok?\" , time_series_merge ); // Define arguments for animation function parameters. var videoArgs = { dimensions : 720 , framesPerSecond : 0.5 , region : roi }; // Print thumbnail to the Console. print ( ui . Thumbnail ( time_series_merge , videoArgs ));","title":"Make animation"},{"location":"lessons/spectral_signatures/","text":"Introduction What is a spectral signature chart and how do these charts serve as guides for making and reading RGB composites? Background Remote sensing systems Reflectance Electromagnetic spectrum Spectral signatures Sentinel Spectral signature app We will explore spectral signatures from Sentinel 2 images with this app . Start with familiar Pan and zoom to playing fields at Middlebury College. Sample two locations in the field encircled by the track. Use this key to interpret how reflectance changes with wavelength. Create NIR and SWIR false color composites and explain what the colors mean. Revisit Muynak Type \u201cMuynak, Uzbekistan\u201d into the app\u2019s Search bar. Pan out a little and redraw the scene. Clear chart and samples. Create signature charts for bright green, bright cyan, black, and white pixels. Move between false color and natural color composites. What do you think the different colors represent? Gather a library Please gather a library of spectral signatures for six (6) of the following land cover types: Salt flats Water with algae Clear water Water with silt Sand Snow Coniferous forest Deciduous forest with leaves on Deciduous forest with leaves off Cultivated crops Grassland Asphalt For each, please sample two points. So your chart should have two lines total and the lines should be similar. If they aren\u2019t similar, clear the chart and try again. They don\u2019t need to be exactly identical, but they should show similar signatures. Please use this Google form to submit each chart. You can take a break and return to your work if necessary, so use this form to make 6 individual submissions. Please use the lecture time today to make progress on this task and complete your submissions by 5pm on Monday 11/14 or contact me if you would like to request additional time. Thank you. App instructions Pan map to a place you would like to explore. Click, hold, and drag map to pan. Click + or - to zoom in or out. Add a S2 image to the map. Click DRAW S2 scene at map center. Change the date range filter for the image. This will let you change the season or year of the image that you add to the map. Check the Define start and end dates box. Enter a start and end date to define a time window. You must use YYYY-MM-DD format. Try to use two month windows. Click the DRAW S2 scene at map center button to apply your date filter. If the layer panel on the map turns red, try changing the date window and click the DRAW button again. Plot spectral signature at one or more sample points for each land cover type. Check the Explore spectral signatures box. Click a location on the map to chart the spectral signature of that location based on date in the S2 image. You can click more than one location to add multiple signatures to the chart. Click Clear chart and samples to, well, to do that. If you get a chart error, clear the chart and try again. Download the spectral signature chart as a .png. Click the little gray icon in the upper right corner to open the chart in a new browser tab. Click the Download PNG to download the chart as an image to your Google drive. Thank you to Valerie Pasquarella ( @valpasq ) for her inspiring Spectral Encounter app.","title":"Spectral signature scavenger hunt"},{"location":"lessons/spectral_signatures/#introduction","text":"What is a spectral signature chart and how do these charts serve as guides for making and reading RGB composites?","title":"Introduction"},{"location":"lessons/spectral_signatures/#background","text":"Remote sensing systems Reflectance Electromagnetic spectrum Spectral signatures Sentinel","title":"Background"},{"location":"lessons/spectral_signatures/#spectral-signature-app","text":"We will explore spectral signatures from Sentinel 2 images with this app .","title":"Spectral signature app"},{"location":"lessons/spectral_signatures/#start-with-familiar","text":"Pan and zoom to playing fields at Middlebury College. Sample two locations in the field encircled by the track. Use this key to interpret how reflectance changes with wavelength. Create NIR and SWIR false color composites and explain what the colors mean.","title":"Start with familiar"},{"location":"lessons/spectral_signatures/#revisit-muynak","text":"Type \u201cMuynak, Uzbekistan\u201d into the app\u2019s Search bar. Pan out a little and redraw the scene. Clear chart and samples. Create signature charts for bright green, bright cyan, black, and white pixels. Move between false color and natural color composites. What do you think the different colors represent?","title":"Revisit Muynak"},{"location":"lessons/spectral_signatures/#gather-a-library","text":"Please gather a library of spectral signatures for six (6) of the following land cover types: Salt flats Water with algae Clear water Water with silt Sand Snow Coniferous forest Deciduous forest with leaves on Deciduous forest with leaves off Cultivated crops Grassland Asphalt For each, please sample two points. So your chart should have two lines total and the lines should be similar. If they aren\u2019t similar, clear the chart and try again. They don\u2019t need to be exactly identical, but they should show similar signatures. Please use this Google form to submit each chart. You can take a break and return to your work if necessary, so use this form to make 6 individual submissions. Please use the lecture time today to make progress on this task and complete your submissions by 5pm on Monday 11/14 or contact me if you would like to request additional time. Thank you.","title":"Gather a library"},{"location":"lessons/spectral_signatures/#app-instructions","text":"Pan map to a place you would like to explore. Click, hold, and drag map to pan. Click + or - to zoom in or out. Add a S2 image to the map. Click DRAW S2 scene at map center. Change the date range filter for the image. This will let you change the season or year of the image that you add to the map. Check the Define start and end dates box. Enter a start and end date to define a time window. You must use YYYY-MM-DD format. Try to use two month windows. Click the DRAW S2 scene at map center button to apply your date filter. If the layer panel on the map turns red, try changing the date window and click the DRAW button again. Plot spectral signature at one or more sample points for each land cover type. Check the Explore spectral signatures box. Click a location on the map to chart the spectral signature of that location based on date in the S2 image. You can click more than one location to add multiple signatures to the chart. Click Clear chart and samples to, well, to do that. If you get a chart error, clear the chart and try again. Download the spectral signature chart as a .png. Click the little gray icon in the upper right corner to open the chart in a new browser tab. Click the Download PNG to download the chart as an image to your Google drive. Thank you to Valerie Pasquarella ( @valpasq ) for her inspiring Spectral Encounter app.","title":"App instructions"}]}